---
name: 전체 데이터 마이그레이션
overview: 로컬 개발 환경의 모든 데이터(D1 74개 테이블 + R2 7,638개 파일)를 Cloudflare Production으로 안전하게 마이그레이션하고, 새 User Profile 테이블 3개를 추가합니다.
todos:
  - id: phase1-login
    content: Cloudflare 로그인 및 현재 리소스 확인
    status: pending
  - id: phase1-backup
    content: Production DB 현재 상태 백업 및 확인
    status: pending
  - id: phase2-dump
    content: 로컬 D1 덤프 생성 (migration_backup.sql)
    status: pending
  - id: phase2-sync
    content: Production D1에 데이터 동기화
    status: pending
    dependencies:
      - phase2-dump
  - id: phase2-newtables
    content: 새 User Profile 테이블 3개 추가
    status: pending
    dependencies:
      - phase2-sync
  - id: phase3-bucket
    content: R2 버킷 생성 또는 확인
    status: pending
  - id: phase3-upload
    content: R2 이미지 7,638개 업로드
    status: pending
    dependencies:
      - phase3-bucket
  - id: phase4-vectorize
    content: Vectorize 인덱스 생성 및 데이터 인덱싱
    status: pending
  - id: phase5-secrets
    content: Cloudflare Pages Secrets 설정
    status: pending
  - id: phase6-cleanup
    content: careerwiki-jobs DB 삭제 및 최종 검증
    status: pending
    dependencies:
      - phase2-newtables
      - phase3-upload
---

# 전체 데이터 마이그레이션 가이드

## 현재 상황 분석

### 로컬 데이터 (마이그레이션 원본)

```
.wrangler/state/v3/
├── d1/miniflare-D1DatabaseObject/
│   └── careerwiki-db.sqlite        # 74개 테이블, 6,945 jobs, 1,682 태깅
├── r2/careerwiki-uploads/blobs/    # 7,638개 이미지 파일
└── kv/KV/blobs/                    # 342개 캐시 (선택적)
```

### Cloudflare 현황 (마이그레이션 대상)

- `careerwiki-db` (ID: edc21e23-c2ac-4693-bb79-389b6914e173) - 사용 중
- `careerwiki-jobs` - 오래된 테스트용, 삭제 예정
- `careerwiki-uploads` R2 버킷 - 생성 필요 여부 확인
- `careerwiki-embeddings` Vectorize - 생성 필요

---

## Phase 1: 사전 준비 (필수)

### 1-1. Cloudflare 로그인 및 권한 확인

```bash
npx wrangler login
npx wrangler whoami
```

### 1-2. 현재 Cloudflare 리소스 확인

```bash
# D1 데이터베이스 목록
npx wrangler d1 list

# R2 버킷 목록
npx wrangler r2 bucket list

# Vectorize 인덱스 목록
npx wrangler vectorize list
```

### 1-3. Production DB 현재 상태 백업

```bash
# Cloudflare careerwiki-db의 테이블 목록 확인
npx wrangler d1 execute careerwiki-db --remote --command "SELECT name FROM sqlite_master WHERE type='table';"

# 중요 데이터 개수 확인
npx wrangler d1 execute careerwiki-db --remote --command "SELECT COUNT(*) FROM jobs;"
```

---

## Phase 2: D1 데이터베이스 마이그레이션

### 2-1. 로컬 DB 덤프 생성

```bash
# 방법 1: wrangler export (권장)
npx wrangler d1 export careerwiki-db --local --output=migration_backup.sql

# 방법 2: 직접 SQLite 복사 (백업용)
copy ".wrangler\state\v3\d1\miniflare-D1DatabaseObject\careerwiki-db.sqlite" "backup_local_db_20260107.sqlite"
```

### 2-2. Production DB 초기화 (주의: 기존 데이터 덮어쓰기)

**옵션 A: 완전 교체 (로컬이 최신이므로 권장)**

```bash
# 기존 테이블 확인 후 필요시 초기화
npx wrangler d1 execute careerwiki-db --remote --file=migration_backup.sql
```

**옵션 B: 신규 테이블만 추가 (기존 데이터 유지)**

```bash
# 새 테이블 3개만 추가 (User Profile)
npx wrangler d1 execute careerwiki-db --remote --file=src/services/ai-analyzer/migrations/009_add_user_profile_tables.sql
```

### 2-3. 데이터 동기화 확인

```bash
# Production 테이블 개수 확인
npx wrangler d1 execute careerwiki-db --remote --command "SELECT COUNT(*) FROM sqlite_master WHERE type='table';"

# 주요 데이터 개수 확인
npx wrangler d1 execute careerwiki-db --remote --command "SELECT COUNT(*) FROM jobs;"
npx wrangler d1 execute careerwiki-db --remote --command "SELECT COUNT(*) FROM job_attributes;"
```

---

## Phase 3: R2 이미지 마이그레이션

### 3-1. R2 버킷 생성 (없으면)

```bash
# 버킷 존재 여부 확인
npx wrangler r2 bucket list

# 없으면 생성
npx wrangler r2 bucket create careerwiki-uploads
```

### 3-2. 로컬 R2 데이터 구조 확인

로컬 R2 파일 위치: `.wrangler/state/v3/r2/careerwiki-uploads/blobs/`

- 7,638개 파일 (이미지)

### 3-3. R2 업로드 스크립트 작성 및 실행

```bash
# 방법 1: wrangler r2 object put (개별 파일)
npx wrangler r2 object put careerwiki-uploads/jobs/image1.png --file=./local/path/image1.png

# 방법 2: 배치 업로드 스크립트 (대량 파일용)
node scripts/migrate-r2-images.js
```

**주의**: 로컬 R2 blobs는 해시 기반 파일명으로 저장됨. 매핑 테이블(SQLite) 참조 필요.

---

## Phase 4: Vectorize 설정

### 4-1. Vectorize 인덱스 생성

```bash
npx wrangler vectorize create careerwiki-embeddings --dimensions=768 --metric=cosine
```

### 4-2. 직업 데이터 인덱싱

Production 배포 후 API를 통해 실행하거나, 별도 스크립트로 실행:

```bash
# indexJobsToVectorize() 함수 호출
curl -X POST https://your-domain.com/api/ai-analyzer/admin/index-jobs
```

---

## Phase 5: 환경 변수 및 Secrets 설정

### 5-1. Cloudflare Pages Secrets 설정

```bash
# API Keys
npx wrangler pages secret put CAREER_NET_API_KEY
npx wrangler pages secret put GOYONG24_MAJOR_API_KEY
npx wrangler pages secret put GOYONG24_JOB_API_KEY

# AI API Keys
npx wrangler pages secret put GEMINI_API_KEY

# Auth
npx wrangler pages secret put GOOGLE_CLIENT_ID
npx wrangler pages secret put GOOGLE_CLIENT_SECRET
npx wrangler pages secret put JWT_SECRET

# Admin
npx wrangler pages secret put ADMIN_SECRET
```

### 5-2. wrangler.jsonc 바인딩 확인

- [wrangler.jsonc](wrangler.jsonc) 파일에서 D1, R2, Vectorize, AI 바인딩 확인

---

## Phase 6: 정리 작업

### 6-1. 오래된 DB 삭제

```bash
# careerwiki-jobs DB 삭제 (사용자 확인 후)
npx wrangler d1 delete careerwiki-jobs
```

### 6-2. 배포 및 검증

```bash
# Production 배포
npm run deploy:prod

# 검증
curl https://your-domain.com/api/ai-analyzer/health
```

---

## 체크리스트

### 마이그레이션 전

- [ ] Cloudflare 로그인 완료
- [ ] 로컬 DB 백업 완료
- [ ] Production DB 현재 상태 확인

### D1 마이그레이션

- [ ] 로컬 DB 덤프 생성
- [ ] Production DB에 데이터 동기화
- [ ] 새 테이블 3개 추가 확인
- [ ] 데이터 개수 일치 확인 (jobs: 6,945, job_attributes: 1,682)

### R2 마이그레이션

- [ ] R2 버킷 생성/확인
- [ ] 이미지 파일 업로드 (7,638개)
- [ ] uploaded_files 테이블과 매핑 확인

### Vectorize 설정

- [ ] 인덱스 생성
- [ ] 직업 데이터 인덱싱

### 환경 변수

- [ ] 모든 Secrets 설정 완료

### 정리

- [ ] careerwiki-jobs DB 삭제
- [ ] Production 배포 및 검증

---

## 예상 소요 시간

| 단계 | 예상 시간 |

|------|-----------|

| Phase 1: 사전 준비 | 30분 |

| Phase 2: D1 마이그레이션 | 1-2시간 |

| Phase 3: R2 이미지 업로드 | 2-4시간 (7,638개) |

| Phase 4: Vectorize 설정 | 1-2시간 |

| Phase 5: 환경 변수 | 30분 |

| Phase 6: 검증 및 정리 | 1시간 |

| **총 예상 시간** | **6-10시간** |

---

## 롤백 계획

문제 발생 시:

1. `.wrangler/backups/d1_backup_20251112_162756.sqlite` 사용
2. 또는 `migration_backup.sql`에서 복원
3. R2는 로컬 `.wrangler/state/v3/r2/` 에서 재업로드