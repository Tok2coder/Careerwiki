// CareerWiki AI Analyzer API Routes
// Version: v2.0.0-stage-based (Universal Intake + Stage-based Follow-up)
// Framework: Hono (Cloudflare Workers)

import { Hono } from 'hono'
import type { D1Database, VectorizeIndex, Ai } from '@cloudflare/workers-types'
import {
  VERSIONS,
  assertConstraintType,
  isValidStage,
  isMinorStage,
  isExperienceAllowed,
  type AnalysisRequestPayload,
  type AnalysisResultJSON,
  type AnalysisRequestPayloadV3,
  type UniversalAnswers,
  type AnalysisStage,
  type FollowupResponsePayload,
  type FollowupResult,
  type DeepIntakeInput,
  type UserInsight,
  type DebugInfo,
} from './types'
import {
  calculateFactBoosts,
  applyFactBoostsToJob,
  normalizeDeepIntake,
  type JobScores,
  type NormalizedDeepIntake,
} from './fact-score-mapping'
import {
  generateFollowupQuestions,
  type ScoredJob,
  type FollowupQuestion,
} from './question-generation'
import { taggerRoutes } from './tagger-routes'
import {
  UNIVERSAL_QUESTIONS,
  getUniversalQuestionsForStage,
  INSIGHT_WORDING,
  type UniversalQuestion,
} from './universal-questions'
import {
  getQuestionsForStage,
  getQuestionText,
  type StageQuestion,
} from './stage-question-banks'
import {
  handleFollowupNo,
  applyDiversityGuard,
  type FollowupNoResult,
  type RankChangeInfo,
} from './safe-replacement'
import {
  buildEvidenceLinks,
  generateDefaultEvidence,
  type Fact as EvidenceFact,
  type ScoredJobForEvidence,
} from './evidence-generator'
import {
  generatePremiumReport,
  type PremiumReportInput,
} from './premium-report-generator'
import {
  generatePurposeBasedFollowups,
  type PurposeBasedFollowupInput,
} from './question-generation'
import {
  saveConversationTurn,
  saveProfileSnapshot,
  buildProfileFromTurns,
  getNextTurnNumber,
  getLatestProfileSnapshot,
  extractSignalsFromAnswer,
  type TurnType,
  type ProfileSnapshot,
} from './user-profile'
import {
  expandCandidates,
  vectorResultsToScoredJobs,
  buildSearchQuery,
} from './vectorize-pipeline'

// ============================================
// Error Handling (V3 í‘œì¤€í™”)
// ============================================
type ErrorCode = 
  | 'VALIDATION_ERROR'     // 400: ì…ë ¥ ê²€ì¦ ì‹¤íŒ¨
  | 'INVALID_STAGE'        // 400: ì˜ëª»ëœ Stage
  | 'INVALID_PAYLOAD'      // 400: ì˜ëª»ëœ í˜ì´ë¡œë“œ í˜•ì‹
  | 'SESSION_NOT_FOUND'    // 404: ì„¸ì…˜ ì—†ìŒ
  | 'REQUEST_NOT_FOUND'    // 404: ë¶„ì„ ìš”ì²­ ì—†ìŒ
  | 'RESULT_NOT_FOUND'     // 404: ë¶„ì„ ê²°ê³¼ ì—†ìŒ
  | 'DB_ERROR'             // 500: ë°ì´í„°ë² ì´ìŠ¤ ì˜¤ë¥˜
  | 'ANALYSIS_FAILED'      // 500: ë¶„ì„ ì²˜ë¦¬ ì‹¤íŒ¨
  | 'INTERNAL_ERROR'       // 500: ë‚´ë¶€ ì˜¤ë¥˜

interface ApiError {
  error: string
  code: ErrorCode
  details?: Record<string, unknown>
  request_id?: number
  timestamp: string
}

function createErrorResponse(
  code: ErrorCode,
  message: string,
  details?: Record<string, unknown>,
  requestId?: number
): ApiError {
  return {
    error: message,
    code,
    details,
    request_id: requestId,
    timestamp: new Date().toISOString(),
  }
}

// Error logging (development only - no file logging)
function logError(
  code: ErrorCode, 
  message: string, 
  context?: Record<string, unknown>
): void {
  console.error(`[AI-ANALYZER ERROR] ${code}: ${message}`, context ? JSON.stringify(context) : '')
}

// ============================================
// Phase 4 Metrics Events Helper
// ============================================
async function savePhase4MetricsEvents(
  db: D1Database,
  sessionId: string,
  userId: string | undefined,
  requestId: number,
  result: AnalysisResultJSON
): Promise<void> {
  try {
    // Phase 4 ì ìš© ì—¬ë¶€ í™•ì¸
    if (!result.phase4_applied) return
    
    // 1. Diversity Guard ì ìš© ì´ë²¤íŠ¸
    if (result.diversity_guard_active) {
      await db.prepare(`
        INSERT INTO raw_events (user_id, session_id, event_type, payload_json)
        VALUES (?, ?, 'DIVERSITY_GUARD_APPLIED', ?)
      `).bind(
        userId || null,
        sessionId,
        JSON.stringify({
          request_id: requestId,
          changes: result.diversity_changes || [],
          top3_jobs: result.fit_top3?.map(j => j.job_name) || [],
        })
      ).run()
    }
    
    // 2. Research Bias Cap ì ìš© ì—¬ë¶€ í™•ì¸ (diversity_changesì—ì„œ ì¶”ë¡ )
    const researchBiasApplied = (result.diversity_changes || []).some(
      change => change.includes('ì—°êµ¬') || change.includes('research') || change.includes('Diversity Guard')
    )
    if (researchBiasApplied) {
      await db.prepare(`
        INSERT INTO raw_events (user_id, session_id, event_type, payload_json)
        VALUES (?, ?, 'RESEARCH_BIAS_CAP_APPLIED', ?)
      `).bind(
        userId || null,
        sessionId,
        JSON.stringify({
          request_id: requestId,
          original_changes: result.diversity_changes || [],
        })
      ).run()
    }
    
  } catch (error) {
    // ë©”íŠ¸ë¦­ìŠ¤ ì €ì¥ ì‹¤íŒ¨ëŠ” ë¶„ì„ ê²°ê³¼ì— ì˜í–¥ ì—†ìŒ
    console.error('Phase4 metrics event save failed:', error)
  }
}

// ============================================
// Bindings (Cloudflare Workers)
// ============================================
interface Bindings {
  DB: D1Database
  VECTORIZE?: VectorizeIndex
  AI?: Ai
  GEMINI_API_KEY?: string
  [key: string]: unknown
}

// ============================================
// AI Analyzer Routes
// ============================================
const analyzerRoutes = new Hono<{ Bindings: Bindings }>()

// ============================================
// POST /analyze - ë¶„ì„ ìš”ì²­ (V3: Stage-based + V2 í˜¸í™˜)
// ============================================
interface AnalysisRequestPayloadV2 extends AnalysisRequestPayload {
  deep_intake?: DeepIntakeInput
}

// V3 ë˜ëŠ” V2 í˜ì´ë¡œë“œ ë‘˜ ë‹¤ ì²˜ë¦¬
type AnalyzePayload = AnalysisRequestPayloadV3 | AnalysisRequestPayloadV2

analyzerRoutes.post('/analyze', async (c) => {
  const db = c.env.DB
  const rawPayload = await c.req.json<AnalyzePayload>()
  
  try {
    // V3 vs V2 íŒë³„
    const isV3 = 'stage' in rawPayload && 'universal_answers' in rawPayload
    
    // 1. ì…ë ¥ ê²€ì¦
    if (!rawPayload.session_id) {
      logError('VALIDATION_ERROR', 'session_id is required')
      return c.json(createErrorResponse('VALIDATION_ERROR', 'session_id is required'), 400)
    }
    
    // V3 ì¶”ê°€ ê²€ì¦
    let stage: AnalysisStage | undefined
    let universalAnswers: UniversalAnswers = {}
    let debugMode = false
    
    if (isV3) {
      const v3Payload = rawPayload as AnalysisRequestPayloadV3
      debugMode = v3Payload.debug || false
      
      if (!isValidStage(v3Payload.stage)) {
        logError('INVALID_STAGE', `Invalid stage: ${v3Payload.stage}`, { provided: v3Payload.stage })
        return c.json(createErrorResponse('INVALID_STAGE', `Invalid stage: ${v3Payload.stage}`, {
          provided: v3Payload.stage,
          allowed: ['job_explore', 'job_student', 'job_early', 'major_explore', 'major_student', 'major_early']
        }), 400)
      }
      stage = v3Payload.stage
      universalAnswers = v3Payload.universal_answers || {}
      
      // 2a. Stage ì„ íƒ ì´ë²¤íŠ¸ ì €ì¥ [ìˆ˜ì •ì‚¬í•­ 2: CHECK ì œì•½ ì—†ì´ ë¬¸ìì—´ë¡œ ê¸°ë¡]
      await db.prepare(`
        INSERT INTO raw_events (user_id, session_id, event_type, payload_json, client_meta)
        VALUES (?, ?, 'STAGE_SELECTED', ?, ?)
      `).bind(
        v3Payload.user_id || null,
        v3Payload.session_id,
        JSON.stringify({ stage: v3Payload.stage, analysis_type: v3Payload.analysis_type }),
        c.req.header('User-Agent') || null
      ).run()
      
      // 2b. Universal ì œì¶œ ì´ë²¤íŠ¸ ì €ì¥
      if (Object.keys(universalAnswers).length > 0) {
        await db.prepare(`
          INSERT INTO raw_events (user_id, session_id, event_type, payload_json)
          VALUES (?, ?, 'UNIVERSAL_SUBMITTED', ?)
        `).bind(
          v3Payload.user_id || null,
          v3Payload.session_id,
          JSON.stringify(universalAnswers)
        ).run()
        
        // Universal answers â†’ facts ì €ì¥
        await saveUniversalFacts(db, v3Payload.session_id, v3Payload.user_id, universalAnswers, stage)
        
        // ============================================
        // Conversation Turns ì €ì¥ (P1 ê¸°ëŠ¥)
        // ============================================
        try {
          let turnNumber = await getNextTurnNumber(db, v3Payload.session_id)
          
          for (const [questionId, answer] of Object.entries(universalAnswers)) {
            if (answer === null || answer === undefined) continue
            
            const answerStr = Array.isArray(answer) ? answer.join(', ') : String(answer)
            const signals = extractSignalsFromAnswer(answerStr)
            
            await saveConversationTurn(db, {
              session_id: v3Payload.session_id,
              user_id: v3Payload.user_id,
              turn_number: turnNumber,
              turn_type: 'universal_intake',
              question_id: questionId,
              answer_raw: answerStr,
              answer_type: Array.isArray(answer) ? 'multi_choice' : 'text',
              extracted_signals: signals,
            })
            turnNumber++
          }
          console.log(`[V3 Analyze] Saved ${Object.keys(universalAnswers).length} conversation turns`)
        } catch (turnError) {
          // ëŒ€í™” í„´ ì €ì¥ ì‹¤íŒ¨í•´ë„ ë¶„ì„ì€ ê³„ì† ì§„í–‰ (graceful degradation)
          console.error('[V3 Analyze] Conversation turn save failed:', turnError)
        }
      }
    } else {
      // V2: ê¸°ì¡´ ë¡œì§
      const v2Payload = rawPayload as AnalysisRequestPayloadV2
      if (!v2Payload.profile) {
        logError('VALIDATION_ERROR', 'profile is required for V2 payload', { session_id: rawPayload.session_id })
        return c.json(createErrorResponse('VALIDATION_ERROR', 'profile is required for V2 payload'), 400)
      }
      
      // 2. Raw event ì €ì¥ (V2)
      await db.prepare(`
        INSERT INTO raw_events (user_id, session_id, event_type, payload_json, client_meta)
        VALUES (?, ?, 'ANALYSIS_REQUESTED', ?, ?)
      `).bind(
        v2Payload.user_id || null,
        v2Payload.session_id,
        JSON.stringify(v2Payload),
        c.req.header('User-Agent') || null
      ).run()
    }
    
    // 3. Deep Intake ì²˜ë¦¬ (V2/V3 ê³µí†µ)
    let normalizedDeepIntake: NormalizedDeepIntake | undefined
    const deepIntake = (rawPayload as any).deep_intake as DeepIntakeInput | undefined
    
    if (deepIntake) {
      // [ìˆ˜ì •ì‚¬í•­ 3] ë¯¸ì„±ë…„ ë‹¨ê³„ì—ì„œëŠ” Deep Intake ì„œì‚¬í˜• ì§ˆë¬¸ ì œí•œ
      if (stage && isMinorStage(stage)) {
        // ë¯¸ì„±ë…„ ë‹¨ê³„: MBTI, priorityë§Œ í—ˆìš©, ì˜¤í”ˆí…ìŠ¤íŠ¸ëŠ” ë¬´ì‹œ
        const sanitizedDeepIntake: DeepIntakeInput = {
          mbti: deepIntake.mbti,
          priority_top1: deepIntake.priority_top1,
          // best_moment, worst_moment, change_reasonì€ ë¬´ì‹œ
        }
        normalizedDeepIntake = normalizeDeepIntake(sanitizedDeepIntake)
      } else {
        normalizedDeepIntake = normalizeDeepIntake(deepIntake)
      }
      
      // Deep Intake ì´ë²¤íŠ¸ ì €ì¥
      await db.prepare(`
        INSERT INTO raw_events (user_id, session_id, event_type, payload_json)
        VALUES (?, ?, 'DEEP_INTAKE_SUBMITTED', ?)
      `).bind(
        rawPayload.user_id || null,
        rawPayload.session_id,
        JSON.stringify({ raw: deepIntake, normalized: normalizedDeepIntake })
      ).run()
      
      // Deep Intake â†’ facts ì €ì¥
      await saveDeepIntakeFacts(db, rawPayload.session_id, rawPayload.user_id, normalizedDeepIntake)
    }
    
    // 4. ê¸°ì¡´ facts ì¡°íšŒ (ì´ ì„¸ì…˜ì—ì„œ ìˆ˜ì§‘ëœ)
    const existingFacts = await db.prepare(`
      SELECT fact_key, value_json, confidence, fact_level
      FROM facts
      WHERE session_id = ?
      ORDER BY fact_level ASC, collected_at DESC
    `).bind(rawPayload.session_id).all<{
      fact_key: string
      value_json: string
      confidence: number
      fact_level: number
    }>()
    
    const facts = existingFacts.results || []
    
    // 5. ë¶„ì„ ìš”ì²­ ìƒì„± (ë²„ì „ ì ê¸ˆ)
    const analysisType = isV3 
      ? ((rawPayload as AnalysisRequestPayloadV3).analysis_type || 'job')
      : ((rawPayload as AnalysisRequestPayloadV2).analysis_type || 'job')
    
    const promptPayload = isV3
      ? JSON.stringify({ stage, universal_answers: universalAnswers })
      : JSON.stringify((rawPayload as AnalysisRequestPayloadV2).profile)
    
    const requestResult = await db.prepare(`
      INSERT INTO ai_analysis_requests (
        session_id, user_id, analysis_type, pricing_tier, prompt_payload,
        status, recipe_version, tagger_version, scoring_version
      )
      VALUES (?, ?, ?, ?, ?, 'processing', ?, ?, ?)
      RETURNING id
    `).bind(
      rawPayload.session_id,
      rawPayload.user_id || null,
      analysisType,
      (rawPayload as any).pricing_tier || 'free',
      promptPayload,
      VERSIONS.recipe,
      VERSIONS.tagger,
      VERSIONS.scoring
    ).first<{ id: number }>()
    
    if (!requestResult) {
      logError('DB_ERROR', 'Failed to create analysis request', { session_id: rawPayload.session_id })
      return c.json(createErrorResponse('DB_ERROR', 'Failed to create analysis request'), 500)
    }
    
    const requestId = requestResult.id
    
    // 6. ë¶„ì„ ì‹¤í–‰ (V3ëŠ” stage + debug ì „ë‹¬ + Vectorize í™•ì¥)
    const env = c.env as Bindings
    const result = await runAnalysisV3(
      db, 
      rawPayload, 
      requestId, 
      facts, 
      normalizedDeepIntake,
      stage,
      debugMode,
      { VECTORIZE: env.VECTORIZE, AI: env.AI }
    )
    
    // 7. ê²°ê³¼ ì €ì¥
    await db.prepare(`
      INSERT INTO ai_analysis_results (request_id, result_json)
      VALUES (?, ?)
    `).bind(requestId, JSON.stringify(result)).run()
    
    // 8. ìš”ì²­ ìƒíƒœ ì—…ë°ì´íŠ¸
    await db.prepare(`
      UPDATE ai_analysis_requests
      SET status = 'completed', processed_at = CURRENT_TIMESTAMP
      WHERE id = ?
    `).bind(requestId).run()
    
    // 9. ì™„ë£Œ ì´ë²¤íŠ¸ ì €ì¥
    await db.prepare(`
      INSERT INTO raw_events (user_id, session_id, event_type, payload_json)
      VALUES (?, ?, 'ANALYSIS_COMPLETED', ?)
    `).bind(
      rawPayload.user_id || null,
      rawPayload.session_id,
      JSON.stringify({ 
        request_id: requestId, 
        facts_applied: facts.length,
        has_deep_intake: !!normalizedDeepIntake,
        stage: stage || null,
        is_v3: isV3,
      })
    ).run()
    
    // 10. Phase 4 ë©”íŠ¸ë¦­ìŠ¤ ì´ë²¤íŠ¸ ì €ì¥
    await savePhase4MetricsEvents(db, rawPayload.session_id, rawPayload.user_id, requestId, result)
    
    return c.json({
      success: true,
      request_id: requestId,
      result,
      facts_applied: facts.length,
      deep_intake_processed: !!normalizedDeepIntake,
      stage: stage || null,
    })
    
  } catch (error) {
    const message = error instanceof Error ? error.message : 'Unknown error'
    logError('ANALYSIS_FAILED', message, { 
      session_id: rawPayload.session_id,
      stack: error instanceof Error ? error.stack : undefined
    })
    return c.json(createErrorResponse('ANALYSIS_FAILED', 'Analysis failed', { 
      message, 
      session_id: rawPayload.session_id 
    }), 500)
  }
})

// ============================================
// V3: Universal Answers â†’ facts ì €ì¥
// ============================================
async function saveUniversalFacts(
  db: D1Database,
  sessionId: string,
  userId: string | undefined,
  answers: UniversalAnswers,
  stage?: AnalysisStage
): Promise<void> {
  const UNIVERSAL_QUESTIONS_MAP = new Map(
    UNIVERSAL_QUESTIONS.map(q => [q.question_id, q])
  )
  
  // Universal ì œì•½ì¡°ê±´ â†’ confirmed_constraint ë§¤í•‘
  // ì‚¬ìš©ìê°€ "ì ˆëŒ€ ë¶ˆê°€"ë¡œ ì„ íƒí•œ ì œì•½ì€ Phase 4 Hard Filter ëŒ€ìƒ
  const CONSTRAINT_TO_CONFIRMED: Record<string, string> = {
    'no_overtime': 'work_hours_strict',
    'no_shift': 'shift_work_no',
    'no_travel': 'travel_impossible',
    'remote_only': 'remote_only',
    'no_degree': 'degree_impossible',
    'no_license': 'license_impossible',
  }
  
  for (const [questionId, answer] of Object.entries(answers)) {
    if (answer === null || answer === undefined) continue
    if (Array.isArray(answer) && answer.length === 0) continue
    if (typeof answer === 'string' && answer.trim() === '') continue
    
    const question = UNIVERSAL_QUESTIONS_MAP.get(questionId)
    if (!question) continue
    
    // ì •ê·œí™”
    const normalized = normalizeUniversalAnswer(question, answer)
    
    // fact_level ê²°ì •
    const factLevel = determineUniversalFactLevel(question.fact_key)
    
    await db.prepare(`
      INSERT INTO facts (session_id, user_id, fact_key, value_json, confidence, source_type, fact_level, question_id)
      VALUES (?, ?, ?, ?, 1.0, 'universal', ?, ?)
      ON CONFLICT(session_id, fact_key) DO UPDATE SET
        value_json = excluded.value_json,
        collected_at = CURRENT_TIMESTAMP
    `).bind(
      sessionId,
      userId || null,
      question.fact_key,
      JSON.stringify(normalized),
      factLevel,
      questionId
    ).run()
    
    // ğŸ”¥ Phase 4 Hard Filter ìë™ ìŠ¹ê²©
    // ì œì•½ì¡°ê±´ ì§ˆë¬¸(univ_constraint_*)ì´ë©´ confirmed_constraintë„ ì €ì¥
    if (questionId.startsWith('univ_constraint_')) {
      const values = Array.isArray(answer) ? answer : [answer]
      
      for (const value of values) {
        const confirmedType = CONSTRAINT_TO_CONFIRMED[value]
        if (confirmedType) {
          await db.prepare(`
            INSERT INTO facts (session_id, user_id, fact_key, value_json, confidence, source_type, fact_level)
            VALUES (?, ?, ?, ?, 1.0, 'universal', 1)
            ON CONFLICT(session_id, fact_key) DO UPDATE SET
              value_json = excluded.value_json,
              fact_level = 1,
              collected_at = CURRENT_TIMESTAMP
          `).bind(
            sessionId,
            userId || null,
            `confirmed_constraint.${confirmedType}`,
            JSON.stringify({ 
              confirmed: true, 
              source: 'universal_intake',
              original_value: value,
              confirmed_at: new Date().toISOString() 
            })
          ).run()
        }
      }
    }
  }
}

// Universal ë‹µë³€ ì •ê·œí™”
function normalizeUniversalAnswer(
  question: UniversalQuestion,
  answer: string | string[]
): { value: string | string[]; tags: string[]; raw?: string } {
  const values = Array.isArray(answer) ? answer : [answer]
  const tags: string[] = []
  
  // ì˜µì…˜ì—ì„œ íƒœê·¸ ì¶”ì¶œ
  if (question.options) {
    for (const val of values) {
      const option = question.options.find(o => o.value === val)
      if (option?.tags) {
        tags.push(...option.tags)
      }
    }
  }
  
  // í…ìŠ¤íŠ¸ ì •ê·œí™” (freetextìš©)
  if (question.normalize_rule === 'keywords' && typeof answer === 'string') {
    return {
      value: answer,
      tags: extractKeywordTags(answer),
      raw: answer,
    }
  }
  
  return {
    value: answer,
    tags: [...new Set(tags)],
  }
}

// í‚¤ì›Œë“œ ê¸°ë°˜ íƒœê·¸ ì¶”ì¶œ (freetextìš©)
function extractKeywordTags(text: string): string[] {
  const tags: string[] = []
  const lowerText = text.toLowerCase()
  
  // ê°„ë‹¨í•œ í‚¤ì›Œë“œ ë§¤ì¹­
  const keywordMap: Record<string, string[]> = {
    'ê±´ê°•': ['health'],
    'ì•„í”„': ['health'],
    'ëŒë´„': ['caregiving'],
    'ê°€ì¡±': ['caregiving', 'family'],
    'ì•„ì´': ['caregiving'],
    'ì›ê²©': ['remote'],
    'ì¬íƒ': ['remote'],
    'ì•¼ê·¼': ['work_hours_strict'],
    'ì €ë…': ['work_hours_strict', 'wlb'],
    'ì¶œí‡´ê·¼': ['commute'],
  }
  
  for (const [keyword, keywordTags] of Object.entries(keywordMap)) {
    if (lowerText.includes(keyword)) {
      tags.push(...keywordTags)
    }
  }
  
  return [...new Set(tags)]
}

// Universal fact_level ê²°ì •
function determineUniversalFactLevel(factKey: string): number {
  if (factKey.startsWith('profile.constraints')) return 2
  if (factKey.startsWith('priority.')) return 2
  if (factKey.startsWith('profile.life_constraint')) return 2
  if (factKey.startsWith('profile.')) return 4
  if (factKey.startsWith('discovery.')) return 3
  return 4
}

// ============================================
// Phase 1C: Deep Intake â†’ facts ì €ì¥
// ============================================
async function saveDeepIntakeFacts(
  db: D1Database,
  sessionId: string,
  userId: string | undefined,
  normalized: NormalizedDeepIntake
): Promise<void> {
  // MBTI ì €ì¥
  if (normalized.mbti) {
    await db.prepare(`
      INSERT INTO facts (session_id, user_id, fact_key, value_json, confidence, source_type, fact_level)
      VALUES (?, ?, 'profile.mbti', ?, 1.0, 'deep_intake', 4)
      ON CONFLICT(session_id, fact_key) DO UPDATE SET
        value_json = excluded.value_json,
        collected_at = CURRENT_TIMESTAMP
    `).bind(
      sessionId,
      userId || null,
      JSON.stringify({ value: normalized.mbti, traits: normalized.mbti_traits })
    ).run()
  }
  
  // best_moment ì €ì¥
  if (normalized.best_moment) {
    await db.prepare(`
      INSERT INTO facts (session_id, user_id, fact_key, value_json, confidence, source_type, fact_level)
      VALUES (?, ?, 'discovery.best_moment', ?, 0.9, 'deep_intake', 3)
      ON CONFLICT(session_id, fact_key) DO UPDATE SET
        value_json = excluded.value_json,
        collected_at = CURRENT_TIMESTAMP
    `).bind(
      sessionId,
      userId || null,
      JSON.stringify(normalized.best_moment)
    ).run()
  }
  
  // worst_moment ì €ì¥
  if (normalized.worst_moment) {
    await db.prepare(`
      INSERT INTO facts (session_id, user_id, fact_key, value_json, confidence, source_type, fact_level)
      VALUES (?, ?, 'discovery.worst_moment', ?, 0.9, 'deep_intake', 3)
      ON CONFLICT(session_id, fact_key) DO UPDATE SET
        value_json = excluded.value_json,
        collected_at = CURRENT_TIMESTAMP
    `).bind(
      sessionId,
      userId || null,
      JSON.stringify(normalized.worst_moment)
    ).run()
  }
  
  // change_reason ì €ì¥
  if (normalized.change_reason) {
    await db.prepare(`
      INSERT INTO facts (session_id, user_id, fact_key, value_json, confidence, source_type, fact_level)
      VALUES (?, ?, 'motivation.change_reason', ?, 0.9, 'deep_intake', 3)
      ON CONFLICT(session_id, fact_key) DO UPDATE SET
        value_json = excluded.value_json,
        collected_at = CURRENT_TIMESTAMP
    `).bind(
      sessionId,
      userId || null,
      JSON.stringify(normalized.change_reason)
    ).run()
  }
  
  // priority.top1 ì €ì¥
  if (normalized.priority_top1) {
    await db.prepare(`
      INSERT INTO facts (session_id, user_id, fact_key, value_json, confidence, source_type, fact_level)
      VALUES (?, ?, 'priority.top1', ?, 1.0, 'deep_intake', 2)
      ON CONFLICT(session_id, fact_key) DO UPDATE SET
        value_json = excluded.value_json,
        collected_at = CURRENT_TIMESTAMP
    `).bind(
      sessionId,
      userId || null,
      JSON.stringify({ value: normalized.priority_top1 })
    ).run()
  }
}

// ============================================
// POST /followup - Follow-up ì‘ë‹µ ì²˜ë¦¬ (facts ì €ì¥)
// ============================================
analyzerRoutes.post('/followup', async (c) => {
  const db = c.env.DB
  const payload = await c.req.json<FollowupPayloadV2>()
  
  try {
    // 1. í•„ìˆ˜ í•„ë“œ ê²€ì¦
    if (!payload.session_id || !payload.question_id || payload.answer === undefined) {
      logError('VALIDATION_ERROR', 'session_id, question_id, and answer are required')
      return c.json(createErrorResponse('VALIDATION_ERROR', 'session_id, question_id, and answer are required'), 400)
    }
    
    // 2. Raw event ì €ì¥ (ì›ë¬¸ ê·¸ëŒ€ë¡œ)
    await db.prepare(`
      INSERT INTO raw_events (user_id, session_id, event_type, payload_json)
      VALUES (?, ?, 'FOLLOWUP_ANSWERED', ?)
    `).bind(
      payload.user_id || null,
      payload.session_id,
      JSON.stringify(payload)
    ).run()
    
    // ============================================
    // Conversation Turn ì €ì¥ (P1 ê¸°ëŠ¥)
    // ============================================
    try {
      const turnNumber = await getNextTurnNumber(db, payload.session_id)
      const answerStr = payload.answer_text || String(payload.answer)
      const signals = extractSignalsFromAnswer(answerStr)
      
      // FollowupV3 íƒ€ì…ì¸ì§€ í™•ì¸ (purpose ê¸°ë°˜)
      const turnType: TurnType = payload.question_type ? 
        (['contradiction_resolver', 'decision_variable', 'reality_constraint'].includes(payload.question_type) 
          ? 'followup_v3' 
          : 'followup_v2') 
        : 'followup_v2'
      
      await saveConversationTurn(db, {
        session_id: payload.session_id,
        user_id: payload.user_id,
        request_id: payload.request_id,
        turn_number: turnNumber,
        turn_type: turnType,
        question_id: payload.question_id,
        question_type: payload.question_type as any,
        answer_raw: answerStr,
        answer_type: payload.answer_text ? 'text' : 'single_choice',
        extracted_signals: signals,
        affected_dimensions: payload.affects_attributes,
      })
      console.log(`[Followup] Saved conversation turn #${turnNumber} for session: ${payload.session_id}`)
    } catch (turnError) {
      // ëŒ€í™” í„´ ì €ì¥ ì‹¤íŒ¨í•´ë„ followup ì²˜ë¦¬ëŠ” ê³„ì† ì§„í–‰
      console.error('[Followup] Conversation turn save failed:', turnError)
    }
    
    // 3. fact_keyì™€ value ê²°ì •
    const factKey = payload.fact_key || `answer.${payload.question_id}`
    const factLevel = determineFctLevel(factKey)
    
    // 4. facts í…Œì´ë¸”ì— ì €ì¥ (UPSERT - ê°™ì€ session+fact_keyë©´ ê°±ì‹ )
    await db.prepare(`
      INSERT INTO facts (
        session_id, user_id, fact_key, value_json, confidence, question_id, 
        source_type, fact_level
      )
      VALUES (?, ?, ?, ?, ?, ?, 'followup', ?)
      ON CONFLICT(session_id, fact_key) DO UPDATE SET
        value_json = excluded.value_json,
        confidence = excluded.confidence,
        collected_at = CURRENT_TIMESTAMP
    `).bind(
      payload.session_id,
      payload.user_id || null,
      factKey,
      JSON.stringify({ value: payload.answer, raw: payload.answer_text }),
      payload.confidence || 1.0,
      payload.question_id,
      factLevel
    ).run()
    
    // 5. question_history ì—…ë°ì´íŠ¸
    await db.prepare(`
      INSERT INTO question_history (session_id, question_id, question_type, attribute, answer_value)
      VALUES (?, ?, ?, ?, ?)
      ON CONFLICT(session_id, question_id) DO UPDATE SET
        answered_at = CURRENT_TIMESTAMP,
        answer_value = excluded.answer_value
    `).bind(
      payload.session_id,
      payload.question_id,
      payload.question_type || 'unknown',
      payload.attribute || null,
      payload.answer
    ).run()
    
    // 6. ê¸°ì¡´ constraint ê¸°ë°˜ followup ì²˜ë¦¬ (backward compatibility)
    if (payload.constraint && payload.request_id) {
      const constraint = assertConstraintType(payload.constraint)
      
      await db.prepare(`
        INSERT INTO followup_responses (
          request_id, question_id, constraint_type, job_id, job_name, answer
        )
        VALUES (?, ?, ?, ?, ?, ?)
      `).bind(
        payload.request_id,
        payload.question_id,
        constraint,
        payload.job_id || null,
        payload.job_name || null,
        payload.answer
      ).run()
      
      if (payload.answer === 'no') {
        await db.prepare(`
          INSERT OR IGNORE INTO confirmed_constraints (
            session_id, user_id, request_id, constraint_type, constraint_value
          )
          VALUES (?, ?, ?, ?, ?)
        `).bind(
          payload.session_id,
          payload.user_id || null,
          payload.request_id,
          constraint,
          'true'
        ).run()
      }
    }
    
    // 7. Phase 4: answer="no"ë©´ Safe Replacement ì²˜ë¦¬
    if (payload.answer === 'no' && payload.job_id && payload.request_id) {
      // í˜„ì¬ ê²°ê³¼ì—ì„œ í›„ë³´êµ°ê³¼ TOP3 ê°€ì ¸ì˜¤ê¸°
      const existingResult = await db.prepare(`
        SELECT result_json FROM ai_analysis_results WHERE request_id = ?
      `).bind(payload.request_id).first<{ result_json: string }>()
      
      if (existingResult) {
        try {
          const resultData = JSON.parse(existingResult.result_json)
          
          // TOP3ì™€ ì „ì²´ í›„ë³´êµ° êµ¬ì„±
          const originalTop3: ScoredJob[] = (resultData.fit_top3 || []).map((j: any) => ({
            job_id: j.job_id,
            job_name: j.job_name,
            scores: {
              fit: j.fit_score || 0,
              like: j.like_score || 0,
              can: j.can_score || 0,
              risk_penalty: 0,
            },
            attributes: {},
          }))
          
          // ì „ì²´ í›„ë³´ëŠ” like_top10ê³¼ can_top10 í•©ì³ì„œ êµ¬ì„± (dedupe)
          const allJobIds = new Set<string>()
          const allCandidates: ScoredJob[] = []
          
          for (const source of [resultData.fit_top3, resultData.like_top10, resultData.can_top10]) {
            if (!source) continue
            for (const j of source) {
              if (!allJobIds.has(j.job_id)) {
                allJobIds.add(j.job_id)
                allCandidates.push({
                  job_id: j.job_id,
                  job_name: j.job_name,
                  scores: {
                    fit: j.fit_score || j.like_score || 0,
                    like: j.like_score || 0,
                    can: j.can_score || 0,
                    risk_penalty: 0,
                  },
                  attributes: {},
                })
              }
            }
          }
          
          // constraint íƒ€ì… ê²°ì •
          const constraintType = payload.constraint || factKey.replace('answer.', '')
          
          // Phase 4 Safe Replacement ì‹¤í–‰
          const safeResult = await handleFollowupNo(
            db,
            payload.session_id,
            payload.user_id,
            payload.question_id,
            constraintType,
            payload.job_id,
            allCandidates,
            originalTop3,
            payload.request_id
          )
          
          // Phase 4 ê²°ê³¼ ë°˜í™˜
          return c.json({
            success: true,
            phase4_applied: true,
            fact_saved: {
              fact_key: factKey,
              value: payload.answer,
              fact_level: factLevel,
            },
            ...safeResult,
          })
          
        } catch (error) {
          console.error('Phase 4 Safe Replacement error:', error)
          // Phase 4 ì‹¤íŒ¨í•´ë„ ê¸°ë³¸ ì‘ë‹µì€ ë°˜í™˜
        }
      }
    }
    
    // 8. ì‘ë‹µ êµ¬ì„± (Phase 4 ë¯¸ì ìš© ì‹œ)
    const result: FollowupResultV2 = {
      success: true,
      fact_saved: {
        fact_key: factKey,
        value: payload.answer,
        fact_level: factLevel,
      },
      reanalyze_available: true,
      message: 'ë‹µë³€ì´ ì €ì¥ë˜ì—ˆìŠµë‹ˆë‹¤. ì¬ë¶„ì„ ì‹œ ë°˜ì˜ë©ë‹ˆë‹¤.',
    }
    
    return c.json(result)
    
  } catch (error) {
    const message = error instanceof Error ? error.message : 'Unknown error'
    logError('ANALYSIS_FAILED', `Followup failed: ${message}`, {
      stack: error instanceof Error ? error.stack : undefined
    })
    return c.json(createErrorResponse('ANALYSIS_FAILED', 'Followup failed', { message }), 500)
  }
})

// ============================================
// GET /result/:requestId - ê²°ê³¼ ì¡°íšŒ
// ============================================
analyzerRoutes.get('/result/:requestId', async (c) => {
  const db = c.env.DB
  const requestId = parseInt(c.req.param('requestId'), 10)
  
  const providedId = c.req.param('requestId')
  if (isNaN(requestId)) {
    logError('VALIDATION_ERROR', 'Invalid request_id', { provided: providedId })
    return c.json(createErrorResponse('VALIDATION_ERROR', 'Invalid request_id', { provided: providedId }), 400)
  }
  
  try {
    const request = await db.prepare(`
      SELECT * FROM ai_analysis_requests WHERE id = ?
    `).bind(requestId).first()
    
    if (!request) {
      logError('REQUEST_NOT_FOUND', 'Analysis request not found', { request_id: requestId })
      return c.json(createErrorResponse('REQUEST_NOT_FOUND', 'Analysis request not found', { request_id: requestId }), 404)
    }
    
    const result = await db.prepare(`
      SELECT * FROM ai_analysis_results WHERE request_id = ?
    `).bind(requestId).first<{ result_json: string }>()
    
    const followups = await db.prepare(`
      SELECT * FROM followup_responses WHERE request_id = ?
    `).bind(requestId).all()
    
    return c.json({
      request,
      result: result ? JSON.parse(result.result_json) : null,
      followups: followups.results,
    })
    
  } catch (error) {
    const message = error instanceof Error ? error.message : 'Unknown error'
    logError('DB_ERROR', `Failed to fetch result: ${message}`, {
      request_id: requestId,
      stack: error instanceof Error ? error.stack : undefined
    })
    return c.json(createErrorResponse('DB_ERROR', 'Failed to fetch result', { 
      message, 
      request_id: requestId 
    }), 500)
  }
})

// ============================================
// GET /session/:sessionId - ì„¸ì…˜ ì´ë ¥ ì¡°íšŒ
// ============================================
analyzerRoutes.get('/session/:sessionId', async (c) => {
  const db = c.env.DB
  const sessionId = c.req.param('sessionId')
  
  try {
    const events = await db.prepare(`
      SELECT * FROM raw_events 
      WHERE session_id = ?
      ORDER BY created_at ASC
    `).bind(sessionId).all()
    
    const requests = await db.prepare(`
      SELECT * FROM ai_analysis_requests
      WHERE session_id = ?
      ORDER BY requested_at ASC
    `).bind(sessionId).all()
    
    const constraints = await db.prepare(`
      SELECT * FROM confirmed_constraints
      WHERE session_id = ?
    `).bind(sessionId).all()
    
    // Phase 1A: facts ì¡°íšŒ ì¶”ê°€
    const facts = await db.prepare(`
      SELECT * FROM facts
      WHERE session_id = ?
      ORDER BY fact_level ASC, collected_at DESC
    `).bind(sessionId).all()
    
    return c.json({
      session_id: sessionId,
      events: events.results,
      requests: requests.results,
      confirmed_constraints: constraints.results,
      facts: facts.results,  // Phase 1A ì¶”ê°€
    })
    
  } catch (error) {
    const message = error instanceof Error ? error.message : 'Unknown error'
    logError('DB_ERROR', `Failed to fetch session: ${message}`, {
      session_id: sessionId,
      stack: error instanceof Error ? error.stack : undefined
    })
    return c.json(createErrorResponse('DB_ERROR', 'Failed to fetch session', { 
      message, 
      session_id: sessionId 
    }), 500)
  }
})

// ============================================
// GET /facts/:sessionId - ì„¸ì…˜ì˜ factsë§Œ ì¡°íšŒ
// ============================================
analyzerRoutes.get('/facts/:sessionId', async (c) => {
  const db = c.env.DB
  const sessionId = c.req.param('sessionId')
  
  try {
    const facts = await db.prepare(`
      SELECT * FROM facts
      WHERE session_id = ?
      ORDER BY fact_level ASC, collected_at DESC
    `).bind(sessionId).all()
    
    return c.json({
      session_id: sessionId,
      facts: facts.results,
      count: facts.results?.length || 0,
    })
    
  } catch (error) {
    logError('DB_ERROR', 'Failed to fetch facts', { session_id: sessionId })
    return c.json(createErrorResponse('DB_ERROR', 'Failed to fetch facts', { session_id: sessionId }), 500)
  }
})

// ============================================
// Analysis Logic (Phase 1A MVE)
// ============================================

interface Fact {
  fact_key: string
  value_json: string
  confidence: number
  fact_level: number
}

async function runAnalysis(
  db: D1Database,
  payload: AnalysisRequestPayloadV2,
  requestId: number,
  facts: Fact[],
  deepIntake?: NormalizedDeepIntake
): Promise<AnalysisResultJSON> {
  // 1. job_attributesì—ì„œ ì‹¤ì œ ë°ì´í„° ì¡°íšŒ (Phase 1B)
  const jobAttrs = await db.prepare(`
    SELECT 
      job_id, job_name,
      wlb, growth, stability, income,
      teamwork, solo_deep, analytical, creative, execution, people_facing,
      work_hours, shift_work, travel, remote_possible,
      degree_required, license_required
    FROM job_attributes
    WHERE tagger_version = 'tagger-v1.0.0'
    LIMIT 80
  `).all<{
    job_id: string
    job_name: string
    wlb: number
    growth: number
    stability: number
    income: number
    teamwork: number
    solo_deep: number
    analytical: number
    creative: number
    execution: number
    people_facing: number
    work_hours: string
    shift_work: string
    travel: string
    remote_possible: string
    degree_required: string
    license_required: string
  }>()
  
  // DBì— ë°ì´í„°ê°€ ì—†ìœ¼ë©´ ìƒ˜í”Œ ì‚¬ìš©
  const sampleJobs = (jobAttrs.results && jobAttrs.results.length > 0)
    ? jobAttrs.results.map(j => ({
        job_id: j.job_id,
        job_name: j.job_name,
        base_like: Math.round((j.wlb + j.growth + j.stability + j.income) / 4),
        // ì—°êµ¬ì§ ê³¼ë„ ëŒ€í‘œ ë°©ì§€ë¥¼ ìœ„í•´ analytical ê°€ì¤‘ì¹˜ ì¡°ì • (0.7ë°°)
        base_can: Math.round((j.teamwork + (j.analytical * 0.7) + j.creative) / 3),
        base_risk: 10,  // ê¸°ë³¸ risk
        attributes: {
          wlb: j.wlb,
          growth: j.growth,
          stability: j.stability,
          income: j.income,
          remote: j.remote_possible === 'full' ? 100 : j.remote_possible === 'partial' ? 50 : 0,
          solo_work: j.solo_deep,
          people_facing: j.people_facing,
          analytical: j.analytical,
          creative: j.creative,
        },
      }))
    : getSampleJobs()
  
  // 2. Fact boosts ê³„ì‚°
  const factBoosts = calculateFactBoosts(facts)
  
  // 3. ê° ì§ì—…ì— ì ìˆ˜ ì ìš©
  const scoredJobs: ScoredJob[] = sampleJobs.map(job => {
    const baseScores: JobScores = {
      like: job.base_like,
      can: job.base_can,
      risk_penalty: job.base_risk,
    }
    
    const adjusted = applyFactBoostsToJob(baseScores, job.attributes, factBoosts)
    const fit = Math.round(0.5 * adjusted.like + 0.5 * adjusted.can - adjusted.risk_penalty)
    
    return {
      job_id: job.job_id,
      job_name: job.job_name,
      scores: {
        fit: Math.max(0, fit),
        like: adjusted.like,
        can: adjusted.can,
        risk_penalty: adjusted.risk_penalty,
      },
      attributes: job.attributes,
    }
  })
  
  // 4. ì •ë ¬ (Fit ê¸°ì¤€)
  scoredJobs.sort((a, b) => b.scores.fit - a.scores.fit)
  
  const top10 = scoredJobs.slice(0, 10)
  
  // Phase 4: Diversity Guard (ë‚´ë¶€ì—ì„œ Research Bias Capë„ ì ìš©)
  const rawTop3 = top10.slice(0, 3)
  const diversityResult = applyDiversityGuard(rawTop3, scoredJobs)
  const top3 = diversityResult.adjusted
  
  // 5. Follow-up ì§ˆë¬¸ ìƒì„± (Deep Intake ì—¬ë¶€ ë°˜ì˜)
  const existingFactKeys = facts.map(f => ({ fact_key: f.fact_key }))
  const followupQuestions = generateFollowupQuestions({
    candidates: scoredJobs,
    topK: top10,
    existingFacts: existingFactKeys,
    hasDeepIntake: !!deepIntake,  // Phase 1C: Deep Intake ì—¬ë¶€ ì „ë‹¬
  }, 3)
  
  // 6. Phase 1C: User Insight ìƒì„±
  const userInsight = generateUserInsight(facts, deepIntake, factBoosts.applied_rules)
  
  // 7. ê²°ê³¼ êµ¬ì„±
  const result: AnalysisResultJSON = {
    engine_state: deepIntake ? 'phase1a_mve' : (facts.length > 0 ? 'phase1a_mve' : 'phase1a_initial'),
    
    versions: {
      recipe: VERSIONS.recipe,
      tagger: VERSIONS.tagger,
      scoring: VERSIONS.scoring,
    },
    
    input_summary: {
      profile_revision_id: `rev-${requestId}-${Date.now()}`,
      key_interests: payload.profile.interest.keywords.slice(0, 3),
      key_skills: payload.profile.skill.map(s => s.name).slice(0, 3),
      non_negotiables: Object.entries(payload.profile.constraints || {})
        .filter(([_, v]) => v === true)
        .map(([k]) => k),
      preferences: [],
      facts_applied: facts.length,
      applied_rules: factBoosts.applied_rules,
      // Phase 1C: Deep Intake ì •ë³´ ì¶”ê°€
      deep_intake_provided: !!deepIntake,
      insight_tags: userInsight?.key_traits.map(t => t.trait) || [],
    },
    
    fit_top3: top3.map(j => {
      // Evidence ìƒì„± (facts + job attributes ê¸°ë°˜)
      // âš ï¸ EvidenceëŠ” "ì„¤ëª…ìš©"ì´ë©° ì ìˆ˜ ê³„ì‚°ì— ì˜í–¥ ì—†ìŒ
      const evidenceFacts: EvidenceFact[] = facts.map(f => ({
        fact_key: f.fact_key,
        value_json: f.value_json,
      }))
      let evidenceLinks = buildEvidenceLinks(evidenceFacts, j)
      
      // Evidenceê°€ ë¶€ì¡±í•˜ë©´ ê¸°ë³¸ Evidence ì¶”ê°€
      if (evidenceLinks.length < 3) {
        const defaultEvidence = generateDefaultEvidence(j)
        evidenceLinks = [...evidenceLinks, ...defaultEvidence].slice(0, 10)
      }
      
      return {
        job_id: j.job_id,
        job_name: j.job_name,
        fit_score: j.scores.fit,
        like_score: j.scores.like,
        can_score: j.scores.can,
        risk_details: [],
        evidence_links: evidenceLinks,
      }
    }),
    
    like_top10: top10.map(j => ({
      job_id: j.job_id,
      job_name: j.job_name,
      like_score: j.scores.like,
    })),
    
    can_top10: top10.map(j => ({
      job_id: j.job_id,
      job_name: j.job_name,
      can_score: j.scores.can,
    })),
    
    caution_jobs: scoredJobs
      .filter(j => j.scores.risk_penalty > 20)
      .slice(0, 5)
      .map(j => ({
        job_id: j.job_id,
        job_name: j.job_name,
        risk_penalty: j.scores.risk_penalty,
      })),
    
    // Phase 1A: followup_questions ì¶”ê°€!
    followup_questions: followupQuestions,
    
    ux_flags: {
      has_caution_in_top3: top3.some(j => j.scores.risk_penalty > 20),
      has_unknown_in_top3: false,
      needs_followup_question: followupQuestions.length > 0,
      counts_before_filter: {
        safe_known: scoredJobs.filter(j => j.scores.risk_penalty <= 10).length,
        safe_unknown: scoredJobs.filter(j => j.scores.risk_penalty > 10 && j.scores.risk_penalty <= 20).length,
        caution: scoredJobs.filter(j => j.scores.risk_penalty > 20).length,
        total: scoredJobs.length,
      },
      counts_after_filter: {
        safe_known: scoredJobs.filter(j => j.scores.risk_penalty <= 10).length,
        safe_unknown: scoredJobs.filter(j => j.scores.risk_penalty > 10 && j.scores.risk_penalty <= 20).length,
        caution: scoredJobs.filter(j => j.scores.risk_penalty > 20).length,
        total: scoredJobs.length,
      },
    },
    
    llm_explanation: generateExplanation(facts, factBoosts.applied_rules, top3[0]?.job_name),
    generated_at: new Date().toISOString(),
    total_candidates: scoredJobs.length,
    
    // Phase 1C: User Insight ì¶”ê°€
    user_insight: userInsight,
    
    // Phase 4: Diversity Guard ì •ë³´
    phase4_applied: true,
    diversity_guard_active: diversityResult.diversityApplied,
    diversity_changes: diversityResult.changes,
  }
  
  return result
}

// ============================================
// Explainability V3: ì§ì—…ë³„ ì¶”ì²œ ê·¼ê±° ìƒì„±
// ============================================
interface JobExplanation {
  like_reason: string    // Like ì ìˆ˜ ê·¼ê±°
  can_reason: string     // Can ì ìˆ˜ ê·¼ê±°
  risk_warning: string   // Risk ê²½ê³  (ìˆìœ¼ë©´)
}

function generateJobExplanation(
  job: ScoredJob,
  facts: Fact[],
  appliedRules: string[]
): JobExplanation {
  // Like ê·¼ê±° (interest/values ë§¤ì¹­)
  let likeReason = 'ê¸°ë³¸ ì í•©ë„ ê¸°ë°˜ ì¶”ì²œ'
  const interestFacts = facts.filter(f => 
    f.fact_key.includes('interest') || f.fact_key.includes('priority')
  )
  if (interestFacts.length > 0) {
    try {
      const firstInterest = JSON.parse(interestFacts[0].value_json)
      const interestValue = Array.isArray(firstInterest.value) 
        ? firstInterest.value[0] 
        : firstInterest.value
      if (interestValue) {
        likeReason = `"${interestValue}" ê´€ì‹¬ì‚¬ì™€ ë§¤ì¹­`
      }
    } catch { /* ignore */ }
  }
  
  // ê·œì¹™ ê¸°ë°˜ ë³´ì •
  if (appliedRules.includes('profile.interest.keywords')) {
    likeReason = 'ê´€ì‹¬ í‚¤ì›Œë“œ ê¸°ë°˜ ë§¤ì¹­'
  }
  if (appliedRules.includes('priority.top1')) {
    const priorityFact = facts.find(f => f.fact_key === 'priority.top1')
    if (priorityFact) {
      try {
        const pv = JSON.parse(priorityFact.value_json)
        const priorityMap: Record<string, string> = {
          growth: 'ì„±ì¥ ê°€ëŠ¥ì„±', wlb: 'ì›Œë¼ë°¸', income: 'ìˆ˜ì…', stability: 'ì•ˆì •ì„±'
        }
        likeReason = `${priorityMap[pv.value] || pv.value} ìš°ì„ ìˆœìœ„ ë°˜ì˜`
      } catch { /* ignore */ }
    }
  }
  
  // Can ê·¼ê±° (skill/degree/license ê´€ë ¨)
  let canReason = 'ê¸°ë³¸ ì—­ëŸ‰ ì í•©ë„ ê¸°ë°˜'
  const constraintFacts = facts.filter(f => 
    f.fact_key.includes('constraint') || f.fact_key.includes('qualification')
  )
  if (constraintFacts.length === 0) {
    canReason = 'íŠ¹ë³„í•œ ìê²© ìš”ê±´ ì—†ìŒ'
  } else {
    canReason = 'ì…ë ¥ëœ ì œì•½ ì¡°ê±´ê³¼ í˜¸í™˜'
  }
  
  // workstyle ë°˜ì˜
  if (appliedRules.includes('profile.workstyle.social')) {
    const styleFact = facts.find(f => f.fact_key === 'profile.workstyle.social')
    if (styleFact) {
      try {
        const sv = JSON.parse(styleFact.value_json)
        const styleMap: Record<string, string> = {
          solo: 'ë…ë¦½ì  ì—…ë¬´ í™˜ê²½', team: 'íŒ€ í˜‘ì—… í™˜ê²½', balanced: 'ê· í˜• ì¡íŒ ì—…ë¬´ í™˜ê²½'
        }
        canReason = `${styleMap[sv.value] || 'ì„ í˜¸'} ì—…ë¬´ ìŠ¤íƒ€ì¼ ë¶€í•©`
      } catch { /* ignore */ }
    }
  }
  
  // Risk ê²½ê³  (confirmed_constraint ì¶©ëŒ ì²´í¬)
  let riskWarning = ''
  const confirmedConstraints = facts.filter(f => f.fact_key.startsWith('confirmed_constraint.'))
  if (confirmedConstraints.length > 0) {
    // ì œì•½ì´ ìˆì§€ë§Œ ì´ ì§ì—…ì´ ì¶”ì²œëë‹¤ë©´ í†µê³¼í•œ ê²ƒ
    riskWarning = 'ì œì•½ ì¡°ê±´ ì¶©ì¡±'
  }
  if (job.scores.risk_penalty > 15) {
    riskWarning = 'ì¼ë¶€ ì£¼ì˜ í•„ìš” (ìƒì„¸ ì •ë³´ í™•ì¸ ê¶Œì¥)'
  } else if (job.scores.risk_penalty > 10) {
    riskWarning = 'ê²½ë¯¸í•œ ë¶ˆí™•ì‹¤ì„± ìˆìŒ'
  }
  if (!riskWarning) {
    riskWarning = 'íŠ¹ë³„í•œ ìœ„í—˜ ìš”ì†Œ ì—†ìŒ'
  }
  
  return {
    like_reason: likeReason,
    can_reason: canReason,
    risk_warning: riskWarning,
  }
}

// ============================================
// Phase 1C: User Insight ìƒì„±
// ============================================
function generateUserInsight(
  facts: Fact[],
  deepIntake?: NormalizedDeepIntake,
  appliedRules?: string[]
): UserInsight | undefined {
  if (!deepIntake && facts.length === 0) {
    return undefined
  }
  
  const keyTraits: UserInsight['key_traits'] = []
  const appliedFacts: UserInsight['applied_facts'] = []
  
  // MBTI ê¸°ë°˜ ì¸ì‚¬ì´íŠ¸
  if (deepIntake?.mbti && deepIntake.mbti_traits) {
    const workStyles = deepIntake.mbti_traits.workStyles
    
    if (workStyles.includes('solo_deep')) {
      keyTraits.push({
        trait: 'í˜¼ì ê¹Šê²Œ íŒŒê¸°ë¥¼ ì„ í˜¸',
        evidence: `MBTI ${deepIntake.mbti}ì˜ íŠ¹ì„±`,
        score_impact: 'ë¶„ì„/ì—°êµ¬ ì¤‘ì‹¬ ì§ì—…ì— +5~10 ë¶€ìŠ¤íŠ¸',
      })
    }
    if (workStyles.includes('team_collab')) {
      keyTraits.push({
        trait: 'íŒ€ í˜‘ì—…ì„ ì„ í˜¸',
        evidence: `MBTI ${deepIntake.mbti}ì˜ íŠ¹ì„±`,
        score_impact: 'í˜‘ì—… ì¤‘ì‹¬ ì§ì—…ì— +5~10 ë¶€ìŠ¤íŠ¸',
      })
    }
    if (workStyles.includes('creative')) {
      keyTraits.push({
        trait: 'ì°½ì˜ì ì¸ ì—…ë¬´ ì„ í˜¸',
        evidence: `MBTI ${deepIntake.mbti}ì˜ íŠ¹ì„±`,
        score_impact: 'ì°½ì˜ì„± ìš”êµ¬ ì§ì—…ì— +5 ë¶€ìŠ¤íŠ¸',
      })
    }
  }
  
  // best_moment ê¸°ë°˜ ì¸ì‚¬ì´íŠ¸
  if (deepIntake?.best_moment) {
    const tags = deepIntake.best_moment.tags
    if (tags.includes('solo_deep')) {
      keyTraits.push({
        trait: 'ëª°ì…í•˜ë©° ì¼í•  ë•Œ ì—ë„ˆì§€ë¥¼ ì–»ìŒ',
        evidence: `"${deepIntake.best_moment.raw.slice(0, 30)}..."`,
        score_impact: 'ë¶„ì„/ì—°êµ¬ ì§ì—…ì— +15 ë¶€ìŠ¤íŠ¸',
      })
    }
    if (tags.includes('team_collab') || tags.includes('people_facing')) {
      keyTraits.push({
        trait: 'ì‚¬ëŒë“¤ê³¼ í•¨ê»˜í•  ë•Œ ì—ë„ˆì§€ë¥¼ ì–»ìŒ',
        evidence: `"${deepIntake.best_moment.raw.slice(0, 30)}..."`,
        score_impact: 'í˜‘ì—…/ëŒ€ì¸ ì§ì—…ì— +15 ë¶€ìŠ¤íŠ¸',
      })
    }
    if (tags.includes('creative')) {
      keyTraits.push({
        trait: 'ì°½ì‘/ê¸°íší•  ë•Œ ë³´ëŒì„ ëŠë‚Œ',
        evidence: `"${deepIntake.best_moment.raw.slice(0, 30)}..."`,
        score_impact: 'ì°½ì˜ì„± ìš”êµ¬ ì§ì—…ì— +15 ë¶€ìŠ¤íŠ¸',
      })
    }
  }
  
  // worst_moment ê¸°ë°˜ ì¸ì‚¬ì´íŠ¸
  if (deepIntake?.worst_moment) {
    const stressTrigger = deepIntake.worst_moment.stress_trigger
    if (stressTrigger === 'people') {
      keyTraits.push({
        trait: 'ëŒ€ì¸ ê°ˆë“±ì— ìŠ¤íŠ¸ë ˆìŠ¤ë¥¼ ë°›ìŒ',
        evidence: `"${deepIntake.worst_moment.raw.slice(0, 30)}..."`,
        score_impact: 'ë…ë¦½ì  ì—…ë¬´ ì§ì—…ì— +15 ë¶€ìŠ¤íŠ¸',
      })
    }
    if (stressTrigger === 'deadline') {
      keyTraits.push({
        trait: 'ë§ˆê° ì••ë°•ì— ìŠ¤íŠ¸ë ˆìŠ¤ë¥¼ ë°›ìŒ',
        evidence: `"${deepIntake.worst_moment.raw.slice(0, 30)}..."`,
        score_impact: 'ì›Œë¼ë°¸ ì¢‹ì€ ì§ì—…ì— +10 ë¶€ìŠ¤íŠ¸',
      })
    }
    if (stressTrigger === 'meeting') {
      keyTraits.push({
        trait: 'íšŒì˜ê°€ ë§ì€ í™˜ê²½ì„ í”¼í•˜ê³  ì‹¶ìŒ',
        evidence: `"${deepIntake.worst_moment.raw.slice(0, 30)}..."`,
        score_impact: 'ë…ë¦½ì /ì›ê²© ì§ì—…ì— +10 ë¶€ìŠ¤íŠ¸',
      })
    }
  }
  
  // priority.top1 ê¸°ë°˜ ì¸ì‚¬ì´íŠ¸
  if (deepIntake?.priority_top1) {
    const priorityMap: Record<string, string> = {
      'growth': 'ì„±ì¥ ê°€ëŠ¥ì„±',
      'stability': 'ì•ˆì •ì„±',
      'wlb': 'ì›Œë¼ë°¸',
      'income': 'ë†’ì€ ìˆ˜ì…',
    }
    keyTraits.push({
      trait: `${priorityMap[deepIntake.priority_top1] || deepIntake.priority_top1}ì„(ë¥¼) ìµœìš°ì„ ì‹œ`,
      evidence: 'ì§ì ‘ ì„ íƒ',
      score_impact: `í•´ë‹¹ ì†ì„±ì— +20 ë¶€ìŠ¤íŠ¸`,
    })
  }
  
  // facts ê¸°ë°˜ ì¸ì‚¬ì´íŠ¸
  for (const fact of facts) {
    try {
      const value = JSON.parse(fact.value_json)
      
      if (fact.fact_key === 'tradeoff.salary_vs_wlb') {
        appliedFacts.push({
          fact_key: fact.fact_key,
          effect_summary: value.value === 'wlb' 
            ? 'ì›Œë¼ë°¸ì„ ì—°ë´‰ë³´ë‹¤ ì¤‘ì‹œ â†’ WLB ì§ì—…ì— +15 ë¶€ìŠ¤íŠ¸'
            : 'ì—°ë´‰ì„ ì›Œë¼ë°¸ë³´ë‹¤ ì¤‘ì‹œ â†’ ê³ ìˆ˜ì… ì§ì—…ì— +15 ë¶€ìŠ¤íŠ¸',
        })
      }
      if (fact.fact_key === 'tradeoff.growth_vs_stability') {
        appliedFacts.push({
          fact_key: fact.fact_key,
          effect_summary: value.value === 'growth'
            ? 'ì„±ì¥ì„ ì•ˆì •ë³´ë‹¤ ì¤‘ì‹œ â†’ ì„±ì¥ ê°€ëŠ¥ì„± ì§ì—…ì— +15 ë¶€ìŠ¤íŠ¸'
            : 'ì•ˆì •ì„ ì„±ì¥ë³´ë‹¤ ì¤‘ì‹œ â†’ ì•ˆì •ì  ì§ì—…ì— +15 ë¶€ìŠ¤íŠ¸',
        })
      }
      if (fact.fact_key === 'motivation.work_hours_reason') {
        appliedFacts.push({
          fact_key: fact.fact_key,
          effect_summary: `"${value.value || value.raw?.slice(0, 20)}" ì´ìœ ë¡œ ì•¼ê·¼ ê¸°í”¼ â†’ ê´€ë ¨ ë¦¬ìŠ¤í¬ í˜ë„í‹° ì¡°ì •`,
        })
      }
    } catch {
      // JSON íŒŒì‹± ì‹¤íŒ¨ ì‹œ ë¬´ì‹œ
    }
  }
  
  // ìš”ì•½ ë¬¸ì¥ ìƒì„±
  let summary = ''
  if (keyTraits.length > 0) {
    const topTraits = keyTraits.slice(0, 3).map(t => t.trait)
    summary = `ë‹¹ì‹ ì€ ${topTraits.join(', ')} ì‚¬ëŒì…ë‹ˆë‹¤.`
  } else if (appliedFacts.length > 0) {
    summary = `${appliedFacts.length}ê°œì˜ ë‹µë³€ì„ ë¶„ì„í•˜ì—¬ ë§ì¶¤ ì¶”ì²œì„ ì œê³µí–ˆìŠµë‹ˆë‹¤.`
  } else {
    summary = 'ë” ë§ì€ ì •ë³´ë¥¼ ì…ë ¥í•˜ì‹œë©´ ë” ì •í™•í•œ ì¸ì‚¬ì´íŠ¸ë¥¼ ë“œë¦´ ìˆ˜ ìˆì–´ìš”.'
  }
  
  return {
    summary,
    key_traits: keyTraits,
    applied_facts: appliedFacts,
  }
}

// ============================================
// ì„¤ëª… ìƒì„± (Phase 1A ê°„ë‹¨ ë²„ì „)
// ============================================
function generateExplanation(
  facts: Fact[],
  appliedRules: string[],
  topJobName?: string
): string {
  if (facts.length === 0) {
    return 'ì¶”ê°€ ì§ˆë¬¸ì— ë‹µë³€í•˜ì‹œë©´ ë” ì •í™•í•œ ì¶”ì²œì„ ë°›ìœ¼ì‹¤ ìˆ˜ ìˆìŠµë‹ˆë‹¤.'
  }
  
  const explanations: string[] = []
  
  for (const fact of facts) {
    try {
      const value = JSON.parse(fact.value_json)
      const choice = value.value || value
      
      if (fact.fact_key === 'tradeoff.salary_vs_wlb') {
        if (choice === 'wlb') {
          explanations.push('ì›Œë¼ë°¸ì„ ì¤‘ì‹œí•œë‹¤ëŠ” ë‹µë³€ì„ ë°˜ì˜í–ˆìŠµë‹ˆë‹¤')
        } else {
          explanations.push('ì—°ë´‰ì„ ì¤‘ì‹œí•œë‹¤ëŠ” ë‹µë³€ì„ ë°˜ì˜í–ˆìŠµë‹ˆë‹¤')
        }
      } else if (fact.fact_key === 'tradeoff.growth_vs_stability') {
        if (choice === 'growth') {
          explanations.push('ì„±ì¥ ê°€ëŠ¥ì„±ì„ ì¤‘ì‹œí•œë‹¤ëŠ” ë‹µë³€ì„ ë°˜ì˜í–ˆìŠµë‹ˆë‹¤')
        } else {
          explanations.push('ì•ˆì •ì„±ì„ ì¤‘ì‹œí•œë‹¤ëŠ” ë‹µë³€ì„ ë°˜ì˜í–ˆìŠµë‹ˆë‹¤')
        }
      } else if (fact.fact_key.startsWith('motivation.')) {
        explanations.push(`"${choice}" ë•Œë¬¸ì´ë¼ëŠ” ì´ìœ ë¥¼ ë°˜ì˜í–ˆìŠµë‹ˆë‹¤`)
      }
    } catch {
      // JSON íŒŒì‹± ì‹¤íŒ¨ ì‹œ ë¬´ì‹œ
    }
  }
  
  if (explanations.length === 0) {
    return `${facts.length}ê°œì˜ ë‹µë³€ì„ ë°˜ì˜í•˜ì—¬ ì¶”ì²œí–ˆìŠµë‹ˆë‹¤.`
  }
  
  return explanations.join('. ') + '.'
}

// ============================================
// ìƒ˜í”Œ ì§ì—… ë°ì´í„° (Phase 1A)
// ============================================
interface SampleJob {
  job_id: string
  job_name: string
  base_like: number
  base_can: number
  base_risk: number
  attributes: Record<string, string | number>
}

function getSampleJobs(): SampleJob[] {
  return [
    {
      job_id: 'data-analyst',
      job_name: 'ë°ì´í„° ë¶„ì„ê°€',
      base_like: 70,
      base_can: 65,
      base_risk: 5,
      attributes: { wlb: 80, growth: 85, stability: 70, income: 75, solo_work: 70, remote: 60 },
    },
    {
      job_id: 'software-developer',
      job_name: 'ì†Œí”„íŠ¸ì›¨ì–´ ê°œë°œì',
      base_like: 75,
      base_can: 60,
      base_risk: 10,
      attributes: { wlb: 50, growth: 90, stability: 75, income: 85, solo_work: 65, remote: 80 },
    },
    {
      job_id: 'ux-designer',
      job_name: 'UX ë””ìì´ë„ˆ',
      base_like: 72,
      base_can: 55,
      base_risk: 8,
      attributes: { wlb: 70, growth: 80, stability: 65, income: 70, solo_work: 50, remote: 75, creative: 90 },
    },
    {
      job_id: 'project-manager',
      job_name: 'í”„ë¡œì íŠ¸ ë§¤ë‹ˆì €',
      base_like: 68,
      base_can: 70,
      base_risk: 15,
      attributes: { wlb: 40, growth: 75, stability: 70, income: 80, people_facing: 90, solo_work: 20 },
    },
    {
      job_id: 'accountant',
      job_name: 'íšŒê³„ì‚¬',
      base_like: 60,
      base_can: 75,
      base_risk: 5,
      attributes: { wlb: 60, growth: 50, stability: 95, income: 75, solo_work: 80, analytical: 90 },
    },
    {
      job_id: 'marketing-specialist',
      job_name: 'ë§ˆì¼€íŒ… ì „ë¬¸ê°€',
      base_like: 70,
      base_can: 60,
      base_risk: 20,
      attributes: { wlb: 45, growth: 70, stability: 55, income: 65, people_facing: 75, creative: 80 },
    },
    {
      job_id: 'hr-manager',
      job_name: 'ì¸ì‚¬ ë‹´ë‹¹ì',
      base_like: 62,
      base_can: 68,
      base_risk: 10,
      attributes: { wlb: 70, growth: 60, stability: 80, income: 65, people_facing: 95, solo_work: 30 },
    },
    {
      job_id: 'financial-analyst',
      job_name: 'ì¬ë¬´ ë¶„ì„ê°€',
      base_like: 65,
      base_can: 62,
      base_risk: 25,
      attributes: { wlb: 35, growth: 75, stability: 70, income: 90, solo_work: 65, analytical: 95 },
    },
    {
      job_id: 'content-creator',
      job_name: 'ì½˜í…ì¸  í¬ë¦¬ì—ì´í„°',
      base_like: 78,
      base_can: 50,
      base_risk: 30,
      attributes: { wlb: 55, growth: 65, stability: 30, income: 50, solo_work: 75, remote: 90, creative: 95 },
    },
    {
      job_id: 'teacher',
      job_name: 'êµì‚¬',
      base_like: 60,
      base_can: 70,
      base_risk: 5,
      attributes: { wlb: 80, growth: 40, stability: 95, income: 55, people_facing: 90, impact: 85 },
    },
    {
      job_id: 'nurse',
      job_name: 'ê°„í˜¸ì‚¬',
      base_like: 55,
      base_can: 60,
      base_risk: 35,
      attributes: { wlb: 25, growth: 50, stability: 90, income: 65, people_facing: 95, impact: 95 },
    },
    {
      job_id: 'consultant',
      job_name: 'ê²½ì˜ ì»¨ì„¤í„´íŠ¸',
      base_like: 72,
      base_can: 55,
      base_risk: 40,
      attributes: { wlb: 20, growth: 95, stability: 50, income: 95, people_facing: 80, analytical: 85 },
    },
  ]
}

// ============================================
// Helper Types
// ============================================
interface FollowupPayloadV2 {
  session_id: string
  user_id?: string
  question_id: string
  question_type?: string
  attribute?: string
  fact_key?: string
  answer: string
  answer_text?: string  // ììœ ì‘ë‹µ ì›ë¬¸
  confidence?: number
  // backward compatibility
  request_id?: number
  constraint?: string
  job_id?: string
  job_name?: string
}

interface FollowupResultV2 {
  success: boolean
  fact_saved: {
    fact_key: string
    value: string
    fact_level: number
  }
  reanalyze_available: boolean
  message: string
}

function determineFctLevel(factKey: string): number {
  if (factKey.startsWith('confirmed_constraint')) return 1
  if (factKey.startsWith('priority.dealbreaker')) return 2
  if (factKey.startsWith('priority.') || factKey.startsWith('tradeoff.') || factKey.startsWith('motivation.')) return 3
  return 4
}

// ============================================
// V3: Stage-based Analysis (with stage-aware follow-ups)
// ============================================
async function runAnalysisV3(
  db: D1Database,
  payload: AnalyzePayload,
  requestId: number,
  facts: Fact[],
  deepIntake?: NormalizedDeepIntake,
  stage?: AnalysisStage,
  debugMode?: boolean,
  env?: { VECTORIZE?: VectorizeIndex; AI?: Ai }
): Promise<AnalysisResultJSON> {
  // ============================================
  // P1 Vectorize ê¸°ë°˜ í›„ë³´êµ° í™•ì¥
  // ============================================
  let candidateSource: 'vectorize' | 'tagged' | 'sample_fallback' = 'tagged'
  let taggedCount = 0
  let vectorizeUsed = false
  let totalCandidates = 0
  
  // Vectorize ì‚¬ìš© ê°€ëŠ¥ ì—¬ë¶€ í™•ì¸
  const useVectorize = env?.VECTORIZE && env?.AI
  
  let sampleJobs: Array<{
    job_id: string
    job_name: string
    base_like: number
    base_can: number
    base_risk: number
    attributes: Record<string, number | string>
  }>
  
  if (useVectorize) {
    // Vectorize ê¸°ë°˜ í›„ë³´êµ° í™•ì¥ (80 â†’ 500)
    try {
      console.log('[V3 Analyze] Using Vectorize for candidate expansion')
      const expansionResult = await expandCandidates(
        db,
        env!.VECTORIZE,
        env!.AI,
        facts.map(f => ({ fact_key: f.fact_key, value_json: f.value_json })),
        { targetSize: 500, minTaggedJobs: 80 }
      )
      
      if (!expansionResult.fallback_used && expansionResult.candidates.length > 0) {
        // ë²¡í„° ê²°ê³¼ë¥¼ ScoredJob í˜•íƒœë¡œ ë³€í™˜
        sampleJobs = await vectorResultsToScoredJobs(db, expansionResult.candidates)
        candidateSource = 'vectorize'
        vectorizeUsed = true
        totalCandidates = expansionResult.candidates.length
        console.log(`[V3 Analyze] Vectorize returned ${totalCandidates} candidates in ${expansionResult.search_duration_ms}ms`)
      } else {
        // Vectorize ì‹¤íŒ¨ ì‹œ ê¸°ì¡´ ë°©ì‹
        throw new Error('Vectorize fallback triggered')
      }
    } catch (vectorError) {
      console.log('[V3 Analyze] Vectorize unavailable, using tagged jobs')
      // ê¸°ì¡´ ë°©ì‹ìœ¼ë¡œ í´ë°±
      const jobAttrs = await db.prepare(`
        SELECT 
          job_id, job_name,
          wlb, growth, stability, income,
          teamwork, solo_deep, analytical, creative, execution, people_facing,
          work_hours, shift_work, travel, remote_possible,
          degree_required, license_required
        FROM job_attributes
        WHERE tagger_version = 'tagger-v1.0.0'
        LIMIT 80
      `).all<{
        job_id: string
        job_name: string
        wlb: number
        growth: number
        stability: number
        income: number
        teamwork: number
        solo_deep: number
        analytical: number
        creative: number
        execution: number
        people_facing: number
        work_hours: string
        shift_work: string
        travel: string
        remote_possible: string
        degree_required: string
        license_required: string
      }>()
      
      const useDbData = jobAttrs.results && jobAttrs.results.length > 0
      candidateSource = useDbData ? 'tagged' : 'sample_fallback'
      taggedCount = jobAttrs.results?.length || 0
      
      sampleJobs = useDbData
        ? jobAttrs.results!.map(j => ({
            job_id: j.job_id,
            job_name: j.job_name,
            base_like: Math.round((j.wlb + j.growth + j.stability + j.income) / 4),
            base_can: Math.round((j.teamwork + (j.analytical * 0.7) + j.creative) / 3),
            base_risk: 10,
            attributes: {
              wlb: j.wlb,
              growth: j.growth,
              stability: j.stability,
              income: j.income,
              remote: j.remote_possible === 'full' ? 100 : j.remote_possible === 'partial' ? 50 : 0,
              solo_work: j.solo_deep,
              people_facing: j.people_facing,
              analytical: j.analytical,
              creative: j.creative,
            },
          }))
        : getSampleJobs()
    }
  } else {
    // Vectorize ì—†ì´ ê¸°ì¡´ ë°©ì‹
    const jobAttrs = await db.prepare(`
      SELECT 
        job_id, job_name,
        wlb, growth, stability, income,
        teamwork, solo_deep, analytical, creative, execution, people_facing,
        work_hours, shift_work, travel, remote_possible,
        degree_required, license_required
      FROM job_attributes
      WHERE tagger_version = 'tagger-v1.0.0'
      LIMIT 80
    `).all<{
      job_id: string
      job_name: string
      wlb: number
      growth: number
      stability: number
      income: number
      teamwork: number
      solo_deep: number
      analytical: number
      creative: number
      execution: number
      people_facing: number
      work_hours: string
      shift_work: string
      travel: string
      remote_possible: string
      degree_required: string
      license_required: string
    }>()
    
    const useDbData = jobAttrs.results && jobAttrs.results.length > 0
    candidateSource = useDbData ? 'tagged' : 'sample_fallback'
    taggedCount = jobAttrs.results?.length || 0
    
    sampleJobs = useDbData
      ? jobAttrs.results!.map(j => ({
          job_id: j.job_id,
          job_name: j.job_name,
          base_like: Math.round((j.wlb + j.growth + j.stability + j.income) / 4),
          base_can: Math.round((j.teamwork + (j.analytical * 0.7) + j.creative) / 3),
          base_risk: 10,
          attributes: {
            wlb: j.wlb,
            growth: j.growth,
            stability: j.stability,
            income: j.income,
            remote: j.remote_possible === 'full' ? 100 : j.remote_possible === 'partial' ? 50 : 0,
            solo_work: j.solo_deep,
            people_facing: j.people_facing,
            analytical: j.analytical,
            creative: j.creative,
          },
        }))
      : getSampleJobs()
  }
  
  // 2. Fact boosts ê³„ì‚°
  const factBoosts = calculateFactBoosts(facts)
  
  // 3. ê° ì§ì—…ì— ì ìˆ˜ ì ìš©
  const scoredJobs: ScoredJob[] = sampleJobs.map(job => {
    const baseScores: JobScores = {
      like: job.base_like,
      can: job.base_can,
      risk_penalty: job.base_risk,
    }
    
    const adjusted = applyFactBoostsToJob(baseScores, job.attributes, factBoosts)
    const fit = Math.round(0.5 * adjusted.like + 0.5 * adjusted.can - adjusted.risk_penalty)
    
    return {
      job_id: job.job_id,
      job_name: job.job_name,
      scores: {
        fit: Math.max(0, fit),
        like: adjusted.like,
        can: adjusted.can,
        risk_penalty: adjusted.risk_penalty,
      },
      attributes: job.attributes,
    }
  })
  
  // 4. ì •ë ¬ (Fit ê¸°ì¤€)
  scoredJobs.sort((a, b) => b.scores.fit - a.scores.fit)
  
  const top10 = scoredJobs.slice(0, 10)
  
  // Phase 4: Diversity Guard (Research Bias ë°©ì§€ + ë‹¤ì–‘ì„± í™•ë³´)
  const rawTop3 = top10.slice(0, 3)
  const diversityResult = applyDiversityGuard(rawTop3, scoredJobs)
  const top3 = diversityResult.adjusted
  
  // 5. Follow-up ì§ˆë¬¸ ìƒì„± (Stage-aware)
  const existingFactKeys = facts.map(f => ({ fact_key: f.fact_key }))
  
  let followupQuestions: FollowupQuestion[]
  if (stage) {
    // V3: Stage ê¸°ë°˜ ì§ˆë¬¸ ìƒì„±
    followupQuestions = generateStageBasedFollowups(
      scoredJobs,
      top10,
      existingFactKeys,
      stage,
      !!deepIntake
    )
  } else {
    // V2 fallback: ê¸°ì¡´ ë°©ì‹
    followupQuestions = generateFollowupQuestions({
      candidates: scoredJobs,
      topK: top10,
      existingFacts: existingFactKeys,
      hasDeepIntake: !!deepIntake,
    }, 3)
  }
  
  // 6. User Insight ìƒì„± (Stage-aware)
  const userInsight = stage
    ? generateStageAwareInsight(facts, deepIntake, factBoosts.applied_rules, stage)
    : generateUserInsight(facts, deepIntake, factBoosts.applied_rules)
  
  // 7. ì…ë ¥ ì •ë³´ ì •ë¦¬
  const lifeConstraints = extractLifeConstraints(facts)
  const universalFactsCount = facts.filter(f => f.fact_key.startsWith('profile.')).length
  const confirmedConstraints = facts
    .filter(f => f.fact_key.startsWith('confirmed_constraint.'))
    .map(f => f.fact_key.replace('confirmed_constraint.', ''))
  
  // Legacy profile ì¶”ì¶œ (V2 í˜¸í™˜)
  const legacyProfile = 'profile' in payload && payload.profile
    ? payload.profile
    : { interest: { keywords: [] }, value: { priority: [] }, skill: [], dislike: { keywords: [] }, constraints: {} }
  
  // 8. ê²°ê³¼ êµ¬ì„±
  const result: AnalysisResultJSON = {
    engine_state: stage 
      ? 'phase2_stage_based'  // V3: Stage-based ë¶„ì„
      : (deepIntake ? 'phase1a_mve' : (facts.length > 0 ? 'phase1a_mve' : 'phase1a_initial')),
    
    versions: {
      recipe: VERSIONS.recipe,
      tagger: VERSIONS.tagger,
      scoring: VERSIONS.scoring,
    },
    
    input_summary: {
      profile_revision_id: `rev-${requestId}-${Date.now()}`,
      key_interests: legacyProfile.interest?.keywords?.slice(0, 3) || [],
      key_skills: legacyProfile.skill?.map(s => s.name).slice(0, 3) || [],
      non_negotiables: Object.entries(legacyProfile.constraints || {})
        .filter(([_, v]) => v === true)
        .map(([k]) => k),
      preferences: [],
      facts_applied: facts.length,
      applied_rules: factBoosts.applied_rules,
      deep_intake_provided: !!deepIntake,
      insight_tags: userInsight?.key_traits.map(t => t.trait) || [],
      // V3 ì¶”ê°€ í•„ë“œ
      stage: stage,
      universal_facts_count: universalFactsCount,
      life_constraints: lifeConstraints,
      confirmed_constraints: confirmedConstraints,
    },
    
    fit_top3: top3.map(j => {
      // Evidence ìƒì„± (facts + job attributes ê¸°ë°˜)
      // âš ï¸ EvidenceëŠ” "ì„¤ëª…ìš©"ì´ë©° ì ìˆ˜ ê³„ì‚°ì— ì˜í–¥ ì—†ìŒ
      const evidenceFacts: EvidenceFact[] = facts.map(f => ({
        fact_key: f.fact_key,
        value_json: f.value_json,
      }))
      let evidenceLinks = buildEvidenceLinks(evidenceFacts, j)
      
      // Evidenceê°€ ë¶€ì¡±í•˜ë©´ ê¸°ë³¸ Evidence ì¶”ê°€
      if (evidenceLinks.length < 3) {
        const defaultEvidence = generateDefaultEvidence(j)
        evidenceLinks = [...evidenceLinks, ...defaultEvidence].slice(0, 10)
      }
      
      return {
        job_id: j.job_id,
        job_name: j.job_name,
        fit_score: j.scores.fit,
        like_score: j.scores.like,
        can_score: j.scores.can,
        risk_details: [],
        evidence_links: evidenceLinks,
      }
    }),
    
    like_top10: top10.map(j => ({
      job_id: j.job_id,
      job_name: j.job_name,
      like_score: j.scores.like,
    })),
    
    can_top10: top10.map(j => ({
      job_id: j.job_id,
      job_name: j.job_name,
      can_score: j.scores.can,
    })),
    
    caution_jobs: scoredJobs
      .filter(j => j.scores.risk_penalty > 20)
      .slice(0, 5)
      .map(j => ({
        job_id: j.job_id,
        job_name: j.job_name,
        risk_penalty: j.scores.risk_penalty,
      })),
    
    followup_questions: followupQuestions,
    
    ux_flags: {
      has_caution_in_top3: top3.some(j => j.scores.risk_penalty > 20),
      has_unknown_in_top3: false,
      needs_followup_question: followupQuestions.length > 0,
      counts_before_filter: {
        safe_known: scoredJobs.filter(j => j.scores.risk_penalty <= 10).length,
        safe_unknown: scoredJobs.filter(j => j.scores.risk_penalty > 10 && j.scores.risk_penalty <= 20).length,
        caution: scoredJobs.filter(j => j.scores.risk_penalty > 20).length,
        total: scoredJobs.length,
      },
      counts_after_filter: {
        safe_known: scoredJobs.filter(j => j.scores.risk_penalty <= 10).length,
        safe_unknown: scoredJobs.filter(j => j.scores.risk_penalty > 10 && j.scores.risk_penalty <= 20).length,
        caution: scoredJobs.filter(j => j.scores.risk_penalty > 20).length,
        total: scoredJobs.length,
      },
    },
    
    llm_explanation: generateExplanation(facts, factBoosts.applied_rules, top3[0]?.job_name),
    generated_at: new Date().toISOString(),
    total_candidates: scoredJobs.length,
    
    user_insight: userInsight,
    
    // Phase 4: Diversity Guard ì •ë³´
    phase4_applied: true,
    diversity_guard_active: diversityResult.diversityApplied,
    diversity_changes: diversityResult.changes,
    
    // V3: Stage ê¸°ë°˜ ë¶„ì„ ì •ë³´
    analysis_stage: stage,
    stage_specific_insights: stage ? generateStageInsights(stage, facts) : undefined,
    
    // Debug info (only included when debugMode=true)
    debug_info: debugMode ? generateDebugInfo(
      candidateSource,
      taggedCount,
      sampleJobs.length,
      top3,
      sampleJobs,
      factBoosts,
      facts,
      followupQuestions[0],
      diversityResult
    ) : undefined,
  }
  
  return result
}

// ============================================
// Debug Info Generator (for test UI)
// ============================================
interface SampleJobWithBase {
  job_id: string
  job_name: string
  base_like: number
  base_can: number
  base_risk: number
  attributes: Record<string, number>
}

function generateDebugInfo(
  candidateSource: 'tagged' | 'sample_fallback' | 'vector' | 'random',
  taggedCount: number,
  totalInDb: number,
  top3: ScoredJob[],
  sampleJobs: SampleJobWithBase[],
  factBoosts: { like_boost: number; can_boost: number; risk_boost: number; applied_rules: string[] },
  facts: Fact[],
  firstFollowup: FollowupQuestion | undefined,
  diversityResult: { diversityApplied: boolean; changes: string[] }
): DebugInfo {
  // Score breakdown for TOP3
  const scoreBreakdown = top3.map(job => {
    const baseJob = sampleJobs.find(s => s.job_id === job.job_id)
    const baseLike = baseJob?.base_like || 50
    const baseCan = baseJob?.base_can || 50
    const baseRisk = baseJob?.base_risk || 10
    
    // Calculate boosts per rule (simplified)
    const likeBoosts = factBoosts.applied_rules
      .filter(r => r.includes('interest') || r.includes('priority') || r.includes('value'))
      .map(r => ({ rule: r, delta: Math.round(factBoosts.like_boost / Math.max(1, factBoosts.applied_rules.length)) }))
    
    const canBoosts = factBoosts.applied_rules
      .filter(r => r.includes('workstyle') || r.includes('strength') || r.includes('skill'))
      .map(r => ({ rule: r, delta: Math.round(factBoosts.can_boost / Math.max(1, factBoosts.applied_rules.length)) }))
    
    const riskBoosts = factBoosts.applied_rules
      .filter(r => r.includes('constraint') || r.includes('dislike'))
      .map(r => ({ rule: r, delta: factBoosts.risk_boost }))
    
    return {
      job_id: job.job_id,
      job_name: job.job_name,
      base_like: baseLike,
      base_can: baseCan,
      base_risk: baseRisk,
      like_boosts: likeBoosts,
      can_boosts: canBoosts,
      risk_boosts: riskBoosts,
      final_like: job.scores.like,
      final_can: job.scores.can,
      final_risk: job.scores.risk_penalty,
      final_fit: job.scores.fit,
    }
  })
  
  // Applied facts summary
  const appliedFacts = facts.slice(0, 10).map(f => {
    let valueStr = ''
    try {
      const parsed = JSON.parse(f.value_json)
      valueStr = Array.isArray(parsed.value) ? parsed.value.join(', ') : String(parsed.value || '')
    } catch {
      valueStr = f.value_json
    }
    return {
      fact_key: f.fact_key,
      value: valueStr.slice(0, 50),
      effect: factBoosts.applied_rules.includes(f.fact_key) ? 'applied' : 'stored',
    }
  })
  
  // Followup rationale
  const followupRationale = firstFollowup ? {
    split_attribute: firstFollowup.affects_attributes?.[0] || firstFollowup.constraint || 'unknown',
    split_gain: 0.5, // Placeholder - actual splitGain calculation is complex
    reason: firstFollowup.context || 'TOP3 ì§ì—… ê°„ ë¶„ë³„ë ¥ í–¥ìƒ',
  } : undefined
  
  return {
    candidate_source: candidateSource,
    tagged_count: taggedCount,
    total_in_db: totalInDb,
    score_breakdown: scoreBreakdown,
    followup_rationale: followupRationale,
    applied_facts: appliedFacts,
    versions: {
      recipe: VERSIONS.recipe,
      tagger: VERSIONS.tagger,
      scoring: VERSIONS.scoring,
      embedding: 'none', // Will be updated when Vectorize is implemented
    },
    diversity_guard_triggered: diversityResult.diversityApplied,
    research_bias_cap_applied: diversityResult.changes.some(c => c.includes('Research')),
  }
}

// V3: Stageë³„ ë§ì¶¤ ì¸ì‚¬ì´íŠ¸ ìƒì„±
function generateStageInsights(stage: AnalysisStage, facts: Fact[]): string[] {
  const insights: string[] = []
  
  switch (stage) {
    case 'job_explore':
      insights.push('íƒìƒ‰ ë‹¨ê³„: ë‹¤ì–‘í•œ ì§ì—…êµ°ì„ í­ë„“ê²Œ ê²€í† í•˜ê³  ìˆìŠµë‹ˆë‹¤.')
      if (facts.some(f => f.fact_key.includes('interest'))) {
        insights.push('ê´€ì‹¬ ë¶„ì•¼ë¥¼ ê¸°ë°˜ìœ¼ë¡œ ì¶”ì²œì´ ì¡°ì •ë˜ì—ˆìŠµë‹ˆë‹¤.')
      }
      break
    case 'job_student':
      insights.push('í•™ìƒ ë‹¨ê³„: ì „ê³µÂ·ì§„ë¡œ ì—°ê³„ì„±ì´ ë°˜ì˜ë˜ì—ˆìŠµë‹ˆë‹¤.')
      if (facts.some(f => f.fact_key.includes('constraints'))) {
        insights.push('ìê²©Â·ì‹œê°„ ì œì•½ì´ ê³ ë ¤ë˜ì—ˆìŠµë‹ˆë‹¤.')
      }
      break
    case 'job_early':
      insights.push('ì´ˆê¸° ê²½ë ¥ ë‹¨ê³„: ì„±ì¥ ê°€ëŠ¥ì„±ì´ ê°•ì¡°ë˜ì—ˆìŠµë‹ˆë‹¤.')
      if (facts.some(f => f.fact_key.includes('dislike'))) {
        insights.push('ê¸°í”¼ ìš”ì†Œê°€ ì œì™¸ í•„í„°ì— ë°˜ì˜ë˜ì—ˆìŠµë‹ˆë‹¤.')
      }
      break
    default:
      break
  }
  
  return insights
}

// ============================================
// Stage-based Follow-up ì§ˆë¬¸ ìƒì„±
// ============================================
function generateStageBasedFollowups(
  candidates: ScoredJob[],
  topK: ScoredJob[],
  existingFacts: { fact_key: string }[],
  stage: AnalysisStage,
  hasDeepIntake: boolean
): FollowupQuestion[] {
  // Stageì— ë§ëŠ” ì§ˆë¬¸ í’€ ê°€ì ¸ì˜¤ê¸°
  const stageQuestions = getQuestionsForStage(stage)
  
  // ì´ë¯¸ ë‹µë³€í•œ fact_key ì œì™¸
  const answeredKeys = new Set(existingFacts.map(f => f.fact_key))
  const availableQuestions = stageQuestions.filter(q => !answeredKeys.has(q.fact_key))
  
  if (availableQuestions.length === 0) {
    // Stage ì§ˆë¬¸ì´ ì—†ìœ¼ë©´ ê¸°ì¡´ ë°©ì‹ìœ¼ë¡œ fallback
    return generateFollowupQuestions({
      candidates,
      topK,
      existingFacts,
      hasDeepIntake,
    }, 3)
  }
  
  // ì •ë³´ì´ë“ ì ìˆ˜ ê³„ì‚° (ê°„ë‹¨ ë²„ì „)
  const scoredQuestions = availableQuestions.map(q => {
    let score = 0
    
    // 1. íƒ€ì…ë³„ ê¸°ë³¸ ì ìˆ˜
    if (q.type === 'behavior' && !hasDeepIntake) score += 10
    if (q.type === 'tradeoff') score += 8
    if (q.type === 'projection') score += 6
    if (q.type === 'scenario') score += 5
    if (q.type === 'narrative' && hasDeepIntake) score += 7
    
    // 2. TOP10 ì°¨ë³„í™” ê°€ëŠ¥ì„±
    const affectedAttrs = q.affects_attributes
    const attrVariance = calculateAttributeVariance(topK, affectedAttrs)
    score += attrVariance * 2
    
    return { question: q, score }
  })
  
  // ì ìˆ˜ ê¸°ì¤€ ì •ë ¬
  scoredQuestions.sort((a, b) => b.score - a.score)
  
  // ìƒìœ„ 3~5ê°œ ì„ íƒ
  const maxQuestions = hasDeepIntake ? 3 : 5
  const selected = scoredQuestions.slice(0, maxQuestions)
  
  // FollowupQuestion í˜•ì‹ìœ¼ë¡œ ë³€í™˜
  return selected.map(sq => ({
    id: sq.question.question_id,
    type: sq.question.type as any,
    question: getQuestionText(sq.question, stage),
    context: `Stage: ${stage}`,
    options: sq.question.options.map(o => ({
      value: o.value,
      label: o.label,
      tags: o.tags,
    })),
    fact_key: sq.question.fact_key,
    affects_attributes: sq.question.affects_attributes,
  }))
}

// ì†ì„± ë¶„ì‚° ê³„ì‚° (TOP10 ê°„ ì°¨ì´)
function calculateAttributeVariance(jobs: ScoredJob[], attrs: string[]): number {
  if (jobs.length === 0 || attrs.length === 0) return 0
  
  let totalVariance = 0
  for (const attr of attrs) {
    const values = jobs.map(j => {
      const attrValue = (j.attributes as any)[attr]
      return typeof attrValue === 'number' ? attrValue : 50
    })
    
    const mean = values.reduce((a, b) => a + b, 0) / values.length
    const variance = values.reduce((sum, v) => sum + Math.pow(v - mean, 2), 0) / values.length
    totalVariance += Math.sqrt(variance)
  }
  
  return totalVariance / attrs.length
}

// ============================================
// Stage-aware User Insight ìƒì„±
// ============================================
function generateStageAwareInsight(
  facts: Fact[],
  deepIntake: NormalizedDeepIntake | undefined,
  appliedRules: string[],
  stage: AnalysisStage
): UserInsight | undefined {
  // ê¸°ë³¸ ì¸ì‚¬ì´íŠ¸ ìƒì„±
  const baseInsight = generateUserInsight(facts, deepIntake, appliedRules)
  
  if (!baseInsight) {
    // factsê°€ ì—†ì–´ë„ stage ê¸°ë°˜ ê¸°ë³¸ ë©”ì‹œì§€
    const wording = INSIGHT_WORDING[stage]
    return {
      summary: 'ë” ë§ì€ ì •ë³´ë¥¼ ì…ë ¥í•˜ì‹œë©´ ë§ì¶¤ ì¸ì‚¬ì´íŠ¸ë¥¼ ë“œë¦´ ìˆ˜ ìˆì–´ìš”.',
      key_traits: [],
      applied_facts: [],
    }
  }
  
  // Stageë³„ wording ì ìš©
  const wording = INSIGHT_WORDING[stage]
  
  // ìš”ì•½ ë¬¸ì¥ ì¬ìƒì„±
  const topTraits = baseInsight.key_traits.slice(0, 3).map(t => t.trait)
  let summary = ''
  
  if (topTraits.length > 0) {
    summary = `${wording.summary_prefix}${topTraits.join(', ')} ì„±í–¥ì´ ë³´ì…ë‹ˆë‹¤.`
  } else if (baseInsight.applied_facts.length > 0) {
    summary = `${baseInsight.applied_facts.length}ê°œì˜ ë‹µë³€ì„ ë¶„ì„í•˜ì—¬ ë§ì¶¤ ì¶”ì²œì„ ì œê³µí–ˆìŠµë‹ˆë‹¤.`
  } else {
    summary = 'ë” ë§ì€ ì •ë³´ë¥¼ ì…ë ¥í•˜ì‹œë©´ ë” ì •í™•í•œ ì¸ì‚¬ì´íŠ¸ë¥¼ ë“œë¦´ ìˆ˜ ìˆì–´ìš”.'
  }
  
  // evidence ë¼ë²¨ ì—…ë°ì´íŠ¸
  const updatedTraits = baseInsight.key_traits.map(trait => ({
    ...trait,
    evidence: trait.evidence.includes('ì„ íƒ') 
      ? trait.evidence.replace('ì„ íƒ', wording.evidence_label)
      : trait.evidence,
  }))
  
  return {
    summary,
    key_traits: updatedTraits,
    applied_facts: baseInsight.applied_facts,
  }
}

// Life constraints ì¶”ì¶œ
function extractLifeConstraints(facts: Fact[]): string[] {
  const lifeConstraintFact = facts.find(f => f.fact_key === 'profile.life_constraint')
  if (!lifeConstraintFact) return []
  
  try {
    const value = JSON.parse(lifeConstraintFact.value_json)
    return Array.isArray(value.value) ? value.value : [value.value]
  } catch {
    return []
  }
}

// ============================================
// V3 Premium Report API
// ============================================
analyzerRoutes.post('/v3/report', async (c) => {
  const env = c.env as { DB: D1Database }
  const db = env.DB
  
  try {
    const body = await c.req.json<{
      request_id: number
      session_id?: string
    }>()
    
    // request_idë¡œ ë¶„ì„ ê²°ê³¼ ì¡°íšŒ
    const requestRow = await db.prepare(`
      SELECT request_id, request_payload, result_json
      FROM ai_analysis_requests
      WHERE request_id = ?
    `).bind(body.request_id).first<{
      request_id: number
      request_payload: string
      result_json: string | null
    }>()
    
    if (!requestRow) {
      return c.json({ 
        error: 'NOT_FOUND',
        message: 'ë¶„ì„ ê²°ê³¼ë¥¼ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤'
      }, 404)
    }
    
    if (!requestRow.result_json) {
      return c.json({
        error: 'NOT_READY',
        message: 'ë¶„ì„ì´ ì•„ì§ ì™„ë£Œë˜ì§€ ì•Šì•˜ìŠµë‹ˆë‹¤'
      }, 400)
    }
    
    // ë¶„ì„ ê²°ê³¼ íŒŒì‹±
    const analysisResult = JSON.parse(requestRow.result_json)
    
    // facts ì¡°íšŒ
    const factsResult = await db.prepare(`
      SELECT fact_key, value_json
      FROM raw_events
      WHERE session_id = ?
      ORDER BY created_at DESC
    `).bind(body.session_id || `session-${body.request_id}`).all<{
      fact_key: string
      value_json: string
    }>()
    
    const facts = factsResult.results || []
    
    // ì¶”ì²œ ê²°ê³¼ë¥¼ ScoredJobForEvidence í˜•íƒœë¡œ ë³€í™˜
    const recommendations: ScoredJobForEvidence[] = (analysisResult.fit_top3 || []).map((job: any) => ({
      job_id: job.job_id,
      job_name: job.job_name,
      scores: {
        fit: job.fit_score || 50,
        like: job.like_score || 50,
        can: job.can_score || 50,
        risk_penalty: 0,
      },
      attributes: {
        wlb: 50,
        growth: 50,
        stability: 50,
        income: 50,
        ...job.attributes,
      },
    }))
    
    // Premium Report ìƒì„±
    const sessionId = body.session_id || `session-${body.request_id}`
    const reportInput: PremiumReportInput = {
      session_id: sessionId,
      facts: facts.map(f => ({
        fact_key: f.fact_key,
        value_json: f.value_json,
      })),
      recommendations,
      userInsight: analysisResult.user_insight,
      stage: analysisResult.input_summary?.stage,
    }
    
    const premiumReport = generatePremiumReport(reportInput)
    
    // ============================================
    // User Profile ìŠ¤ëƒ…ìƒ· ì €ì¥ (P1 ê¸°ëŠ¥)
    // ============================================
    try {
      // ì´ì „ ìŠ¤ëƒ…ìƒ· ì¡°íšŒ
      const previousSnapshot = await getLatestProfileSnapshot(db, sessionId)
      
      // í”„ë¡œí•„ ë¹Œë“œ
      const profile = await buildProfileFromTurns(db, sessionId, previousSnapshot?.profile)
      
      // ìŠ¤ëƒ…ìƒ· ì €ì¥
      await saveProfileSnapshot(
        db,
        sessionId,
        undefined, // user_idëŠ” ì„¸ì…˜ì—ì„œ ê°€ì ¸ì˜¬ ìˆ˜ ìˆìœ¼ë©´ ì¶”ê°€
        body.request_id,
        premiumReport.report_id,
        'premium_report',
        profile,
        previousSnapshot?.id
      )
      
      console.log(`[V3 Report] Profile snapshot saved for session: ${sessionId}`)
    } catch (profileError) {
      // í”„ë¡œí•„ ì €ì¥ ì‹¤íŒ¨í•´ë„ ë³´ê³ ì„œëŠ” ë°˜í™˜ (graceful degradation)
      console.error('[V3 Report] Profile snapshot save failed:', profileError)
    }
    
    return c.json({
      success: true,
      report: premiumReport,
    })
    
  } catch (error) {
    console.error('[V3 Report] Error:', error)
    return c.json({
      error: 'INTERNAL_ERROR',
      message: error instanceof Error ? error.message : 'Unknown error',
    }, 500)
  }
})

// ============================================
// V3 Purpose-based Followup API
// ============================================
analyzerRoutes.post('/v3/followup-questions', async (c) => {
  const env = c.env as { DB: D1Database }
  const db = env.DB
  
  try {
    const body = await c.req.json<{
      request_id: number
      session_id?: string
    }>()
    
    // ê¸°ì¡´ ë¶„ì„ ê²°ê³¼ ì¡°íšŒ
    const requestRow = await db.prepare(`
      SELECT result_json
      FROM ai_analysis_requests
      WHERE request_id = ?
    `).bind(body.request_id).first<{ result_json: string | null }>()
    
    if (!requestRow?.result_json) {
      return c.json({
        error: 'NOT_FOUND',
        message: 'ë¶„ì„ ê²°ê³¼ë¥¼ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤'
      }, 404)
    }
    
    const analysisResult = JSON.parse(requestRow.result_json)
    
    // facts ì¡°íšŒ
    const factsResult = await db.prepare(`
      SELECT fact_key, value_json
      FROM raw_events
      WHERE session_id = ?
    `).bind(body.session_id || `session-${body.request_id}`).all<{
      fact_key: string
      value_json: string
    }>()
    
    const facts = factsResult.results || []
    
    // ì¶”ì²œ ê²°ê³¼ ë³€í™˜
    const topK = (analysisResult.fit_top3 || []).map((job: any) => ({
      job_id: job.job_id,
      job_name: job.job_name,
      scores: {
        fit: job.fit_score || 50,
        like: job.like_score || 50,
        can: job.can_score || 50,
        risk_penalty: 0,
      },
      attributes: {
        wlb: 50,
        growth: 50,
        stability: 50,
        income: 50,
        people_facing: 50,
        remote_possible: 50,
        ...job.attributes,
      },
    }))
    
    // Purpose-based Followup ìƒì„±
    const input: PurposeBasedFollowupInput = {
      candidates: topK,
      topK,
      facts,
      maxQuestions: 3,
    }
    
    const followupQuestions = generatePurposeBasedFollowups(input)
    
    return c.json({
      success: true,
      followup_questions: followupQuestions,
      question_count: followupQuestions.length,
    })
    
  } catch (error) {
    console.error('[V3 Followup Questions] Error:', error)
    return c.json({
      error: 'INTERNAL_ERROR',
      message: error instanceof Error ? error.message : 'Unknown error',
    }, 500)
  }
})

// ============================================
// Mount Tagger Routes
// ============================================
analyzerRoutes.route('/tagger', taggerRoutes)

export { analyzerRoutes }
