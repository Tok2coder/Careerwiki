// CareerWiki AI Analyzer API Routes
// Version: v2.0.0-stage-based (Universal Intake + Stage-based Follow-up)
// Framework: Hono (Cloudflare Workers)

import { Hono } from 'hono'
import type { D1Database, VectorizeIndex, Ai } from '@cloudflare/workers-types'
import {
  VERSIONS,
  assertConstraintType,
  isValidStage,
  isMinorStage,
  isExperienceAllowed,
  type AnalysisRequestPayload,
  type AnalysisResultJSON,
  type AnalysisRequestPayloadV3,
  type UniversalAnswers,
  type AnalysisStage,
  type FollowupResponsePayload,
  type FollowupResult,
  type DeepIntakeInput,
  type UserInsight,
  type DebugInfo,
  type NarrativeFacts,
  type RoundAnswer,
} from './types'
import {
  calculateFactBoosts,
  applyFactBoostsToJob,
  normalizeDeepIntake,
  applyBalanceCap,  // P0: Can-Like ë°¸ëŸ°ìŠ¤ ìº¡
  type JobScores,
  type NormalizedDeepIntake,
} from './fact-score-mapping'
import {
  generateFollowupQuestions,
  type ScoredJob,
  type FollowupQuestion,
} from './question-generation'
// 2026-01-26: tagger-routes ì œê±°ë¨ (íƒœê¹… ì˜ì¡´ë„ ì™„ì „ ì œê±°)
import {
  UNIVERSAL_QUESTIONS,
  getUniversalQuestionsForStage,
  INSIGHT_WORDING,
  type UniversalQuestion,
} from './universal-questions'
import {
  getQuestionsForStage,
  getQuestionText,
  type StageQuestion,
} from './stage-question-banks'
import {
  extractUserConstraints,
  applyTagFilter,
  applyMiniModuleHardFilter,
  calculateMiniModuleRiskPenalty,
  deriveConstraintsFromHardBias,
  applyHardBiasPenalty,
  // P2: Can ê¸°ë°˜ TAG í•„í„°
  applyCanBasedFilter,
  extractVerifiedCanFromFacts,
  type FilteredCandidate,
  type VerifiedCanMap,
} from './tag-filter'
// P3: ì„±ì¥ê³¡ì„  â†’ ì§ì—… ë§¤í•‘
import {
  extractGrowthPreference,
  matchGrowthCurves,
  type UserGrowthPreference,
} from './growth-curve-mapper'
// P3: ë‚´ë©´ê°ˆë“± â†’ Risk ì¡°ì •
import {
  calculateConflictRisk,
  calculateConflictSeverity,
} from './internal-conflict-risk'
// ìë™í™” í…ŒìŠ¤íŠ¸ ì‹œë‚˜ë¦¬ì˜¤
import {
  TEST_SCENARIOS,
  getScenarioById,
  getAllScenarioSummary,
  findAutoAnswer,
  getCanValidationAnswer,
  getConstraintIntensityAnswer,
  verifyTestResults,
  type TestScenario,
  type TestVerificationResult,
} from './test-scenarios'
import {
  handleFollowupNo,
  applyDiversityGuard,
  type FollowupNoResult,
  type RankChangeInfo,
} from './safe-replacement'
import {
  buildEvidenceLinks,
  generateDefaultEvidence,
  type Fact as EvidenceFact,
  type ScoredJobForEvidence,
} from './evidence-generator'
import {
  generatePremiumReport,
  type PremiumReportInput,
} from './premium-report-generator'
import {
  generatePremiumReportV3 as generateLLMPremiumReport,
  fixParticlesDeep,
  type ReporterInput,
} from './llm-reporter'
import {
  generatePurposeBasedFollowups,
  type PurposeBasedFollowupInput,
} from './question-generation'
import { TOKEN_TO_KOREAN } from './mini-module-questions'
import {
  saveConversationTurn,
  saveProfileSnapshot,
  buildProfileFromTurns,
  getNextTurnNumber,
  getLatestProfileSnapshot,
  extractSignalsFromAnswer,
  type TurnType,
  type ProfileSnapshot,
} from './user-profile'
import { draftRoutes } from './draft-routes'
import { historyRoutes } from './history-routes'
import { resumeRoutes } from './resume-routes'
import { profileRoutes } from './profile-routes'
import {
  expandCandidates,
  expandCandidatesV3WithCache,
  expandCandidatesV3,
  buildSearchProfileFromMiniModule,
  vectorResultsToScoredJobs,
  buildSearchQuery,
  indexJobsToVectorize,
  extractJobDescription,
} from './vectorize-pipeline'
import { calculatePersonalizedBaseScores } from './personalized-scoring'
import { generateOpenAIEmbedding, OPENAI_EMBEDDING_DIMENSIONS } from './openai-client'
import {
  buildAggregatedProfile,
  assertReadyForLLM,
  createEmptyProfile,
  type AggregatedProfile,
  type GatePhase,
  type DraftData as AggDraftData,
} from './aggregated-profile'
import { updateMemory } from './llm-memory'
// LLM ëª¨ë“ˆ í™œì„±í™” (2026-02-03)
import { 
  judgeCandidates, 
  type JudgeInput, 
  type JudgeOutput 
} from './llm-judge'
import {
  calculateConfidenceScore,
  extractKeyDecisionVariables,
  createScoringTrace,
  type ScoringTrace,
  type KeyDecisionVariable,
  type ConfidenceResult,
} from './scoring-trace'
import {
  synthesizeVectorInsights,
  synthesizeVectorInsightsSync,
  visToPromptHints,
  type VISOutput,
} from './vis-synthesizer'
import {
  getOrCreateCAGState,
  saveCAGState,
  logAskedQuestion,
  logAnswerReceived,
  isQuestionAlreadyAsked,
  // P2: ìˆ˜ì§‘ ì§„í–‰ë„ ì¶”ì 
  updateCollectionProgress,
  syncCollectionProgressFromFacts,
  getCollectionProgressSummary,
  getMostNeededDimension,
  type CAGState,
} from './cag-manager'
// P0: Can ê²€ì¦ ì§ˆë¬¸
import {
  selectCanValidationQuestions,
  calculateCanBoost,
  type StrengthToken,
  type CanValidationQuestion,
} from './can-validation-questions'

// ============================================
// Error Handling (V3 í‘œì¤€í™”)
// ============================================
type ErrorCode = 
  | 'VALIDATION_ERROR'     // 400: ì…ë ¥ ê²€ì¦ ì‹¤íŒ¨
  | 'INVALID_STAGE'        // 400: ì˜ëª»ëœ Stage
  | 'INVALID_PAYLOAD'      // 400: ì˜ëª»ëœ í˜ì´ë¡œë“œ í˜•ì‹
  | 'SESSION_NOT_FOUND'    // 404: ì„¸ì…˜ ì—†ìŒ
  | 'REQUEST_NOT_FOUND'    // 404: ë¶„ì„ ìš”ì²­ ì—†ìŒ
  | 'RESULT_NOT_FOUND'     // 404: ë¶„ì„ ê²°ê³¼ ì—†ìŒ
  | 'DB_ERROR'             // 500: ë°ì´í„°ë² ì´ìŠ¤ ì˜¤ë¥˜
  | 'ANALYSIS_FAILED'      // 500: ë¶„ì„ ì²˜ë¦¬ ì‹¤íŒ¨
  | 'INTERNAL_ERROR'       // 500: ë‚´ë¶€ ì˜¤ë¥˜

interface ApiError {
  error: string
  code: ErrorCode
  details?: Record<string, unknown>
  request_id?: number
  timestamp: string
}

function createErrorResponse(
  code: ErrorCode,
  message: string,
  details?: Record<string, unknown>,
  requestId?: number
): ApiError {
  return {
    error: message,
    code,
    details,
    request_id: requestId,
    timestamp: new Date().toISOString(),
  }
}

// Error logging (development only - no file logging)
function logError(
  code: ErrorCode, 
  message: string, 
  context?: Record<string, unknown>
): void {
  console.error(`[AI-ANALYZER ERROR] ${code}: ${message}`, context ? JSON.stringify(context) : '')
}

// ============================================
// Phase 4 Metrics Events Helper
// ============================================
async function savePhase4MetricsEvents(
  db: D1Database,
  sessionId: string,
  userId: string | undefined,
  requestId: number,
  result: AnalysisResultJSON
): Promise<void> {
  try {
    // Phase 4 ì ìš© ì—¬ë¶€ í™•ì¸
    if (!result.phase4_applied) return
    
    // 1. Diversity Guard ì ìš© ì´ë²¤íŠ¸
    if (result.diversity_guard_active) {
      await db.prepare(`
        INSERT INTO raw_events (user_id, session_id, event_type, payload_json)
        VALUES (?, ?, 'DIVERSITY_GUARD_APPLIED', ?)
      `).bind(
        userId || null,
        sessionId,
        JSON.stringify({
          request_id: requestId,
          changes: result.diversity_changes || [],
          top3_jobs: result.fit_top3?.map(j => j.job_name) || [],
        })
      ).run()
    }
    
    // 2. Research Bias Cap ì ìš© ì—¬ë¶€ í™•ì¸ (diversity_changesì—ì„œ ì¶”ë¡ )
    const researchBiasApplied = (result.diversity_changes || []).some(
      change => change.includes('ì—°êµ¬') || change.includes('research') || change.includes('Diversity Guard')
    )
    if (researchBiasApplied) {
      await db.prepare(`
        INSERT INTO raw_events (user_id, session_id, event_type, payload_json)
        VALUES (?, ?, 'RESEARCH_BIAS_CAP_APPLIED', ?)
      `).bind(
        userId || null,
        sessionId,
        JSON.stringify({
          request_id: requestId,
          original_changes: result.diversity_changes || [],
        })
      ).run()
    }
    
  } catch (error) {
    // ë©”íŠ¸ë¦­ìŠ¤ ì €ì¥ ì‹¤íŒ¨ëŠ” ë¶„ì„ ê²°ê³¼ì— ì˜í–¥ ì—†ìŒ
    console.error('Phase4 metrics event save failed:', error)
  }
}

// ============================================
// Bindings (Cloudflare Workers)
// ============================================
interface Bindings {
  DB: D1Database
  VECTORIZE?: VectorizeIndex
  AI?: Ai
  GEMINI_API_KEY?: string
  OPENAI_API_KEY?: string  // .dev.varsì—ì„œ ë¡œë“œë¨
  JWT_SECRET: string
  [key: string]: unknown
}

// ============================================
// AI Analyzer Routes
// ============================================
const analyzerRoutes = new Hono<{ Bindings: Bindings }>()

// ============================================
// POST /analyze - ë¶„ì„ ìš”ì²­ (V3: Stage-based + V2 í˜¸í™˜)
// ============================================
interface AnalysisRequestPayloadV2 extends AnalysisRequestPayload {
  deep_intake?: DeepIntakeInput
}

// V3 ë˜ëŠ” V2 í˜ì´ë¡œë“œ ë‘˜ ë‹¤ ì²˜ë¦¬
type AnalyzePayload = AnalysisRequestPayloadV3 | AnalysisRequestPayloadV2

analyzerRoutes.post('/analyze', async (c) => {
  const db = c.env.DB
  const rawPayload = await c.req.json<AnalyzePayload>()
  
  // ì¸ì¦ëœ ì‚¬ìš©ì ID ê°€ì ¸ì˜¤ê¸° (authMiddlewareì—ì„œ ì„¤ì •)
  const authUser = (c as any).get('user') as { id: number } | null
  const userId = authUser?.id?.toString() || rawPayload.user_id || null
  
  try {
    // V3 vs V2 íŒë³„
    const isV3 = 'stage' in rawPayload && 'universal_answers' in rawPayload
    
    // 1. ì…ë ¥ ê²€ì¦
    if (!rawPayload.session_id) {
      logError('VALIDATION_ERROR', 'session_id is required')
      return c.json(createErrorResponse('VALIDATION_ERROR', 'session_id is required'), 400)
    }
    
    // V3 ì¶”ê°€ ê²€ì¦
    let stage: AnalysisStage | undefined
    let universalAnswers: UniversalAnswers = {}
    let debugMode = false
    
    if (isV3) {
      const v3Payload = rawPayload as AnalysisRequestPayloadV3
      debugMode = v3Payload.debug || false
      
      if (!isValidStage(v3Payload.stage)) {
        logError('INVALID_STAGE', `Invalid stage: ${v3Payload.stage}`, { provided: v3Payload.stage })
        return c.json(createErrorResponse('INVALID_STAGE', `Invalid stage: ${v3Payload.stage}`, {
          provided: v3Payload.stage,
          allowed: ['job_explore', 'job_student', 'job_early', 'major_explore', 'major_student', 'major_early']
        }), 400)
      }
      stage = v3Payload.stage
      universalAnswers = v3Payload.universal_answers || {}
      
      // 2a. Stage ì„ íƒ ì´ë²¤íŠ¸ ì €ì¥ [ìˆ˜ì •ì‚¬í•­ 2: CHECK ì œì•½ ì—†ì´ ë¬¸ìì—´ë¡œ ê¸°ë¡]
      await db.prepare(`
        INSERT INTO raw_events (user_id, session_id, event_type, payload_json, client_meta)
        VALUES (?, ?, 'STAGE_SELECTED', ?, ?)
      `).bind(
        userId,  // ì¸ì¦ëœ ì‚¬ìš©ì ID ì‚¬ìš©
        v3Payload.session_id,
        JSON.stringify({ stage: v3Payload.stage, analysis_type: v3Payload.analysis_type }),
        c.req.header('User-Agent') || null
      ).run()
      
      // 2b. Universal ì œì¶œ ì´ë²¤íŠ¸ ì €ì¥
      if (Object.keys(universalAnswers).length > 0) {
        await db.prepare(`
          INSERT INTO raw_events (user_id, session_id, event_type, payload_json)
          VALUES (?, ?, 'UNIVERSAL_SUBMITTED', ?)
        `).bind(
          v3Payload.user_id || null,
          v3Payload.session_id,
          JSON.stringify(universalAnswers)
        ).run()
        
        // Universal answers â†’ facts ì €ì¥
        await saveUniversalFacts(db, v3Payload.session_id, v3Payload.user_id, universalAnswers, stage)
        
        // ============================================
        // Conversation Turns ì €ì¥ (P1 ê¸°ëŠ¥)
        // ============================================
        try {
          let turnNumber = await getNextTurnNumber(db, v3Payload.session_id)
          
          for (const [questionId, answer] of Object.entries(universalAnswers)) {
            if (answer === null || answer === undefined) continue
            
            const answerStr = Array.isArray(answer) ? answer.join(', ') : String(answer)
            const signals = extractSignalsFromAnswer(answerStr)
            
            await saveConversationTurn(db, {
              session_id: v3Payload.session_id,
              user_id: v3Payload.user_id,
              turn_number: turnNumber,
              turn_type: 'universal_intake',
              question_id: questionId,
              answer_raw: answerStr,
              answer_type: Array.isArray(answer) ? 'multi_choice' : 'text',
              extracted_signals: signals,
            })
            turnNumber++
          }
          console.log(`[V3 Analyze] Saved ${Object.keys(universalAnswers).length} conversation turns`)
        } catch (turnError) {
          // ëŒ€í™” í„´ ì €ì¥ ì‹¤íŒ¨í•´ë„ ë¶„ì„ì€ ê³„ì† ì§„í–‰ (graceful degradation)
          console.error('[V3 Analyze] Conversation turn save failed:', turnError)
        }
      }
      
      // ============================================
      // P0: 5ì¶• ìƒíƒœì¢Œí‘œ ì €ì¥ (career_state)
      // ============================================
      if (v3Payload.career_state) {
        try {
          await saveCareerStateFacts(db, v3Payload.session_id, v3Payload.user_id, v3Payload.career_state)
          console.log(`[V3 Analyze] Saved career_state facts`)
        } catch (stateError) {
          console.error('[V3 Analyze] career_state save failed:', stateError)
        }
      }
      
      // ============================================
      // P0: ì „ì´ ì‹ í˜¸ ì €ì¥ (transition_signal)
      // ============================================
      if (v3Payload.transition_signal) {
        try {
          await saveTransitionSignalFacts(db, v3Payload.session_id, v3Payload.user_id, v3Payload.transition_signal)
          console.log(`[V3 Analyze] Saved transition_signal facts`)
        } catch (transError) {
          console.error('[V3 Analyze] transition_signal save failed:', transError)
        }
      }
    } else {
      // V2: ê¸°ì¡´ ë¡œì§
      const v2Payload = rawPayload as AnalysisRequestPayloadV2
      if (!v2Payload.profile) {
        logError('VALIDATION_ERROR', 'profile is required for V2 payload', { session_id: rawPayload.session_id })
        return c.json(createErrorResponse('VALIDATION_ERROR', 'profile is required for V2 payload'), 400)
      }
      
      // 2. Raw event ì €ì¥ (V2)
      await db.prepare(`
        INSERT INTO raw_events (user_id, session_id, event_type, payload_json, client_meta)
        VALUES (?, ?, 'ANALYSIS_REQUESTED', ?, ?)
      `).bind(
        v2Payload.user_id || null,
        v2Payload.session_id,
        JSON.stringify(v2Payload),
        c.req.header('User-Agent') || null
      ).run()
    }
    
    // 3. Deep Intake ì²˜ë¦¬ (V2/V3 ê³µí†µ)
    let normalizedDeepIntake: NormalizedDeepIntake | undefined
    const deepIntake = (rawPayload as any).deep_intake as DeepIntakeInput | undefined
    
    if (deepIntake) {
      // [ìˆ˜ì •ì‚¬í•­ 3] ë¯¸ì„±ë…„ ë‹¨ê³„ì—ì„œëŠ” Deep Intake ì„œì‚¬í˜• ì§ˆë¬¸ ì œí•œ
      if (stage && isMinorStage(stage)) {
        // ë¯¸ì„±ë…„ ë‹¨ê³„: MBTI, priorityë§Œ í—ˆìš©, ì˜¤í”ˆí…ìŠ¤íŠ¸ëŠ” ë¬´ì‹œ
        const sanitizedDeepIntake: DeepIntakeInput = {
          mbti: deepIntake.mbti,
          priority_top1: deepIntake.priority_top1,
          // best_moment, worst_moment, change_reasonì€ ë¬´ì‹œ
        }
        normalizedDeepIntake = normalizeDeepIntake(sanitizedDeepIntake)
      } else {
        normalizedDeepIntake = normalizeDeepIntake(deepIntake)
      }
      
      // Deep Intake ì´ë²¤íŠ¸ ì €ì¥
      await db.prepare(`
        INSERT INTO raw_events (user_id, session_id, event_type, payload_json)
        VALUES (?, ?, 'DEEP_INTAKE_SUBMITTED', ?)
      `).bind(
        rawPayload.user_id || null,
        rawPayload.session_id,
        JSON.stringify({ raw: deepIntake, normalized: normalizedDeepIntake })
      ).run()
      
      // Deep Intake â†’ facts ì €ì¥
      await saveDeepIntakeFacts(db, rawPayload.session_id, rawPayload.user_id, normalizedDeepIntake)
    }
    
    // 4. ê¸°ì¡´ facts ì¡°íšŒ (ì´ ì„¸ì…˜ì—ì„œ ìˆ˜ì§‘ëœ)
    const existingFacts = await db.prepare(`
      SELECT fact_key, value_json, confidence, fact_level
      FROM facts
      WHERE session_id = ?
      ORDER BY fact_level ASC, collected_at DESC
    `).bind(rawPayload.session_id).all<{
      fact_key: string
      value_json: string
      confidence: number
      fact_level: number
    }>()
    
    const facts = existingFacts.results || []
    
    // 5. ë¶„ì„ ìš”ì²­ ìƒì„± (ë²„ì „ ì ê¸ˆ)
    const analysisType = isV3 
      ? ((rawPayload as AnalysisRequestPayloadV3).analysis_type || 'job')
      : ((rawPayload as AnalysisRequestPayloadV2).analysis_type || 'job')
    
    const promptPayload = isV3
      ? JSON.stringify({ stage, universal_answers: universalAnswers })
      : JSON.stringify((rawPayload as AnalysisRequestPayloadV2).profile)
    
    const requestResult = await db.prepare(`
      INSERT INTO ai_analysis_requests (
        session_id, user_id, analysis_type, pricing_tier, prompt_payload,
        status, recipe_version, tagger_version, scoring_version
      )
      VALUES (?, ?, ?, ?, ?, 'processing', ?, ?, ?)
      RETURNING id
    `).bind(
      rawPayload.session_id,
      userId,  // ì¸ì¦ëœ ì‚¬ìš©ì ID ì‚¬ìš©
      analysisType,
      (rawPayload as any).pricing_tier || 'free',
      promptPayload,
      VERSIONS.recipe,
      VERSIONS.tagger,
      VERSIONS.scoring
    ).first<{ id: number }>()
    
    if (!requestResult) {
      logError('DB_ERROR', 'Failed to create analysis request', { session_id: rawPayload.session_id })
      return c.json(createErrorResponse('DB_ERROR', 'Failed to create analysis request'), 500)
    }
    
    const requestId = requestResult.id
    
    // 6. ë¶„ì„ ì‹¤í–‰ (V3ëŠ” stage + debug ì „ë‹¬ + Vectorize í™•ì¥)
    const env = c.env as Bindings
    const result = await runAnalysisV3(
      db, 
      rawPayload, 
      requestId, 
      facts, 
      normalizedDeepIntake,
      stage,
      debugMode,
      { VECTORIZE: env.VECTORIZE, AI: env.AI }
    )
    
    // 7. ê²°ê³¼ ì €ì¥
    await db.prepare(`
      INSERT INTO ai_analysis_results (request_id, result_json)
      VALUES (?, ?)
    `).bind(requestId, JSON.stringify(result)).run()
    
    // 8. ìš”ì²­ ìƒíƒœ ì—…ë°ì´íŠ¸
    await db.prepare(`
      UPDATE ai_analysis_requests
      SET status = 'completed', processed_at = CURRENT_TIMESTAMP
      WHERE id = ?
    `).bind(requestId).run()
    
    // 9. ì™„ë£Œ ì´ë²¤íŠ¸ ì €ì¥
    await db.prepare(`
      INSERT INTO raw_events (user_id, session_id, event_type, payload_json)
      VALUES (?, ?, 'ANALYSIS_COMPLETED', ?)
    `).bind(
      rawPayload.user_id || null,
      rawPayload.session_id,
      JSON.stringify({ 
        request_id: requestId, 
        facts_applied: facts.length,
        has_deep_intake: !!normalizedDeepIntake,
        stage: stage || null,
        is_v3: isV3,
      })
    ).run()
    
    // 10. Phase 4 ë©”íŠ¸ë¦­ìŠ¤ ì´ë²¤íŠ¸ ì €ì¥
    await savePhase4MetricsEvents(db, rawPayload.session_id, rawPayload.user_id, requestId, result)
    
    return c.json({
      success: true,
      request_id: requestId,
      result,
      facts_applied: facts.length,
      deep_intake_processed: !!normalizedDeepIntake,
      stage: stage || null,
    })
    
  } catch (error) {
    const message = error instanceof Error ? error.message : 'Unknown error'
    const stack = error instanceof Error ? error.stack : undefined
    console.error('[ANALYSIS_FAILED]', message, stack)
    logError('ANALYSIS_FAILED', message, {
      session_id: rawPayload.session_id,
      stack
    })
    return c.json(createErrorResponse('ANALYSIS_FAILED', 'Analysis failed', {
      message,
      stack,
      session_id: rawPayload.session_id
    }), 500)
  }
})

// ============================================
// P0: 5ì¶• ìƒíƒœì¢Œí‘œ â†’ facts ì €ì¥
// ============================================
async function saveCareerStateFacts(
  db: D1Database,
  sessionId: string,
  userId: string | undefined,
  careerState: {
    role_identity: string
    career_stage_years: string
    transition_status: string
    skill_level: number
    constraints: Record<string, { has_constraint: boolean; details?: string }>
  }
): Promise<void> {
  // ì¶• 1: role_identity
  if (careerState.role_identity) {
    await db.prepare(`
      INSERT INTO facts (session_id, user_id, fact_key, value_json, confidence, source_type, fact_level)
      VALUES (?, ?, 'state.role_identity', ?, 1.0, 'career_state', 2)
      ON CONFLICT(session_id, fact_key) DO UPDATE SET
        value_json = excluded.value_json,
        collected_at = CURRENT_TIMESTAMP
    `).bind(sessionId, userId || null, JSON.stringify({ value: careerState.role_identity })).run()
  }
  
  // ì¶• 2: career_stage_years
  if (careerState.career_stage_years) {
    await db.prepare(`
      INSERT INTO facts (session_id, user_id, fact_key, value_json, confidence, source_type, fact_level)
      VALUES (?, ?, 'state.career_stage_years', ?, 1.0, 'career_state', 2)
      ON CONFLICT(session_id, fact_key) DO UPDATE SET
        value_json = excluded.value_json,
        collected_at = CURRENT_TIMESTAMP
    `).bind(sessionId, userId || null, JSON.stringify({ value: careerState.career_stage_years })).run()
  }
  
  // ì¶• 3: transition_status
  if (careerState.transition_status) {
    await db.prepare(`
      INSERT INTO facts (session_id, user_id, fact_key, value_json, confidence, source_type, fact_level)
      VALUES (?, ?, 'state.transition_status', ?, 1.0, 'career_state', 2)
      ON CONFLICT(session_id, fact_key) DO UPDATE SET
        value_json = excluded.value_json,
        collected_at = CURRENT_TIMESTAMP
    `).bind(sessionId, userId || null, JSON.stringify({ value: careerState.transition_status })).run()
  }
  
  // ì¶• 4: skill_level
  if (careerState.skill_level !== undefined) {
    await db.prepare(`
      INSERT INTO facts (session_id, user_id, fact_key, value_json, confidence, source_type, fact_level)
      VALUES (?, ?, 'state.skill_level', ?, 1.0, 'career_state', 2)
      ON CONFLICT(session_id, fact_key) DO UPDATE SET
        value_json = excluded.value_json,
        collected_at = CURRENT_TIMESTAMP
    `).bind(sessionId, userId || null, JSON.stringify({ value: careerState.skill_level })).run()
  }
  
  // ì¶• 5: constraints (P0-6: 2ë‹¨ ì €ì¥)
  for (const [type, constraint] of Object.entries(careerState.constraints || {})) {
    // has_constraint ì €ì¥
    await db.prepare(`
      INSERT INTO facts (session_id, user_id, fact_key, value_json, confidence, source_type, fact_level)
      VALUES (?, ?, ?, ?, 1.0, 'career_state', 2)
      ON CONFLICT(session_id, fact_key) DO UPDATE SET
        value_json = excluded.value_json,
        collected_at = CURRENT_TIMESTAMP
    `).bind(
      sessionId,
      userId || null,
      `state.constraint.${type}`,
      JSON.stringify({ value: !!constraint.has_constraint })
    ).run()
    
    // detail ì €ì¥ (ìˆì„ ë•Œë§Œ)
    if (constraint.has_constraint && constraint.details) {
      await db.prepare(`
        INSERT INTO facts (session_id, user_id, fact_key, value_json, confidence, source_type, fact_level)
        VALUES (?, ?, ?, ?, 1.0, 'career_state', 2)
        ON CONFLICT(session_id, fact_key) DO UPDATE SET
          value_json = excluded.value_json,
          collected_at = CURRENT_TIMESTAMP
      `).bind(
        sessionId,
        userId || null,
        `state.constraint.${type}_detail`,
        JSON.stringify({ value: constraint.details })
      ).run()
    }
  }
}

// ============================================
// P0: ì „ì´ ì‹ í˜¸ â†’ facts ì €ì¥
// ============================================
async function saveTransitionSignalFacts(
  db: D1Database,
  sessionId: string,
  userId: string | undefined,
  transitionSignal: Record<string, string | string[]>
): Promise<void> {
  for (const [questionId, answer] of Object.entries(transitionSignal)) {
    if (answer === null || answer === undefined) continue
    
    // fact_key ë§¤í•‘
    const factKeyMap: Record<string, string> = {
      'trans_desired_type': 'transition.desired_type',
      'trans_motivation': 'transition.motivation_primary',
      'trans_blockers': 'transition.blocker',
      'trans_timeline': 'transition.timeline',
      'trans_time_invest': 'transition.time_invest_hours_bucket',
    }
    
    const factKey = factKeyMap[questionId] || `transition.${questionId}`
    
    if (Array.isArray(answer)) {
      // ë°°ì—´ ë‹µë³€: ranked ì €ì¥ (ìˆœìœ„ í¬í•¨)
      for (let i = 0; i < answer.length; i++) {
        const rankFactKey = `${factKey}[${i}]`
        await db.prepare(`
          INSERT INTO facts (session_id, user_id, fact_key, value_json, confidence, source_type, fact_level)
          VALUES (?, ?, ?, ?, 1.0, 'transition_signal', 3)
          ON CONFLICT(session_id, fact_key) DO UPDATE SET
            value_json = excluded.value_json,
            collected_at = CURRENT_TIMESTAMP
        `).bind(
          sessionId,
          userId || null,
          rankFactKey,
          JSON.stringify({ value: answer[i], rank: i + 1 })
        ).run()
      }
      
      // ì „ì²´ ë°°ì—´ë„ ì €ì¥ (í¸ì˜ìš©)
      await db.prepare(`
        INSERT INTO facts (session_id, user_id, fact_key, value_json, confidence, source_type, fact_level)
        VALUES (?, ?, ?, ?, 1.0, 'transition_signal', 3)
        ON CONFLICT(session_id, fact_key) DO UPDATE SET
          value_json = excluded.value_json,
          collected_at = CURRENT_TIMESTAMP
      `).bind(
        sessionId,
        userId || null,
        factKey,
        JSON.stringify({ value: answer })
      ).run()
    } else {
      // ë‹¨ì¼ ë‹µë³€
      await db.prepare(`
        INSERT INTO facts (session_id, user_id, fact_key, value_json, confidence, source_type, fact_level)
        VALUES (?, ?, ?, ?, 1.0, 'transition_signal', 3)
        ON CONFLICT(session_id, fact_key) DO UPDATE SET
          value_json = excluded.value_json,
          collected_at = CURRENT_TIMESTAMP
      `).bind(
        sessionId,
        userId || null,
        factKey,
        JSON.stringify({ value: answer })
      ).run()
    }
  }
}

// ============================================
// V3: Universal Answers â†’ facts ì €ì¥
// ============================================
async function saveUniversalFacts(
  db: D1Database,
  sessionId: string,
  userId: string | undefined,
  answers: UniversalAnswers,
  stage?: AnalysisStage
): Promise<void> {
  const UNIVERSAL_QUESTIONS_MAP = new Map(
    UNIVERSAL_QUESTIONS.map(q => [q.question_id, q])
  )
  
  // Universal ì œì•½ì¡°ê±´ â†’ confirmed_constraint ë§¤í•‘
  // ì‚¬ìš©ìê°€ "ì ˆëŒ€ ë¶ˆê°€"ë¡œ ì„ íƒí•œ ì œì•½ì€ Phase 4 Hard Filter ëŒ€ìƒ
  const CONSTRAINT_TO_CONFIRMED: Record<string, string> = {
    'no_overtime': 'work_hours_strict',
    'no_shift': 'shift_work_no',
    'no_travel': 'travel_impossible',
    'remote_only': 'remote_only',
    'no_degree': 'degree_impossible',
    'no_license': 'license_impossible',
  }
  
  for (const [questionId, answer] of Object.entries(answers)) {
    if (answer === null || answer === undefined) continue
    if (Array.isArray(answer) && answer.length === 0) continue
    if (typeof answer === 'string' && answer.trim() === '') continue
    
    const question = UNIVERSAL_QUESTIONS_MAP.get(questionId)
    if (!question) continue
    
    // ì •ê·œí™”
    const normalized = normalizeUniversalAnswer(question, answer)
    
    // fact_level ê²°ì •
    const factLevel = determineUniversalFactLevel(question.fact_key)
    
    await db.prepare(`
      INSERT INTO facts (session_id, user_id, fact_key, value_json, confidence, source_type, fact_level, question_id)
      VALUES (?, ?, ?, ?, 1.0, 'universal', ?, ?)
      ON CONFLICT(session_id, fact_key) DO UPDATE SET
        value_json = excluded.value_json,
        collected_at = CURRENT_TIMESTAMP
    `).bind(
      sessionId,
      userId || null,
      question.fact_key,
      JSON.stringify(normalized),
      factLevel,
      questionId
    ).run()
    
    // ğŸ”¥ Phase 4 Hard Filter ìë™ ìŠ¹ê²©
    // ì œì•½ì¡°ê±´ ì§ˆë¬¸(univ_constraint_*)ì´ë©´ confirmed_constraintë„ ì €ì¥
    if (questionId.startsWith('univ_constraint_')) {
      const values = Array.isArray(answer) ? answer : [answer]
      
      for (const value of values) {
        const confirmedType = CONSTRAINT_TO_CONFIRMED[value]
        if (confirmedType) {
          await db.prepare(`
            INSERT INTO facts (session_id, user_id, fact_key, value_json, confidence, source_type, fact_level)
            VALUES (?, ?, ?, ?, 1.0, 'universal', 1)
            ON CONFLICT(session_id, fact_key) DO UPDATE SET
              value_json = excluded.value_json,
              fact_level = 1,
              collected_at = CURRENT_TIMESTAMP
          `).bind(
            sessionId,
            userId || null,
            `confirmed_constraint.${confirmedType}`,
            JSON.stringify({ 
              confirmed: true, 
              source: 'universal_intake',
              original_value: value,
              confirmed_at: new Date().toISOString() 
            })
          ).run()
        }
      }
    }
  }
}

// Universal ë‹µë³€ ì •ê·œí™”
function normalizeUniversalAnswer(
  question: UniversalQuestion,
  answer: string | string[]
): { value: string | string[]; tags: string[]; raw?: string } {
  const values = Array.isArray(answer) ? answer : [answer]
  const tags: string[] = []
  
  // ì˜µì…˜ì—ì„œ íƒœê·¸ ì¶”ì¶œ
  if (question.options) {
    for (const val of values) {
      const option = question.options.find(o => o.value === val)
      if (option?.tags) {
        tags.push(...option.tags)
      }
    }
  }
  
  // í…ìŠ¤íŠ¸ ì •ê·œí™” (freetextìš©)
  if (question.normalize_rule === 'keywords' && typeof answer === 'string') {
    return {
      value: answer,
      tags: extractKeywordTags(answer),
      raw: answer,
    }
  }
  
  return {
    value: answer,
    tags: [...new Set(tags)],
  }
}

// í‚¤ì›Œë“œ ê¸°ë°˜ íƒœê·¸ ì¶”ì¶œ (freetextìš©)
function extractKeywordTags(text: string): string[] {
  const tags: string[] = []
  const lowerText = text.toLowerCase()
  
  // ê°„ë‹¨í•œ í‚¤ì›Œë“œ ë§¤ì¹­
  const keywordMap: Record<string, string[]> = {
    'ê±´ê°•': ['health'],
    'ì•„í”„': ['health'],
    'ëŒë´„': ['caregiving'],
    'ê°€ì¡±': ['caregiving', 'family'],
    'ì•„ì´': ['caregiving'],
    'ì›ê²©': ['remote'],
    'ì¬íƒ': ['remote'],
    'ì•¼ê·¼': ['work_hours_strict'],
    'ì €ë…': ['work_hours_strict', 'wlb'],
    'ì¶œí‡´ê·¼': ['commute'],
  }
  
  for (const [keyword, keywordTags] of Object.entries(keywordMap)) {
    if (lowerText.includes(keyword)) {
      tags.push(...keywordTags)
    }
  }
  
  return [...new Set(tags)]
}

// Universal fact_level ê²°ì •
function determineUniversalFactLevel(factKey: string): number {
  if (factKey.startsWith('profile.constraints')) return 2
  if (factKey.startsWith('priority.')) return 2
  if (factKey.startsWith('profile.life_constraint')) return 2
  if (factKey.startsWith('profile.')) return 4
  if (factKey.startsWith('discovery.')) return 3
  return 4
}

// ============================================
// Phase 1C: Deep Intake â†’ facts ì €ì¥
// ============================================
async function saveDeepIntakeFacts(
  db: D1Database,
  sessionId: string,
  userId: string | undefined,
  normalized: NormalizedDeepIntake
): Promise<void> {
  // MBTI ì €ì¥
  if (normalized.mbti) {
    await db.prepare(`
      INSERT INTO facts (session_id, user_id, fact_key, value_json, confidence, source_type, fact_level)
      VALUES (?, ?, 'profile.mbti', ?, 1.0, 'deep_intake', 4)
      ON CONFLICT(session_id, fact_key) DO UPDATE SET
        value_json = excluded.value_json,
        collected_at = CURRENT_TIMESTAMP
    `).bind(
      sessionId,
      userId || null,
      JSON.stringify({ value: normalized.mbti, traits: normalized.mbti_traits })
    ).run()
  }
  
  // best_moment ì €ì¥
  if (normalized.best_moment) {
    await db.prepare(`
      INSERT INTO facts (session_id, user_id, fact_key, value_json, confidence, source_type, fact_level)
      VALUES (?, ?, 'discovery.best_moment', ?, 0.9, 'deep_intake', 3)
      ON CONFLICT(session_id, fact_key) DO UPDATE SET
        value_json = excluded.value_json,
        collected_at = CURRENT_TIMESTAMP
    `).bind(
      sessionId,
      userId || null,
      JSON.stringify(normalized.best_moment)
    ).run()
  }
  
  // worst_moment ì €ì¥
  if (normalized.worst_moment) {
    await db.prepare(`
      INSERT INTO facts (session_id, user_id, fact_key, value_json, confidence, source_type, fact_level)
      VALUES (?, ?, 'discovery.worst_moment', ?, 0.9, 'deep_intake', 3)
      ON CONFLICT(session_id, fact_key) DO UPDATE SET
        value_json = excluded.value_json,
        collected_at = CURRENT_TIMESTAMP
    `).bind(
      sessionId,
      userId || null,
      JSON.stringify(normalized.worst_moment)
    ).run()
  }
  
  // change_reason ì €ì¥
  if (normalized.change_reason) {
    await db.prepare(`
      INSERT INTO facts (session_id, user_id, fact_key, value_json, confidence, source_type, fact_level)
      VALUES (?, ?, 'motivation.change_reason', ?, 0.9, 'deep_intake', 3)
      ON CONFLICT(session_id, fact_key) DO UPDATE SET
        value_json = excluded.value_json,
        collected_at = CURRENT_TIMESTAMP
    `).bind(
      sessionId,
      userId || null,
      JSON.stringify(normalized.change_reason)
    ).run()
  }
  
  // priority.top1 ì €ì¥
  if (normalized.priority_top1) {
    await db.prepare(`
      INSERT INTO facts (session_id, user_id, fact_key, value_json, confidence, source_type, fact_level)
      VALUES (?, ?, 'priority.top1', ?, 1.0, 'deep_intake', 2)
      ON CONFLICT(session_id, fact_key) DO UPDATE SET
        value_json = excluded.value_json,
        collected_at = CURRENT_TIMESTAMP
    `).bind(
      sessionId,
      userId || null,
      JSON.stringify({ value: normalized.priority_top1 })
    ).run()
  }
}

// ============================================
// POST /followup - Follow-up ì‘ë‹µ ì²˜ë¦¬ (facts ì €ì¥)
// ============================================
analyzerRoutes.post('/followup', async (c) => {
  const db = c.env.DB
  const payload = await c.req.json<FollowupPayloadV2>()
  
  try {
    // 1. í•„ìˆ˜ í•„ë“œ ê²€ì¦
    if (!payload.session_id || !payload.question_id || payload.answer === undefined) {
      logError('VALIDATION_ERROR', 'session_id, question_id, and answer are required')
      return c.json(createErrorResponse('VALIDATION_ERROR', 'session_id, question_id, and answer are required'), 400)
    }
    
    // 2. Raw event ì €ì¥ (ì›ë¬¸ ê·¸ëŒ€ë¡œ)
    await db.prepare(`
      INSERT INTO raw_events (user_id, session_id, event_type, payload_json)
      VALUES (?, ?, 'FOLLOWUP_ANSWERED', ?)
    `).bind(
      payload.user_id || null,
      payload.session_id,
      JSON.stringify(payload)
    ).run()
    
    // ============================================
    // Conversation Turn ì €ì¥ (P1 ê¸°ëŠ¥)
    // ============================================
    try {
      const turnNumber = await getNextTurnNumber(db, payload.session_id)
      const answerStr = payload.answer_text || String(payload.answer)
      const signals = extractSignalsFromAnswer(answerStr)
      
      // FollowupV3 íƒ€ì…ì¸ì§€ í™•ì¸ (purpose ê¸°ë°˜)
      const turnType: TurnType = payload.question_type ? 
        (['contradiction_resolver', 'decision_variable', 'reality_constraint'].includes(payload.question_type) 
          ? 'followup_v3' 
          : 'followup_v2') 
        : 'followup_v2'
      
      await saveConversationTurn(db, {
        session_id: payload.session_id,
        user_id: payload.user_id,
        request_id: payload.request_id,
        turn_number: turnNumber,
        turn_type: turnType,
        question_id: payload.question_id,
        question_type: payload.question_type as any,
        answer_raw: answerStr,
        answer_type: payload.answer_text ? 'text' : 'single_choice',
        extracted_signals: signals,
        affected_dimensions: payload.affects_attributes,
      })
      console.log(`[Followup] Saved conversation turn #${turnNumber} for session: ${payload.session_id}`)
    } catch (turnError) {
      // ëŒ€í™” í„´ ì €ì¥ ì‹¤íŒ¨í•´ë„ followup ì²˜ë¦¬ëŠ” ê³„ì† ì§„í–‰
      console.error('[Followup] Conversation turn save failed:', turnError)
    }
    
    // 3. fact_keyì™€ value ê²°ì •
    const factKey = payload.fact_key || `answer.${payload.question_id}`
    const factLevel = determineFctLevel(factKey)
    
    // 4. facts í…Œì´ë¸”ì— ì €ì¥ (UPSERT - ê°™ì€ session+fact_keyë©´ ê°±ì‹ )
    await db.prepare(`
      INSERT INTO facts (
        session_id, user_id, fact_key, value_json, confidence, question_id, 
        source_type, fact_level
      )
      VALUES (?, ?, ?, ?, ?, ?, 'followup', ?)
      ON CONFLICT(session_id, fact_key) DO UPDATE SET
        value_json = excluded.value_json,
        confidence = excluded.confidence,
        collected_at = CURRENT_TIMESTAMP
    `).bind(
      payload.session_id,
      payload.user_id || null,
      factKey,
      JSON.stringify({ value: payload.answer, raw: payload.answer_text }),
      payload.confidence || 1.0,
      payload.question_id,
      factLevel
    ).run()
    
    // 5. question_history ì—…ë°ì´íŠ¸
    await db.prepare(`
      INSERT INTO question_history (session_id, question_id, question_type, attribute, answer_value)
      VALUES (?, ?, ?, ?, ?)
      ON CONFLICT(session_id, question_id) DO UPDATE SET
        answered_at = CURRENT_TIMESTAMP,
        answer_value = excluded.answer_value
    `).bind(
      payload.session_id,
      payload.question_id,
      payload.question_type || 'unknown',
      payload.attribute || null,
      payload.answer
    ).run()
    
    // 6. ê¸°ì¡´ constraint ê¸°ë°˜ followup ì²˜ë¦¬ (backward compatibility)
    if (payload.constraint && payload.request_id) {
      const constraint = assertConstraintType(payload.constraint)
      
      await db.prepare(`
        INSERT INTO followup_responses (
          request_id, question_id, constraint_type, job_id, job_name, answer
        )
        VALUES (?, ?, ?, ?, ?, ?)
      `).bind(
        payload.request_id,
        payload.question_id,
        constraint,
        payload.job_id || null,
        payload.job_name || null,
        payload.answer
      ).run()
      
      if (payload.answer === 'no') {
        await db.prepare(`
          INSERT OR IGNORE INTO confirmed_constraints (
            session_id, user_id, request_id, constraint_type, constraint_value
          )
          VALUES (?, ?, ?, ?, ?)
        `).bind(
          payload.session_id,
          payload.user_id || null,
          payload.request_id,
          constraint,
          'true'
        ).run()
      }
    }
    
    // 7. Phase 4: answer="no"ë©´ Safe Replacement ì²˜ë¦¬
    if (payload.answer === 'no' && payload.job_id && payload.request_id) {
      // í˜„ì¬ ê²°ê³¼ì—ì„œ í›„ë³´êµ°ê³¼ TOP3 ê°€ì ¸ì˜¤ê¸°
      const existingResult = await db.prepare(`
        SELECT result_json FROM ai_analysis_results WHERE request_id = ?
      `).bind(payload.request_id).first<{ result_json: string }>()
      
      if (existingResult) {
        try {
          const resultData = JSON.parse(existingResult.result_json)
          
          // TOP3ì™€ ì „ì²´ í›„ë³´êµ° êµ¬ì„±
          const originalTop3: ScoredJob[] = (resultData.fit_top3 || []).map((j: any) => ({
            job_id: j.job_id,
            job_name: j.job_name,
            scores: {
              fit: j.fit_score || 0,
              like: j.like_score || 0,
              can: j.can_score || 0,
              risk_penalty: 0,
            },
            attributes: {},
          }))
          
          // ì „ì²´ í›„ë³´ëŠ” like_top10ê³¼ can_top10 í•©ì³ì„œ êµ¬ì„± (dedupe)
          const allJobIds = new Set<string>()
          const allCandidates: ScoredJob[] = []
          
          for (const source of [resultData.fit_top3, resultData.like_top10, resultData.can_top10]) {
            if (!source) continue
            for (const j of source) {
              if (!allJobIds.has(j.job_id)) {
                allJobIds.add(j.job_id)
                allCandidates.push({
                  job_id: j.job_id,
                  job_name: j.job_name,
                  scores: {
                    fit: j.fit_score || j.like_score || 0,
                    like: j.like_score || 0,
                    can: j.can_score || 0,
                    risk_penalty: 0,
                  },
                  attributes: {},
                })
              }
            }
          }
          
          // constraint íƒ€ì… ê²°ì •
          const constraintType = payload.constraint || factKey.replace('answer.', '')
          
          // Phase 4 Safe Replacement ì‹¤í–‰
          const safeResult = await handleFollowupNo(
            db,
            payload.session_id,
            payload.user_id,
            payload.question_id,
            constraintType,
            payload.job_id,
            allCandidates,
            originalTop3,
            payload.request_id
          )
          
          // Phase 4 ê²°ê³¼ ë°˜í™˜
          return c.json({
            success: true,
            phase4_applied: true,
            fact_saved: {
              fact_key: factKey,
              value: payload.answer,
              fact_level: factLevel,
            },
            ...safeResult,
          })
          
        } catch (error) {
          console.error('Phase 4 Safe Replacement error:', error)
          // Phase 4 ì‹¤íŒ¨í•´ë„ ê¸°ë³¸ ì‘ë‹µì€ ë°˜í™˜
        }
      }
    }
    
    // 8. ì‘ë‹µ êµ¬ì„± (Phase 4 ë¯¸ì ìš© ì‹œ)
    const result: FollowupResultV2 = {
      success: true,
      fact_saved: {
        fact_key: factKey,
        value: payload.answer,
        fact_level: factLevel,
      },
      reanalyze_available: true,
      message: 'ë‹µë³€ì´ ì €ì¥ë˜ì—ˆìŠµë‹ˆë‹¤. ì¬ë¶„ì„ ì‹œ ë°˜ì˜ë©ë‹ˆë‹¤.',
    }
    
    return c.json(result)
    
  } catch (error) {
    const message = error instanceof Error ? error.message : 'Unknown error'
    logError('ANALYSIS_FAILED', `Followup failed: ${message}`, {
      stack: error instanceof Error ? error.stack : undefined
    })
    return c.json(createErrorResponse('ANALYSIS_FAILED', 'Followup failed', { message }), 500)
  }
})

// ============================================
// GET /result/:requestId - ê²°ê³¼ ì¡°íšŒ
// ============================================
analyzerRoutes.get('/result/:requestId', async (c) => {
  const db = c.env.DB
  const requestId = parseInt(c.req.param('requestId'), 10)
  
  const providedId = c.req.param('requestId')
  if (isNaN(requestId)) {
    logError('VALIDATION_ERROR', 'Invalid request_id', { provided: providedId })
    return c.json(createErrorResponse('VALIDATION_ERROR', 'Invalid request_id', { provided: providedId }), 400)
  }
  
  try {
    const request = await db.prepare(`
      SELECT * FROM ai_analysis_requests WHERE id = ?
    `).bind(requestId).first()
    
    if (!request) {
      logError('REQUEST_NOT_FOUND', 'Analysis request not found', { request_id: requestId })
      return c.json(createErrorResponse('REQUEST_NOT_FOUND', 'Analysis request not found', { request_id: requestId }), 404)
    }
    
    const result = await db.prepare(`
      SELECT * FROM ai_analysis_results WHERE request_id = ?
    `).bind(requestId).first<{ result_json: string }>()

    const followups = await db.prepare(`
      SELECT * FROM followup_responses WHERE request_id = ?
    `).bind(requestId).all()

    return c.json({
      request,
      result: result ? JSON.parse(result.result_json) : null,
      followups: followups.results,
    })
    
  } catch (error) {
    const message = error instanceof Error ? error.message : 'Unknown error'
    logError('DB_ERROR', `Failed to fetch result: ${message}`, {
      request_id: requestId,
      stack: error instanceof Error ? error.stack : undefined
    })
    return c.json(createErrorResponse('DB_ERROR', 'Failed to fetch result', { 
      message, 
      request_id: requestId 
    }), 500)
  }
})

// ============================================
// GET /session/:sessionId - ì„¸ì…˜ ì´ë ¥ ì¡°íšŒ
// ============================================
analyzerRoutes.get('/session/:sessionId', async (c) => {
  const db = c.env.DB
  const sessionId = c.req.param('sessionId')
  
  try {
    const events = await db.prepare(`
      SELECT * FROM raw_events 
      WHERE session_id = ?
      ORDER BY created_at ASC
    `).bind(sessionId).all()
    
    const requests = await db.prepare(`
      SELECT * FROM ai_analysis_requests
      WHERE session_id = ?
      ORDER BY requested_at ASC
    `).bind(sessionId).all()
    
    const constraints = await db.prepare(`
      SELECT * FROM confirmed_constraints
      WHERE session_id = ?
    `).bind(sessionId).all()
    
    // Phase 1A: facts ì¡°íšŒ ì¶”ê°€
    const facts = await db.prepare(`
      SELECT * FROM facts
      WHERE session_id = ?
      ORDER BY fact_level ASC, collected_at DESC
    `).bind(sessionId).all()
    
    return c.json({
      session_id: sessionId,
      events: events.results,
      requests: requests.results,
      confirmed_constraints: constraints.results,
      facts: facts.results,  // Phase 1A ì¶”ê°€
    })
    
  } catch (error) {
    const message = error instanceof Error ? error.message : 'Unknown error'
    logError('DB_ERROR', `Failed to fetch session: ${message}`, {
      session_id: sessionId,
      stack: error instanceof Error ? error.stack : undefined
    })
    return c.json(createErrorResponse('DB_ERROR', 'Failed to fetch session', { 
      message, 
      session_id: sessionId 
    }), 500)
  }
})

// ============================================
// GET /facts/:sessionId - ì„¸ì…˜ì˜ factsë§Œ ì¡°íšŒ
// ============================================
analyzerRoutes.get('/facts/:sessionId', async (c) => {
  const db = c.env.DB
  const sessionId = c.req.param('sessionId')
  
  try {
    const facts = await db.prepare(`
      SELECT * FROM facts
      WHERE session_id = ?
      ORDER BY fact_level ASC, collected_at DESC
    `).bind(sessionId).all()
    
    return c.json({
      session_id: sessionId,
      facts: facts.results,
      count: facts.results?.length || 0,
    })
    
  } catch (error) {
    logError('DB_ERROR', 'Failed to fetch facts', { session_id: sessionId })
    return c.json(createErrorResponse('DB_ERROR', 'Failed to fetch facts', { session_id: sessionId }), 500)
  }
})

// ============================================
// Analysis Logic (Phase 1A MVE)
// ============================================

interface Fact {
  fact_key: string
  value_json: string
  confidence: number
  fact_level: number
}

async function runAnalysis(
  db: D1Database,
  payload: AnalysisRequestPayloadV2,
  requestId: number,
  facts: Fact[],
  deepIntake?: NormalizedDeepIntake
): Promise<AnalysisResultJSON> {
  // 1. jobs + job_attributes JOINìœ¼ë¡œ ì‹¤ì œ DB ì§ì—…ë§Œ ì¡°íšŒ (íƒœê¹… ì—¬ë¶€ ë¬´ê´€)
  // 2026-01-26: tagger_version ì¡°ê±´ ì œê±° - ëª¨ë“  ì§ì—… ê²€ìƒ‰ ê°€ëŠ¥
  const jobAttrs = await db.prepare(`
    SELECT
      ja.job_id, ja.job_name,
      j.slug, j.image_url, j.api_data_json, j.merged_profile_json, j.ai_data_json,
      COALESCE(ja.wlb, 50) as wlb,
      COALESCE(ja.growth, 50) as growth,
      COALESCE(ja.stability, 50) as stability,
      COALESCE(ja.income, 50) as income,
      COALESCE(ja.teamwork, 50) as teamwork,
      COALESCE(ja.solo_deep, 50) as solo_deep,
      COALESCE(ja.analytical, 50) as analytical,
      COALESCE(ja.creative, 50) as creative,
      COALESCE(ja.execution, 50) as execution,
      COALESCE(ja.people_facing, 50) as people_facing,
      COALESCE(ja.work_hours, 'regular') as work_hours,
      COALESCE(ja.shift_work, 'rare') as shift_work,
      COALESCE(ja.travel, 'rare') as travel,
      COALESCE(ja.remote_possible, 'possible') as remote_possible,
      COALESCE(ja.degree_required, 'none') as degree_required,
      COALESCE(ja.license_required, 'none') as license_required
    FROM job_attributes ja
    INNER JOIN jobs j ON ja.job_id = j.id
    ORDER BY RANDOM()
    LIMIT 500
  `).all<{
    job_id: string
    job_name: string
    slug: string
    image_url: string | null
    api_data_json: string | null
    merged_profile_json: string | null
    ai_data_json: string | null
    wlb: number
    growth: number
    stability: number
    income: number
    teamwork: number
    solo_deep: number
    analytical: number
    creative: number
    execution: number
    people_facing: number
    work_hours: string
    shift_work: string
    travel: string
    remote_possible: string
    degree_required: string
    license_required: string
  }>()

  // V2ì—ì„œë„ mini_module_result ì¶”ì¶œ (ê°œì¸í™” ìŠ¤ì½”ì–´ë§)
  const v2MiniModule = 'mini_module_result' in payload ? (payload as any).mini_module_result : undefined

  // DBì— ë°ì´í„°ê°€ ì—†ìœ¼ë©´ ìƒ˜í”Œ ì‚¬ìš© (ìƒ˜í”Œì€ DBì— ì—†ëŠ” ì§ì—…ì´ë¯€ë¡œ ì¶”ì²œì—ì„œ ì œì™¸)
  const sampleJobs = (jobAttrs.results && jobAttrs.results.length > 0)
    ? jobAttrs.results.map(j => {
        const personalized = calculatePersonalizedBaseScores(j, v2MiniModule)
        return {
          job_id: j.job_id,
          job_name: j.job_name,
          slug: j.slug,
          image_url: j.image_url,
          job_description: extractJobDescription(j.api_data_json, j.merged_profile_json, j.job_name, j.ai_data_json),
          base_like: personalized.like,
          base_can: personalized.can,
          base_risk: 10,  // ê¸°ë³¸ risk
          attributes: {
            wlb: j.wlb,
            growth: j.growth,
            stability: j.stability,
            income: j.income,
            remote: j.remote_possible === 'full' ? 100 : j.remote_possible === 'partial' ? 50 : 0,
            solo_work: j.solo_deep,
            people_facing: j.people_facing,
            analytical: j.analytical,
            creative: j.creative,
          },
        }
      })
    : [] // DB ì—°ê²° ì‹¤íŒ¨ì‹œ ë¹ˆ ë°°ì—´ (ìƒ˜í”Œ ë°ì´í„° ì‚¬ìš© ì•ˆí•¨)
  
  // DB ë°ì´í„°ê°€ ì—†ìœ¼ë©´ ì—ëŸ¬ ë°˜í™˜
  if (sampleJobs.length === 0) {
    throw new Error('ì§ì—… ë°ì´í„°ë¥¼ ë¶ˆëŸ¬ì˜¬ ìˆ˜ ì—†ìŠµë‹ˆë‹¤. job_attributes í…Œì´ë¸”ì„ í™•ì¸í•˜ì„¸ìš”.')
  }
  
  // 2. Fact boosts ê³„ì‚°
  const factBoosts = calculateFactBoosts(facts)
  
  // 3. ê° ì§ì—…ì— ì ìˆ˜ ì ìš© (slug, image_url í¬í•¨)
  const scoredJobs: ScoredJob[] = sampleJobs.map(job => {
    const baseScores: JobScores = {
      like: job.base_like,
      can: job.base_can,
      risk_penalty: job.base_risk,
    }

    const adjusted = applyFactBoostsToJob(baseScores, job.attributes, factBoosts)

    // P0: Can-Like ë°¸ëŸ°ìŠ¤ ìº¡ ì ìš©
    const balanced = applyBalanceCap(adjusted.like, adjusted.can)
    const fit = Math.round(0.55 * balanced.like + 0.45 * balanced.can - adjusted.risk_penalty)

    // ì„ì‹œ scoredJob ê°ì²´ ìƒì„± (explanation ìƒì„±ìš©)
    const tempScoredJob: ScoredJob = {
      job_id: job.job_id,
      job_name: job.job_name,
      slug: job.slug,
      image_url: job.image_url || undefined,
      job_description: job.job_description,
      scores: {
        fit: Math.max(0, fit),
        like: balanced.like,
        can: balanced.can,
        risk_penalty: adjusted.risk_penalty,
      },
      attributes: job.attributes,
    }

    // ê·¼ê±° ìƒì„± (V2: facts ê¸°ë°˜)
    const explanation = generateJobExplanation(tempScoredJob, facts, factBoosts.applied_rules)
    const rationale = `[ì¢‹ì•„í•  ì´ìœ ] ${explanation.like_reason} [ì˜í•  ì´ìœ ] ${explanation.can_reason}`

    return {
      ...tempScoredJob,
      rationale,
      likeReason: explanation.like_reason,
      canReason: explanation.can_reason,
      riskWarning: explanation.risk_warning,
    }
  })
  
  // 4. ì •ë ¬ (Fit ê¸°ì¤€)
  scoredJobs.sort((a, b) => b.scores.fit - a.scores.fit)
  
  // í‰ê°€ ì§ì—… 100ê°œë¡œ í™•ì¥ (ê¸°ì¡´ 10ê°œ -> 100ê°œ)
  const top10 = scoredJobs.slice(0, 100)
  
  // Phase 4: Diversity Guard (ë‚´ë¶€ì—ì„œ Research Bias Capë„ ì ìš©)
  const rawTop5 = top10.slice(0, 5)
  const diversityResult = applyDiversityGuard(rawTop5, scoredJobs)
  const top3 = diversityResult.adjusted

  // 5. Follow-up ì§ˆë¬¸ ìƒì„± (Deep Intake ì—¬ë¶€ ë°˜ì˜)
  const existingFactKeys = facts.map(f => ({ fact_key: f.fact_key }))
  const followupQuestions = generateFollowupQuestions({
    candidates: scoredJobs,
    topK: top10,
    existingFacts: existingFactKeys,
    hasDeepIntake: !!deepIntake,  // Phase 1C: Deep Intake ì—¬ë¶€ ì „ë‹¬
  }, 3)
  
  // 6. Phase 1C: User Insight ìƒì„±
  const userInsight = generateUserInsight(facts, deepIntake, factBoosts.applied_rules)
  
  // 7. Premium Report ìƒì„± (V3)
  const premiumReportInput: PremiumReportInput = {
    session_id: `session-${requestId}`,
    facts: facts.map(f => ({ fact_key: f.fact_key, value_json: f.value_json })),
    recommendations: top10.map(j => ({
      job_id: j.job_id,
      job_name: j.job_name,
      slug: j.slug,
      image_url: j.image_url,
      scores: j.scores,
      attributes: j.attributes as Record<string, number>,
    })),
    userInsight: userInsight,
  }
  const premiumReport = fixParticlesDeep(generatePremiumReport(premiumReportInput))

  // 8. ê²°ê³¼ êµ¬ì„±
  const result: AnalysisResultJSON = {
    engine_state: deepIntake ? 'phase1a_mve' : (facts.length > 0 ? 'phase1a_mve' : 'phase1a_initial'),
    engine_version: 'v3',  // V3 í”„ë¦¬ë¯¸ì—„ ë¦¬í¬íŠ¸ í™œì„±í™”
    mini_module_result: miniModuleForFilter || null,

    versions: {
      recipe: VERSIONS.recipe,
      tagger: VERSIONS.tagger,
      scoring: VERSIONS.scoring,
    },

    input_summary: {
      profile_revision_id: `rev-${requestId}-${Date.now()}`,
      key_interests: payload.profile.interest.keywords.slice(0, 3),
      key_skills: payload.profile.skill.map(s => s.name).slice(0, 3),
      non_negotiables: Object.entries(payload.profile.constraints || {})
        .filter(([_, v]) => v === true)
        .map(([k]) => k),
      preferences: [],
      facts_applied: facts.length,
      applied_rules: factBoosts.applied_rules,
      // Phase 1C: Deep Intake ì •ë³´ ì¶”ê°€
      deep_intake_provided: !!deepIntake,
      insight_tags: userInsight?.key_traits.map(t => t.trait) || [],
    },
    
    fit_top3: top3.map(j => {
      // Evidence ìƒì„± (facts + job attributes ê¸°ë°˜)
      // âš ï¸ EvidenceëŠ” "ì„¤ëª…ìš©"ì´ë©° ì ìˆ˜ ê³„ì‚°ì— ì˜í–¥ ì—†ìŒ
      const evidenceFacts: EvidenceFact[] = facts.map(f => ({
        fact_key: f.fact_key,
        value_json: f.value_json,
      }))
      let evidenceLinks = buildEvidenceLinks(evidenceFacts, j)
      
      // Evidenceê°€ ë¶€ì¡±í•˜ë©´ ê¸°ë³¸ Evidence ì¶”ê°€
      if (evidenceLinks.length < 3) {
        const defaultEvidence = generateDefaultEvidence(j)
        evidenceLinks = [...evidenceLinks, ...defaultEvidence].slice(0, 10)
      }
      
      // Dislike ê²½ê³  ìƒì„±
      const dislikeWarnings = generateDislikeWarnings(facts, j)
      
      return {
        job_id: j.job_id,
        job_name: j.job_name,
        slug: j.slug,
        image_url: j.image_url,
        job_description: j.job_description || null,
        rationale: j.rationale || null,
        like_reason: j.likeReason || null,
        can_reason: j.canReason || null,
        fit_score: j.scores.fit,
        like_score: j.scores.like,
        can_score: j.scores.can,
        risk_details: [],
        evidence_links: evidenceLinks,
        dislike_warnings: dislikeWarnings, // íšŒí”¼ ìš”ì†Œ ê²½ê³ 
      }
    }),

    // like_top10: like_score ê¸°ì¤€ìœ¼ë¡œ ë³„ë„ ì •ë ¬ (Fit >= 25 í•„í„°)
    like_top10: [...scoredJobs]
      .filter(j => j.scores.fit >= 25)
      .sort((a, b) => b.scores.like - a.scores.like)
      .slice(0, 10)
      .map(j => ({
        job_id: j.job_id,
        job_name: j.job_name,
        slug: j.slug,
        image_url: j.image_url,
        job_description: j.job_description || null,
        fit_score: j.scores.fit,
        like_score: j.scores.like,
        can_score: j.scores.can,
        rationale: j.rationale || null,
        like_reason: j.likeReason || null,
        can_reason: j.canReason || null,
      })),

    // can_top10: can_score ê¸°ì¤€ìœ¼ë¡œ ë³„ë„ ì •ë ¬ (Fit >= 25 í•„í„°)
    can_top10: [...scoredJobs]
      .filter(j => j.scores.fit >= 25)
      .sort((a, b) => b.scores.can - a.scores.can)
      .slice(0, 10)
      .map(j => ({
        job_id: j.job_id,
        job_name: j.job_name,
        slug: j.slug,
        image_url: j.image_url,
        job_description: j.job_description || null,
        fit_score: j.scores.fit,
        like_score: j.scores.like,
        can_score: j.scores.can,
        rationale: j.rationale || null,
        like_reason: j.likeReason || null,
        can_reason: j.canReason || null,
      })),

    caution_jobs: scoredJobs
      .filter(j => j.scores.risk_penalty > 20)
      .slice(0, 5)
      .map(j => ({
        job_id: j.job_id,
        job_name: j.job_name,
        slug: j.slug,
        image_url: j.image_url,
        risk_penalty: j.scores.risk_penalty,
      })),
    
    // Phase 1A: followup_questions ì¶”ê°€!
    followup_questions: followupQuestions,
    
    ux_flags: {
      has_caution_in_top3: top3.some(j => j.scores.risk_penalty > 20),
      has_unknown_in_top3: false,
      needs_followup_question: followupQuestions.length > 0,
      counts_before_filter: {
        safe_known: scoredJobs.filter(j => j.scores.risk_penalty <= 10).length,
        safe_unknown: scoredJobs.filter(j => j.scores.risk_penalty > 10 && j.scores.risk_penalty <= 20).length,
        caution: scoredJobs.filter(j => j.scores.risk_penalty > 20).length,
        total: scoredJobs.length,
      },
      counts_after_filter: {
        safe_known: scoredJobs.filter(j => j.scores.risk_penalty <= 10).length,
        safe_unknown: scoredJobs.filter(j => j.scores.risk_penalty > 10 && j.scores.risk_penalty <= 20).length,
        caution: scoredJobs.filter(j => j.scores.risk_penalty > 20).length,
        total: scoredJobs.length,
      },
    },
    
    llm_explanation: generateExplanation(facts, factBoosts.applied_rules, top3[0]?.job_name),
    generated_at: new Date().toISOString(),
    total_candidates: scoredJobs.length,
    
    // Phase 1C: User Insight ì¶”ê°€
    user_insight: userInsight,
    
    // Phase 4: Diversity Guard ì •ë³´
    phase4_applied: true,
    diversity_guard_active: diversityResult.diversityApplied,
    diversity_changes: diversityResult.changes,
    
    // V3: Premium Report
    premium_report: premiumReport,
  }
  
  return result
}

// ============================================
// í•œêµ­ì–´ ì¡°ì‚¬ í—¬í¼ (ë°›ì¹¨ ìœ ë¬´ ìë™ íŒë³„)
// ============================================
function hasBatchim(word: string): boolean {
  if (!word || word.length === 0) return false
  const last = word.charCodeAt(word.length - 1)
  if (last >= 0xAC00 && last <= 0xD7A3) return (last - 0xAC00) % 28 !== 0
  // ìˆ«ì: 0(ì˜),1(ì¼),3(ì‚¼),6(ìœ¡),7(ì¹ ),8(íŒ”) = ë°›ì¹¨ ìˆìŒ
  if (last >= 0x30 && last <= 0x39) return [0,1,3,6,7,8].includes(last - 0x30)
  return false
}
/** ì€/ëŠ” */
function ì€ëŠ”(w: string): string { return hasBatchim(w) ? 'ì€' : 'ëŠ”' }
/** ì´/ê°€ */
function ì´ê°€(w: string): string { return hasBatchim(w) ? 'ì´' : 'ê°€' }
/** ì„/ë¥¼ */
function ì„ë¥¼(w: string): string { return hasBatchim(w) ? 'ì„' : 'ë¥¼' }

// ============================================
// Explainability V3: ì§ì—…ë³„ ì¶”ì²œ ê·¼ê±° ìƒì„±
// ============================================
interface JobExplanation {
  like_reason: string    // Like ì ìˆ˜ ê·¼ê±°
  can_reason: string     // Can ì ìˆ˜ ê·¼ê±°
  risk_warning: string   // Risk ê²½ê³  (ìˆìœ¼ë©´)
}

function generateJobExplanation(
  job: ScoredJob,
  facts: Fact[],
  appliedRules: string[],
  mm?: any  // MiniModuleResult (optional)
): JobExplanation {
  const attrs = (job.attributes || {}) as Record<string, number>
  const jobName = job.job_name || 'ì´ ì§ì—…'

  // ============================================
  // Like ê·¼ê±°: ì‚¬ìš©ì í¥ë¯¸/ê°€ì¹˜ â†” ì§ì—… ì†ì„± ë§¤ì¹­
  // ============================================
  const interestLabels: Record<string, string> = {
    problem_solving: 'ë¬¸ì œí•´ê²°', data_numbers: 'ë°ì´í„°/ìˆ«ì', creating: 'ì°½ì‘/ì°½ì˜',
    influencing: 'ì˜í–¥ë ¥/ì„¤ë“', helping_teaching: 'ë„ì›€/êµìœ¡', organizing: 'ì¡°ì§/ê´€ë¦¬',
    tech: 'ê¸°ìˆ /IT', routine: 'ì •í•´ì§„ ì—…ë¬´',
  }
  const valueLabels: Record<string, string> = {
    growth: 'ì„±ì¥', stability: 'ì•ˆì •', autonomy: 'ììœ¨ì„±', wlb: 'ì›Œë¼ë°¸',
    income: 'ìˆ˜ì…', recognition: 'ì¸ì •', meaning: 'ì˜ë¯¸', expertise: 'ì „ë¬¸ì„±',
    creativity: 'ì°½ì˜ì„±',
  }
  // ì§ì—… ì†ì„±ê³¼ ê°€ì¹˜/í¥ë¯¸ ë§¤í•‘
  const valueAttrMap: Record<string, { attr: string, threshold: number }> = {
    growth: { attr: 'growth', threshold: 60 },
    stability: { attr: 'stability', threshold: 60 },
    wlb: { attr: 'wlb', threshold: 60 },
    income: { attr: 'income', threshold: 60 },
  }
  const interestAttrMap: Record<string, { attr: string, threshold: number }> = {
    data_numbers: { attr: 'analytical', threshold: 60 },
    creating: { attr: 'creative', threshold: 60 },
    tech: { attr: 'analytical', threshold: 55 },
    problem_solving: { attr: 'analytical', threshold: 55 },
    helping_teaching: { attr: 'people_facing', threshold: 60 },
    influencing: { attr: 'people_facing', threshold: 55 },
  }

  let likeReason = ''
  const likeReasons: string[] = []

  if (mm) {
    // 1) í¥ë¯¸ ë§¤ì¹­ (ì§ì—…ëª… + ì†ì„±ê°’ í¬í•¨í•˜ì—¬ ì°¨ë³„í™”)
    const interests = mm.interest_top || mm.interests || []
    for (const token of interests) {
      const mapping = interestAttrMap[token]
      const attrVal = mapping ? (attrs[mapping.attr] || 0) : 0
      if (mapping && attrVal >= mapping.threshold) {
        const label = interestLabels[token] || token
        if (attrVal >= 80) {
          likeReasons.push(`${jobName}${ì€ëŠ”(jobName)} ${label} ì—­ëŸ‰ì´ í•µì‹¬ì¸ ì§ì—…ìœ¼ë¡œ, ë‹¹ì‹ ì˜ ê´€ì‹¬ì‚¬ì™€ ë§¤ìš° ì˜ ë§ìŠµë‹ˆë‹¤`)
        } else {
          likeReasons.push(`${jobName}ì—ì„œ ${label} ê´€ë ¨ ì—…ë¬´ë¥¼ ìˆ˜í–‰í•˜ë©° ê´€ì‹¬ì‚¬ë¥¼ ì‚´ë¦´ ìˆ˜ ìˆìŠµë‹ˆë‹¤`)
        }
        break
      }
    }
    // 2) ê°€ì¹˜ ë§¤ì¹­ (ì§ì—…ëª… + êµ¬ì²´ì  ì†ì„± í¬í•¨)
    const values = mm.value_top || mm.values || []
    for (const token of values) {
      const mapping = valueAttrMap[token]
      const attrVal = mapping ? (attrs[mapping.attr] || 0) : 0
      if (mapping && attrVal >= mapping.threshold) {
        const label = valueLabels[token] || token
        if (attrVal >= 80) {
          likeReasons.push(`${label}${ì„ë¥¼(label)} ì¤‘ì‹œí•˜ëŠ” ë‹¹ì‹ ì—ê²Œ ${jobName}${ì€ëŠ”(jobName)} ë†’ì€ ${label} ì§€ìˆ˜(${attrVal}ì )ë¡œ í° ë§Œì¡±ê°ì„ ì¤„ ìˆ˜ ìˆìŠµë‹ˆë‹¤`)
        } else {
          likeReasons.push(`${jobName}${ì€ëŠ”(jobName)} ${label} ì¸¡ë©´ì—ì„œ ì–‘í˜¸í•œ í™˜ê²½(${attrVal}ì )ì„ ì œê³µí•©ë‹ˆë‹¤`)
        }
        break
      }
    }
    // 3) í¥ë¯¸ ë¼ë²¨ë§Œì´ë¼ë„
    if (likeReasons.length === 0 && interests.length > 0) {
      const labels = interests.slice(0, 2).map((t: string) => interestLabels[t] || t).join(', ')
      likeReasons.push(`${jobName}${ì€ëŠ”(jobName)} ${labels} ë¶„ì•¼ì— ëŒ€í•œ ê´€ì‹¬ê³¼ ì—°ê²°ë˜ëŠ” ì—…ë¬´ì…ë‹ˆë‹¤`)
    }
  }

  // facts ê¸°ë°˜ ë³´ì¡°
  if (likeReasons.length === 0) {
    const interestFacts = facts.filter(f => f.fact_key.includes('interest') || f.fact_key.includes('priority'))
    if (interestFacts.length > 0) {
      try {
        const parsed = JSON.parse(interestFacts[0].value_json)
        const val = Array.isArray(parsed.value) ? parsed.value[0] : parsed.value
        if (val && interestLabels[val]) {
          likeReasons.push(`${interestLabels[val]} ê´€ì‹¬ì‚¬ì™€ ë§¤ì¹­ë˜ëŠ” ì—…ë¬´ í™˜ê²½ì…ë‹ˆë‹¤`)
        }
      } catch { /* ignore */ }
    }
  }

  likeReason = likeReasons.length > 0
    ? likeReasons.join('. ')
    : `${jobName}${ì€ëŠ”(jobName)} ë‹¹ì‹ ì˜ ê´€ì‹¬ ë¶„ì•¼ì™€ ì—°ê´€ëœ ì§ì—…ì…ë‹ˆë‹¤`

  // ============================================
  // Can ê·¼ê±°: ì‚¬ìš©ì ê°•ì /ì›Œí¬ìŠ¤íƒ€ì¼ â†” ì§ì—… ì†ì„± ë§¤ì¹­
  // ============================================
  const strengthLabels: Record<string, string> = {
    analytical: 'ë¶„ì„ë ¥', fast_learning: 'ë¹ ë¥¸ í•™ìŠµë ¥', creative: 'ì°½ì˜ë ¥',
    communication: 'ì†Œí†µ ëŠ¥ë ¥', persistence: 'ëˆê¸°', leadership: 'ë¦¬ë”ì‹­',
    structured_execution: 'ì²´ê³„ì  ì‹¤í–‰ë ¥',
  }
  const strengthAttrMap: Record<string, { attr: string, threshold: number }> = {
    analytical: { attr: 'analytical', threshold: 55 },
    creative: { attr: 'creative', threshold: 55 },
    communication: { attr: 'people_facing', threshold: 55 },
    leadership: { attr: 'people_facing', threshold: 60 },
    structured_execution: { attr: 'execution', threshold: 55 },
  }
  const workstyleLabels: Record<string, string> = {
    solo: 'í˜¼ì ì§‘ì¤‘í•˜ëŠ”', solo_deep: 'ê¹Šì´ ëª°ì…í•˜ëŠ”', team: 'íŒ€ìœ¼ë¡œ í˜‘ë ¥í•˜ëŠ”',
    team_harmony: 'ì¡°í™”ë¡­ê²Œ ì¼í•˜ëŠ”', structured: 'ì²´ê³„ì ì¸', flexible: 'ììœ ë¡œìš´',
  }

  let canReason = ''
  const canReasons: string[] = []

  if (mm) {
    // 1) ê°•ì  ë§¤ì¹­ (ì§ì—…ëª… + ì†ì„±ê°’ í¬í•¨í•˜ì—¬ ì°¨ë³„í™”)
    const strengths = mm.strength_top || mm.strengths || []
    for (const token of strengths) {
      const mapping = strengthAttrMap[token]
      const attrVal = mapping ? (attrs[mapping.attr] || 0) : 0
      if (mapping && attrVal >= mapping.threshold) {
        const label = strengthLabels[token] || token
        if (attrVal >= 75) {
          canReasons.push(`${jobName}${ì€ëŠ”(jobName)} ${label}${ì´ê°€(label)} í•µì‹¬ ì—­ëŸ‰(${attrVal}ì )ì¸ ì§ì—…ìœ¼ë¡œ, ë‹¹ì‹ ì˜ ê°•ì ì„ ì§ì ‘ í™œìš©í•  ìˆ˜ ìˆìŠµë‹ˆë‹¤`)
        } else {
          canReasons.push(`${jobName}ì—ì„œ ë‹¹ì‹ ì˜ ${label} ê°•ì ì´ ë„ì›€ì´ ë  ìˆ˜ ìˆìŠµë‹ˆë‹¤`)
        }
        break
      }
    }
    // 2) ì›Œí¬ìŠ¤íƒ€ì¼ ë§¤ì¹­
    const workstyles = mm.workstyle_top || []
    if (workstyles.length > 0) {
      const wsLabel = workstyleLabels[workstyles[0]] || ''
      const soloAttr = attrs['solo_work'] || attrs['solo_deep'] || 0
      const peopleAttr = attrs['people_facing'] || 0
      if (workstyles[0]?.includes('solo') && soloAttr >= 60) {
        canReasons.push(`${jobName}${ì€ëŠ”(jobName)} ${wsLabel} ì—…ë¬´ ìŠ¤íƒ€ì¼ì— ë§ëŠ” ë…ë¦½ì ì¸ í™˜ê²½ì…ë‹ˆë‹¤`)
      } else if (workstyles[0]?.includes('team') && peopleAttr >= 60) {
        canReasons.push(`${jobName}${ì€ëŠ”(jobName)} ${wsLabel} ì—…ë¬´ ìŠ¤íƒ€ì¼ì— ë§ëŠ” í˜‘ì—… í™˜ê²½ì…ë‹ˆë‹¤`)
      }
    }
    // 3) ê°•ì  ë¼ë²¨ë§Œì´ë¼ë„
    if (canReasons.length === 0) {
      const strengths2 = mm.strength_top || mm.strengths || []
      if (strengths2.length > 0) {
        const sLabels = strengths2.slice(0, 2).map((t: string) => strengthLabels[t] || t)
        const joinedLabels = sLabels.length > 1 ? `${sLabels[0]}ê³¼ ${sLabels[1]}` : sLabels[0]
        canReasons.push(`${jobName}ì—ì„œ ${joinedLabels}${ì„ë¥¼(joinedLabels)} ë°œíœ˜í•  ìˆ˜ ìˆìŠµë‹ˆë‹¤`)
      }
    }
  }

  // workstyle fallback from facts
  if (canReasons.length === 0 && appliedRules.includes('profile.workstyle.social')) {
    const styleFact = facts.find(f => f.fact_key === 'profile.workstyle.social')
    if (styleFact) {
      try {
        const sv = JSON.parse(styleFact.value_json)
        const styleMap: Record<string, string> = {
          solo: 'ë…ë¦½ì  ì—…ë¬´ í™˜ê²½ì— ì í•©í•©ë‹ˆë‹¤', team: 'íŒ€ í˜‘ì—… í™˜ê²½ì— ì í•©í•©ë‹ˆë‹¤',
          balanced: 'ê· í˜• ì¡íŒ ì—…ë¬´ í™˜ê²½ì— ì í•©í•©ë‹ˆë‹¤'
        }
        canReasons.push(styleMap[sv.value] || 'ì„ í˜¸í•˜ëŠ” ì—…ë¬´ ìŠ¤íƒ€ì¼ì— ë¶€í•©í•©ë‹ˆë‹¤')
      } catch { /* ignore */ }
    }
  }

  canReason = canReasons.length > 0
    ? canReasons.join('. ')
    : `${jobName}${ì€ëŠ”(jobName)} ë‹¹ì‹ ì˜ ì—­ëŸ‰ìœ¼ë¡œ ì¶©ë¶„íˆ ë„ì „í•  ìˆ˜ ìˆëŠ” ë¶„ì•¼ì…ë‹ˆë‹¤`

  // ============================================
  // Risk ê²½ê³ 
  // ============================================
  let riskWarning = ''
  const confirmedConstraints = facts.filter(f => f.fact_key.startsWith('confirmed_constraint.'))
  if (confirmedConstraints.length > 0) {
    riskWarning = 'ì œì•½ ì¡°ê±´ ì¶©ì¡±'
  }
  const riskPenalty = job.scores?.risk_penalty || job.risk_penalty || 0
  if (riskPenalty > 15) {
    riskWarning = 'ì¼ë¶€ ì£¼ì˜ í•„ìš” (ìƒì„¸ ì •ë³´ í™•ì¸ ê¶Œì¥)'
  } else if (riskPenalty > 10) {
    riskWarning = 'ê²½ë¯¸í•œ ë¶ˆí™•ì‹¤ì„± ìˆìŒ'
  }
  if (!riskWarning) {
    riskWarning = 'íŠ¹ë³„í•œ ìœ„í—˜ ìš”ì†Œ ì—†ìŒ'
  }

  return {
    like_reason: likeReason,
    can_reason: canReason,
    risk_warning: riskWarning,
  }
}

// ============================================
// Dislike ê²½ê³  ìƒì„± (íšŒí”¼ ìš”ì†Œ í¬í•¨ ì‹œ)
// ============================================
interface DislikeWarning {
  type: string
  label: string
  severity: 'high' | 'medium' | 'low'
}

function generateDislikeWarnings(facts: Fact[], job: ScoredJob): DislikeWarning[] {
  const warnings: DislikeWarning[] = []
  
  // hard_dislike facts ìˆ˜ì§‘
  const hardDislikeFacts = facts.filter(f => f.fact_key === 'profile.hard_dislike')
  const mildDislikeFacts = facts.filter(f => f.fact_key === 'profile.dislike')
  
  // hard_dislike â†’ ì§ì—… ì†ì„± ë§¤í•‘
  const DISLIKE_JOB_ATTR_MAP: Record<string, { attrs: string[]; label: string }> = {
    'sales': { attrs: ['people_facing'], label: 'ì˜ì—…/ì„¤ë“ ì—…ë¬´' },
    'overtime': { attrs: ['wlb', 'work_hours'], label: 'ì•¼ê·¼/ê¸´ ê·¼ë¬´' },
    'public_speaking': { attrs: ['people_facing'], label: 'ë°œí‘œ/ëŒ€ë©´ ì—…ë¬´' },
    'meetings': { attrs: ['teamwork'], label: 'íšŒì˜ ë§ìŒ' },
    'travel': { attrs: ['travel', 'remote_possible'], label: 'ì¶œì¥/ì´ë™' },
    'routine': { attrs: ['creative'], label: 'ë‹¨ìˆœ ë°˜ë³µ ì—…ë¬´' },
    'conflict': { attrs: ['people_facing'], label: 'ê°ˆë“±/ëŒ€ë¦½ ìƒí™©' },
    'physical': { attrs: ['execution'], label: 'ìœ¡ì²´ì  ì—…ë¬´' },
    'shift_work': { attrs: ['shift_work'], label: 'êµëŒ€ ê·¼ë¬´' },
    'woodwork': { attrs: ['execution'], label: 'ëª©ê³µ ê´€ë ¨' },
    'construction': { attrs: ['execution'], label: 'ê±´ì„¤/í˜„ì¥ ì—…ë¬´' },
    'call_center': { attrs: ['people_facing'], label: 'ì½œì„¼í„°/ì „í™” ì—…ë¬´' },
  }
  
  // hard_dislike ë§¤ì¹­ í™•ì¸
  for (const fact of hardDislikeFacts) {
    try {
      const data = JSON.parse(fact.value_json)
      const dislikeType = data.type
      const mapping = DISLIKE_JOB_ATTR_MAP[dislikeType]
      
      if (mapping) {
        // ì§ì—…ì˜ í•´ë‹¹ ì†ì„±ì´ ë†’ìœ¼ë©´ ê²½ê³ 
        const jobAttrs = job.attributes || {}
        for (const attr of mapping.attrs) {
          const attrValue = jobAttrs[attr]
          if (attrValue !== undefined && Number(attrValue) > 50) {
            warnings.push({
              type: dislikeType,
              label: `âš ï¸ ì£¼ì˜: ${mapping.label} ìš”ì†Œ í¬í•¨`,
              severity: 'high',
            })
            break
          }
        }
      }
    } catch { /* ignore */ }
  }
  
  // mild dislike ë§¤ì¹­ (ê²½ë¯¸í•œ ê²½ê³ )
  for (const fact of mildDislikeFacts) {
    try {
      const data = JSON.parse(fact.value_json)
      const dislikeType = data.type
      const mapping = DISLIKE_JOB_ATTR_MAP[dislikeType]
      
      if (mapping && !warnings.some(w => w.type === dislikeType)) {
        const jobAttrs = job.attributes || {}
        for (const attr of mapping.attrs) {
          const attrValue = jobAttrs[attr]
          if (attrValue !== undefined && Number(attrValue) > 70) {
            warnings.push({
              type: dislikeType,
              label: `â„¹ï¸ ì°¸ê³ : ${mapping.label} ìš”ì†Œ ìˆìŒ`,
              severity: 'low',
            })
            break
          }
        }
      }
    } catch { /* ignore */ }
  }
  
  return warnings
}

// ============================================
// Phase 1C: User Insight ìƒì„±
// ============================================
function generateUserInsight(
  facts: Fact[],
  deepIntake?: NormalizedDeepIntake,
  appliedRules?: string[]
): UserInsight | undefined {
  if (!deepIntake && facts.length === 0) {
    return undefined
  }
  
  const keyTraits: UserInsight['key_traits'] = []
  const appliedFacts: UserInsight['applied_facts'] = []
  
  // MBTI ê¸°ë°˜ ì¸ì‚¬ì´íŠ¸
  if (deepIntake?.mbti && deepIntake.mbti_traits) {
    const workStyles = deepIntake.mbti_traits.workStyles
    
    if (workStyles.includes('solo_deep')) {
      keyTraits.push({
        trait: 'í˜¼ì ê¹Šê²Œ íŒŒê¸°ë¥¼ ì„ í˜¸',
        evidence: `MBTI ${deepIntake.mbti}ì˜ íŠ¹ì„±`,
        score_impact: 'ë¶„ì„/ì—°êµ¬ ì¤‘ì‹¬ ì§ì—…ì— +5~10 ë¶€ìŠ¤íŠ¸',
      })
    }
    if (workStyles.includes('team_collab')) {
      keyTraits.push({
        trait: 'íŒ€ í˜‘ì—…ì„ ì„ í˜¸',
        evidence: `MBTI ${deepIntake.mbti}ì˜ íŠ¹ì„±`,
        score_impact: 'í˜‘ì—… ì¤‘ì‹¬ ì§ì—…ì— +5~10 ë¶€ìŠ¤íŠ¸',
      })
    }
    if (workStyles.includes('creative')) {
      keyTraits.push({
        trait: 'ì°½ì˜ì ì¸ ì—…ë¬´ ì„ í˜¸',
        evidence: `MBTI ${deepIntake.mbti}ì˜ íŠ¹ì„±`,
        score_impact: 'ì°½ì˜ì„± ìš”êµ¬ ì§ì—…ì— +5 ë¶€ìŠ¤íŠ¸',
      })
    }
  }
  
  // best_moment ê¸°ë°˜ ì¸ì‚¬ì´íŠ¸
  if (deepIntake?.best_moment) {
    const tags = deepIntake.best_moment.tags
    if (tags.includes('solo_deep')) {
      keyTraits.push({
        trait: 'ëª°ì…í•˜ë©° ì¼í•  ë•Œ ì—ë„ˆì§€ë¥¼ ì–»ìŒ',
        evidence: `"${deepIntake.best_moment.raw.slice(0, 30)}..."`,
        score_impact: 'ë¶„ì„/ì—°êµ¬ ì§ì—…ì— +15 ë¶€ìŠ¤íŠ¸',
      })
    }
    if (tags.includes('team_collab') || tags.includes('people_facing')) {
      keyTraits.push({
        trait: 'ì‚¬ëŒë“¤ê³¼ í•¨ê»˜í•  ë•Œ ì—ë„ˆì§€ë¥¼ ì–»ìŒ',
        evidence: `"${deepIntake.best_moment.raw.slice(0, 30)}..."`,
        score_impact: 'í˜‘ì—…/ëŒ€ì¸ ì§ì—…ì— +15 ë¶€ìŠ¤íŠ¸',
      })
    }
    if (tags.includes('creative')) {
      keyTraits.push({
        trait: 'ì°½ì‘/ê¸°íší•  ë•Œ ë³´ëŒì„ ëŠë‚Œ',
        evidence: `"${deepIntake.best_moment.raw.slice(0, 30)}..."`,
        score_impact: 'ì°½ì˜ì„± ìš”êµ¬ ì§ì—…ì— +15 ë¶€ìŠ¤íŠ¸',
      })
    }
  }
  
  // worst_moment ê¸°ë°˜ ì¸ì‚¬ì´íŠ¸
  if (deepIntake?.worst_moment) {
    const stressTrigger = deepIntake.worst_moment.stress_trigger
    if (stressTrigger === 'people') {
      keyTraits.push({
        trait: 'ëŒ€ì¸ ê°ˆë“±ì— ìŠ¤íŠ¸ë ˆìŠ¤ë¥¼ ë°›ìŒ',
        evidence: `"${deepIntake.worst_moment.raw.slice(0, 30)}..."`,
        score_impact: 'ë…ë¦½ì  ì—…ë¬´ ì§ì—…ì— +15 ë¶€ìŠ¤íŠ¸',
      })
    }
    if (stressTrigger === 'deadline') {
      keyTraits.push({
        trait: 'ë§ˆê° ì••ë°•ì— ìŠ¤íŠ¸ë ˆìŠ¤ë¥¼ ë°›ìŒ',
        evidence: `"${deepIntake.worst_moment.raw.slice(0, 30)}..."`,
        score_impact: 'ì›Œë¼ë°¸ ì¢‹ì€ ì§ì—…ì— +10 ë¶€ìŠ¤íŠ¸',
      })
    }
    if (stressTrigger === 'meeting') {
      keyTraits.push({
        trait: 'íšŒì˜ê°€ ë§ì€ í™˜ê²½ì„ í”¼í•˜ê³  ì‹¶ìŒ',
        evidence: `"${deepIntake.worst_moment.raw.slice(0, 30)}..."`,
        score_impact: 'ë…ë¦½ì /ì›ê²© ì§ì—…ì— +10 ë¶€ìŠ¤íŠ¸',
      })
    }
  }
  
  // priority.top1 ê¸°ë°˜ ì¸ì‚¬ì´íŠ¸
  if (deepIntake?.priority_top1) {
    const priorityMap: Record<string, string> = {
      'growth': 'ì„±ì¥ ê°€ëŠ¥ì„±',
      'stability': 'ì•ˆì •ì„±',
      'wlb': 'ì›Œë¼ë°¸',
      'income': 'ë†’ì€ ìˆ˜ì…',
    }
    keyTraits.push({
      trait: (() => { const p = priorityMap[deepIntake.priority_top1] || deepIntake.priority_top1; return `${p}${ì„ë¥¼(p)} ìµœìš°ì„ ì‹œ` })(),
      evidence: 'ì§ì ‘ ì„ íƒ',
      score_impact: `í•´ë‹¹ ì†ì„±ì— +20 ë¶€ìŠ¤íŠ¸`,
    })
  }
  
  // facts ê¸°ë°˜ ì¸ì‚¬ì´íŠ¸
  for (const fact of facts) {
    try {
      const value = JSON.parse(fact.value_json)
      
      if (fact.fact_key === 'tradeoff.salary_vs_wlb') {
        appliedFacts.push({
          fact_key: fact.fact_key,
          effect_summary: value.value === 'wlb' 
            ? 'ì›Œë¼ë°¸ì„ ì—°ë´‰ë³´ë‹¤ ì¤‘ì‹œ â†’ WLB ì§ì—…ì— +15 ë¶€ìŠ¤íŠ¸'
            : 'ì—°ë´‰ì„ ì›Œë¼ë°¸ë³´ë‹¤ ì¤‘ì‹œ â†’ ê³ ìˆ˜ì… ì§ì—…ì— +15 ë¶€ìŠ¤íŠ¸',
        })
      }
      if (fact.fact_key === 'tradeoff.growth_vs_stability') {
        appliedFacts.push({
          fact_key: fact.fact_key,
          effect_summary: value.value === 'growth'
            ? 'ì„±ì¥ì„ ì•ˆì •ë³´ë‹¤ ì¤‘ì‹œ â†’ ì„±ì¥ ê°€ëŠ¥ì„± ì§ì—…ì— +15 ë¶€ìŠ¤íŠ¸'
            : 'ì•ˆì •ì„ ì„±ì¥ë³´ë‹¤ ì¤‘ì‹œ â†’ ì•ˆì •ì  ì§ì—…ì— +15 ë¶€ìŠ¤íŠ¸',
        })
      }
      if (fact.fact_key === 'motivation.work_hours_reason') {
        appliedFacts.push({
          fact_key: fact.fact_key,
          effect_summary: `"${value.value || value.raw?.slice(0, 20)}" ì´ìœ ë¡œ ì•¼ê·¼ ê¸°í”¼ â†’ ê´€ë ¨ ë¦¬ìŠ¤í¬ í˜ë„í‹° ì¡°ì •`,
        })
      }
    } catch {
      // JSON íŒŒì‹± ì‹¤íŒ¨ ì‹œ ë¬´ì‹œ
    }
  }
  
  // ìš”ì•½ ë¬¸ì¥ ìƒì„±
  let summary = ''
  if (keyTraits.length > 0) {
    const topTraits = keyTraits.slice(0, 3).map(t => t.trait)
    summary = `ë‹¹ì‹ ì€ ${topTraits.join(', ')} ì‚¬ëŒì…ë‹ˆë‹¤.`
  } else if (appliedFacts.length > 0) {
    summary = `${appliedFacts.length}ê°œì˜ ë‹µë³€ì„ ë¶„ì„í•˜ì—¬ ë§ì¶¤ ì¶”ì²œì„ ì œê³µí–ˆìŠµë‹ˆë‹¤.`
  } else {
    summary = 'ë” ë§ì€ ì •ë³´ë¥¼ ì…ë ¥í•˜ì‹œë©´ ë” ì •í™•í•œ ì¸ì‚¬ì´íŠ¸ë¥¼ ë“œë¦´ ìˆ˜ ìˆì–´ìš”.'
  }
  
  return {
    summary,
    key_traits: keyTraits,
    applied_facts: appliedFacts,
  }
}

// ============================================
// ì„¤ëª… ìƒì„± (Phase 1A ê°„ë‹¨ ë²„ì „)
// ============================================
function generateExplanation(
  facts: Fact[],
  appliedRules: string[],
  topJobName?: string
): string {
  if (facts.length === 0) {
    return 'ì¶”ê°€ ì§ˆë¬¸ì— ë‹µë³€í•˜ì‹œë©´ ë” ì •í™•í•œ ì¶”ì²œì„ ë°›ìœ¼ì‹¤ ìˆ˜ ìˆìŠµë‹ˆë‹¤.'
  }
  
  const explanations: string[] = []
  
  for (const fact of facts) {
    try {
      const value = JSON.parse(fact.value_json)
      const choice = value.value || value
      
      if (fact.fact_key === 'tradeoff.salary_vs_wlb') {
        if (choice === 'wlb') {
          explanations.push('ì›Œë¼ë°¸ì„ ì¤‘ì‹œí•œë‹¤ëŠ” ë‹µë³€ì„ ë°˜ì˜í–ˆìŠµë‹ˆë‹¤')
        } else {
          explanations.push('ì—°ë´‰ì„ ì¤‘ì‹œí•œë‹¤ëŠ” ë‹µë³€ì„ ë°˜ì˜í–ˆìŠµë‹ˆë‹¤')
        }
      } else if (fact.fact_key === 'tradeoff.growth_vs_stability') {
        if (choice === 'growth') {
          explanations.push('ì„±ì¥ ê°€ëŠ¥ì„±ì„ ì¤‘ì‹œí•œë‹¤ëŠ” ë‹µë³€ì„ ë°˜ì˜í–ˆìŠµë‹ˆë‹¤')
        } else {
          explanations.push('ì•ˆì •ì„±ì„ ì¤‘ì‹œí•œë‹¤ëŠ” ë‹µë³€ì„ ë°˜ì˜í–ˆìŠµë‹ˆë‹¤')
        }
      } else if (fact.fact_key.startsWith('motivation.')) {
        explanations.push(`"${choice}" ë•Œë¬¸ì´ë¼ëŠ” ì´ìœ ë¥¼ ë°˜ì˜í–ˆìŠµë‹ˆë‹¤`)
      }
    } catch {
      // JSON íŒŒì‹± ì‹¤íŒ¨ ì‹œ ë¬´ì‹œ
    }
  }
  
  if (explanations.length === 0) {
    return `${facts.length}ê°œì˜ ë‹µë³€ì„ ë°˜ì˜í•˜ì—¬ ì¶”ì²œí–ˆìŠµë‹ˆë‹¤.`
  }
  
  return explanations.join('. ') + '.'
}

// ============================================
// ìƒ˜í”Œ ì§ì—… ë°ì´í„° (Phase 1A)
// ============================================
interface SampleJob {
  job_id: string
  job_name: string
  base_like: number
  base_can: number
  base_risk: number
  attributes: Record<string, string | number>
}

function getSampleJobs(): SampleJob[] {
  return [
    {
      job_id: 'data-analyst',
      job_name: 'ë°ì´í„° ë¶„ì„ê°€',
      base_like: 70,
      base_can: 65,
      base_risk: 5,
      attributes: { wlb: 80, growth: 85, stability: 70, income: 75, solo_work: 70, remote: 60 },
    },
    {
      job_id: 'software-developer',
      job_name: 'ì†Œí”„íŠ¸ì›¨ì–´ ê°œë°œì',
      base_like: 75,
      base_can: 60,
      base_risk: 10,
      attributes: { wlb: 50, growth: 90, stability: 75, income: 85, solo_work: 65, remote: 80 },
    },
    {
      job_id: 'ux-designer',
      job_name: 'UX ë””ìì´ë„ˆ',
      base_like: 72,
      base_can: 55,
      base_risk: 8,
      attributes: { wlb: 70, growth: 80, stability: 65, income: 70, solo_work: 50, remote: 75, creative: 90 },
    },
    {
      job_id: 'project-manager',
      job_name: 'í”„ë¡œì íŠ¸ ë§¤ë‹ˆì €',
      base_like: 68,
      base_can: 70,
      base_risk: 15,
      attributes: { wlb: 40, growth: 75, stability: 70, income: 80, people_facing: 90, solo_work: 20 },
    },
    {
      job_id: 'accountant',
      job_name: 'íšŒê³„ì‚¬',
      base_like: 60,
      base_can: 75,
      base_risk: 5,
      attributes: { wlb: 60, growth: 50, stability: 95, income: 75, solo_work: 80, analytical: 90 },
    },
    {
      job_id: 'marketing-specialist',
      job_name: 'ë§ˆì¼€íŒ… ì „ë¬¸ê°€',
      base_like: 70,
      base_can: 60,
      base_risk: 20,
      attributes: { wlb: 45, growth: 70, stability: 55, income: 65, people_facing: 75, creative: 80 },
    },
    {
      job_id: 'hr-manager',
      job_name: 'ì¸ì‚¬ ë‹´ë‹¹ì',
      base_like: 62,
      base_can: 68,
      base_risk: 10,
      attributes: { wlb: 70, growth: 60, stability: 80, income: 65, people_facing: 95, solo_work: 30 },
    },
    {
      job_id: 'financial-analyst',
      job_name: 'ì¬ë¬´ ë¶„ì„ê°€',
      base_like: 65,
      base_can: 62,
      base_risk: 25,
      attributes: { wlb: 35, growth: 75, stability: 70, income: 90, solo_work: 65, analytical: 95 },
    },
    {
      job_id: 'content-creator',
      job_name: 'ì½˜í…ì¸  í¬ë¦¬ì—ì´í„°',
      base_like: 78,
      base_can: 50,
      base_risk: 30,
      attributes: { wlb: 55, growth: 65, stability: 30, income: 50, solo_work: 75, remote: 90, creative: 95 },
    },
    {
      job_id: 'teacher',
      job_name: 'êµì‚¬',
      base_like: 60,
      base_can: 70,
      base_risk: 5,
      attributes: { wlb: 80, growth: 40, stability: 95, income: 55, people_facing: 90, impact: 85 },
    },
    {
      job_id: 'nurse',
      job_name: 'ê°„í˜¸ì‚¬',
      base_like: 55,
      base_can: 60,
      base_risk: 35,
      attributes: { wlb: 25, growth: 50, stability: 90, income: 65, people_facing: 95, impact: 95 },
    },
    {
      job_id: 'consultant',
      job_name: 'ê²½ì˜ ì»¨ì„¤í„´íŠ¸',
      base_like: 72,
      base_can: 55,
      base_risk: 40,
      attributes: { wlb: 20, growth: 95, stability: 50, income: 95, people_facing: 80, analytical: 85 },
    },
  ]
}

// ============================================
// Helper Types
// ============================================
interface FollowupPayloadV2 {
  session_id: string
  user_id?: string
  question_id: string
  question_type?: string
  attribute?: string
  fact_key?: string
  answer: string
  answer_text?: string  // ììœ ì‘ë‹µ ì›ë¬¸
  confidence?: number
  // backward compatibility
  request_id?: number
  constraint?: string
  job_id?: string
  job_name?: string
}

interface FollowupResultV2 {
  success: boolean
  fact_saved: {
    fact_key: string
    value: string
    fact_level: number
  }
  reanalyze_available: boolean
  message: string
}

function determineFctLevel(factKey: string): number {
  if (factKey.startsWith('confirmed_constraint')) return 1
  if (factKey.startsWith('priority.dealbreaker')) return 2
  if (factKey.startsWith('priority.') || factKey.startsWith('tradeoff.') || factKey.startsWith('motivation.')) return 3
  return 4
}

// ============================================
// V3: Stage-based Analysis (with stage-aware follow-ups)
// ============================================
async function runAnalysisV3(
  db: D1Database,
  payload: AnalyzePayload,
  requestId: number,
  facts: Fact[],
  deepIntake?: NormalizedDeepIntake,
  stage?: AnalysisStage,
  debugMode?: boolean,
  env?: { VECTORIZE?: VectorizeIndex; AI?: Ai }
): Promise<AnalysisResultJSON> {
  // ============================================
  // V3 Enhancement: TAG Pre-Filter ì„¤ì •
  // ============================================
  const universalAnswers = payload.universal_answers || {}
  const userConstraints = extractUserConstraints(universalAnswers, payload.career_state)
  const enableTagPreFilter = Object.values(userConstraints).some(v => v === true)
  
  if (enableTagPreFilter) {
    console.log('[V3 Analyze] TAG Pre-Filter enabled:', userConstraints)
  }
  
  // ============================================
  // P1 Vectorize ê¸°ë°˜ í›„ë³´êµ° í™•ì¥ (íƒœê¹… ë¬´ê´€)
  // ============================================
  // 2026-01-26: íƒœê¹… ì˜ì¡´ë„ ì™„ì „ ì œê±°
  // - ë²¡í„° ê²€ìƒ‰ìœ¼ë¡œ ëª¨ë“  ì§ì—… ê²€ìƒ‰ ê°€ëŠ¥
  // - tagger_version ì¡°ê±´ ì—†ì´ í›„ë³´ í™•ë³´
  // ============================================
  let candidateSource: 'vectorize' | 'db_fallback' | 'sample_fallback' = 'db_fallback'
  let candidateCount = 0
  let vectorizeUsed = false
  let totalCandidates = 0

  // mini_module_result ì¶”ì¶œ (ê°œì¸í™” ìŠ¤ì½”ì–´ë§ì— ì‚¬ìš© - sampleJobs ìƒì„± ì „ì— í•„ìš”)
  const miniModuleForFilter = 'mini_module_result' in payload ? (payload as any).mini_module_result : undefined

  // Vectorize ì‚¬ìš© ê°€ëŠ¥ ì—¬ë¶€ í™•ì¸ (OpenAI API í‚¤ í•„ìš”)
  const openaiApiKey = (env as any)?.OPENAI_API_KEY
  const useVectorize = env?.VECTORIZE && openaiApiKey
  
  let sampleJobs: Array<{
    job_id: string
    job_name: string
    base_like: number
    base_can: number
    base_risk: number
    attributes: Record<string, number | string>
  }>
  
  if (useVectorize) {
    // Vectorize ê¸°ë°˜ í›„ë³´êµ° í™•ì¥ (OpenAI Embedding ì‚¬ìš©, íƒœê¹… ë¬´ê´€)
    try {
      console.log('[V3 Analyze] Using Vectorize + OpenAI Embedding for candidate expansion')
      const expansionResult = await expandCandidates(
        db,
        env!.VECTORIZE,
        openaiApiKey,
        facts.map(f => ({ fact_key: f.fact_key, value_json: f.value_json })),
        { targetSize: 500 }  // minTaggedJobs ì œê±°
      )
      
      if (!expansionResult.fallback_used && expansionResult.candidates.length > 0) {
        // ë²¡í„° ê²°ê³¼ë¥¼ ScoredJob í˜•íƒœë¡œ ë³€í™˜
        sampleJobs = await vectorResultsToScoredJobs(db, expansionResult.candidates, miniModuleForFilter)
        candidateSource = 'vectorize'
        vectorizeUsed = true
        totalCandidates = expansionResult.candidates.length
        console.log(`[V3 Analyze] Vectorize returned ${totalCandidates} candidates in ${expansionResult.search_duration_ms}ms`)
      } else {
        // Vectorize ì‹¤íŒ¨ ì‹œ ê¸°ì¡´ ë°©ì‹
        throw new Error('Vectorize fallback triggered')
      }
    } catch (vectorError) {
      console.log('[V3 Analyze] Vectorize unavailable, using DB fallback (no tagging required)')
      // Fallback: jobs JOIN job_attributes (íƒœê¹… ì—¬ë¶€ ë¬´ê´€)
      const jobAttrs = await db.prepare(`
        SELECT 
          ja.job_id, ja.job_name,
          j.slug, j.image_url,
          COALESCE(ja.wlb, 50) as wlb, 
          COALESCE(ja.growth, 50) as growth, 
          COALESCE(ja.stability, 50) as stability, 
          COALESCE(ja.income, 50) as income,
          COALESCE(ja.teamwork, 50) as teamwork, 
          COALESCE(ja.solo_deep, 50) as solo_deep, 
          COALESCE(ja.analytical, 50) as analytical, 
          COALESCE(ja.creative, 50) as creative, 
          COALESCE(ja.execution, 50) as execution, 
          COALESCE(ja.people_facing, 50) as people_facing,
          COALESCE(ja.work_hours, 'regular') as work_hours, 
          COALESCE(ja.shift_work, 'rare') as shift_work, 
          COALESCE(ja.travel, 'rare') as travel, 
          COALESCE(ja.remote_possible, 'possible') as remote_possible,
          COALESCE(ja.degree_required, 'none') as degree_required,
          COALESCE(ja.license_required, 'none') as license_required
        FROM job_attributes ja
        INNER JOIN jobs j ON ja.job_id = j.id
        ORDER BY RANDOM()
        LIMIT 500
      `).all<{
        job_id: string
        job_name: string
        slug: string
        image_url: string | null
        wlb: number
        growth: number
        stability: number
        income: number
        teamwork: number
        solo_deep: number
        analytical: number
        creative: number
        execution: number
        people_facing: number
        work_hours: string
        shift_work: string
        travel: string
        remote_possible: string
        degree_required: string
        license_required: string
      }>()

      const useDbData = jobAttrs.results && jobAttrs.results.length > 0
      candidateSource = useDbData ? 'db_fallback' : 'sample_fallback'
      candidateCount = jobAttrs.results?.length || 0

      sampleJobs = useDbData
        ? jobAttrs.results!.map(j => {
            const personalized = calculatePersonalizedBaseScores(j, miniModuleForFilter)
            return {
              job_id: j.job_id,
              job_name: j.job_name,
              slug: j.slug,
              image_url: j.image_url,
              base_like: personalized.like,
              base_can: personalized.can,
              base_risk: 10,
              attributes: {
                wlb: j.wlb,
                growth: j.growth,
                stability: j.stability,
                income: j.income,
                remote: j.remote_possible === 'full' ? 100 : j.remote_possible === 'partial' ? 50 : 0,
                solo_work: j.solo_deep,
                people_facing: j.people_facing,
                analytical: j.analytical,
                creative: j.creative,
              },
            }
          })
        : []
    }
  } else {
    // Vectorize ì—†ì´ DBì—ì„œ ì§ì ‘ ì¡°íšŒ (íƒœê¹… ì—¬ë¶€ ë¬´ê´€)
    console.log('[V3 Analyze] Vectorize not available, using DB fallback (no tagging required)')
    const jobAttrs = await db.prepare(`
      SELECT
        ja.job_id, ja.job_name,
        j.slug, j.image_url, j.api_data_json, j.merged_profile_json, j.ai_data_json,
        COALESCE(ja.wlb, 50) as wlb,
        COALESCE(ja.growth, 50) as growth,
        COALESCE(ja.stability, 50) as stability,
        COALESCE(ja.income, 50) as income,
        COALESCE(ja.teamwork, 50) as teamwork,
        COALESCE(ja.solo_deep, 50) as solo_deep,
        COALESCE(ja.analytical, 50) as analytical,
        COALESCE(ja.creative, 50) as creative,
        COALESCE(ja.execution, 50) as execution,
        COALESCE(ja.people_facing, 50) as people_facing,
        COALESCE(ja.work_hours, 'regular') as work_hours,
        COALESCE(ja.shift_work, 'rare') as shift_work,
        COALESCE(ja.travel, 'rare') as travel,
        COALESCE(ja.remote_possible, 'possible') as remote_possible,
        COALESCE(ja.degree_required, 'none') as degree_required,
        COALESCE(ja.license_required, 'none') as license_required
      FROM job_attributes ja
      INNER JOIN jobs j ON ja.job_id = j.id
      ORDER BY RANDOM()
      LIMIT 500
    `).all<{
      job_id: string
      job_name: string
      slug: string
      image_url: string | null
      api_data_json: string | null
      merged_profile_json: string | null
      ai_data_json: string | null
      wlb: number
      growth: number
      stability: number
      income: number
      teamwork: number
      solo_deep: number
      analytical: number
      creative: number
      execution: number
      people_facing: number
      work_hours: string
      shift_work: string
      travel: string
      remote_possible: string
      degree_required: string
      license_required: string
    }>()
    
    const useDbData = jobAttrs.results && jobAttrs.results.length > 0
    candidateSource = useDbData ? 'db_fallback' : 'sample_fallback'
    candidateCount = jobAttrs.results?.length || 0
    
    sampleJobs = useDbData
      ? jobAttrs.results!.map(j => {
          const personalized = calculatePersonalizedBaseScores(j, miniModuleForFilter)
          return {
            job_id: j.job_id,
            job_name: j.job_name,
            slug: j.slug,
            image_url: j.image_url,
            job_description: extractJobDescription(j.api_data_json, j.merged_profile_json, j.job_name, j.ai_data_json),
            base_like: personalized.like,
            base_can: personalized.can,
            base_risk: 10,
            attributes: {
              wlb: j.wlb,
              growth: j.growth,
              stability: j.stability,
              income: j.income,
              remote: j.remote_possible === 'full' ? 100 : j.remote_possible === 'partial' ? 50 : 0,
              solo_work: j.solo_deep,
              people_facing: j.people_facing,
              analytical: j.analytical,
              creative: j.creative,
            },
          }
        })
      : []
  }
  
  // DB ë°ì´í„°ê°€ ì—†ìœ¼ë©´ ì—ëŸ¬ ë°˜í™˜
  if (sampleJobs.length === 0) {
    throw new Error('ì§ì—… ë°ì´í„°ë¥¼ ë¶ˆëŸ¬ì˜¬ ìˆ˜ ì—†ìŠµë‹ˆë‹¤. job_attributes í…Œì´ë¸”ì„ í™•ì¸í•˜ì„¸ìš”.')
  }
  
  // ============================================
  // ì „ì²´ ì§ì—… ìˆ˜ ì¡°íšŒ (í‰ê°€ ì§ì—… ìˆ˜ í‘œì‹œìš©)
  // ============================================
  let totalJobsInDB = sampleJobs.length
  try {
    const countResult = await db.prepare('SELECT COUNT(*) as cnt FROM jobs').first<{ cnt: number }>()
    totalJobsInDB = countResult?.cnt || sampleJobs.length
    console.log(`[V3 Analyze] Total jobs in DB: ${totalJobsInDB}`)
  } catch (error) {
    console.warn('[V3 Analyze] Failed to count total jobs:', error)
  }
  
  // ============================================
  // Phase 2.1: ë¯¸ë‹ˆëª¨ë“ˆ ê¸°ë°˜ Hard Filter (ë²¡í„° ê²€ìƒ‰ ì „ ì ìš©)
  // ============================================
  // ë¯¸ë‹ˆëª¨ë“ˆ ê¸°ë°˜ Hard Exclusion ì ìš©
  const { filtered: filteredJobs, filterResult: miniModuleFilterResult } = applyMiniModuleHardFilter(
    sampleJobs,
    miniModuleForFilter
  )
  
  // í•„í„°ë§ ê²°ê³¼ ë¡œê·¸
  if (miniModuleFilterResult.stats.totalFiltered > 0) {
    console.log(`[V3 Analyze] MiniModule Hard Filter: ${miniModuleFilterResult.stats.beforeCount} â†’ ${miniModuleFilterResult.stats.afterCount} jobs`)
    console.log(`[V3 Analyze] Applied rules:`, miniModuleFilterResult.appliedRules.map(r => `${r.ruleId}(${r.excludedCount})`))
  }
  
  // í•„í„°ë§ëœ ì§ì—… ëª©ë¡ ì‚¬ìš©
  const jobsToScore = filteredJobs.length > 0 ? filteredJobs : sampleJobs
  
  // 2. Fact boosts ê³„ì‚°
  const factBoosts = calculateFactBoosts(facts)

  // P2: ê²€ì¦ëœ Can ì¶”ì¶œ (Can ê¸°ë°˜ TAG í•„í„°ìš©)
  const verifiedCan: VerifiedCanMap = extractVerifiedCanFromFacts(
    facts.map(f => ({ fact_key: f.fact_key, value_json: f.value_json }))
  )
  console.log(`[V3 Analyze] Verified Can map:`, verifiedCan)

  // ë¯¸ë‹ˆëª¨ë“ˆ ê¸°ë°˜ ì¶”ê°€ Risk Penalty (Hard Bias derived)
  const hardBiasConstraints = miniModuleForFilter?.energy_drain_flags
    ? deriveConstraintsFromHardBias(miniModuleForFilter.energy_drain_flags)
    : null

  // P3: ì„±ì¥ê³¡ì„  ì„ í˜¸ë„ ì¶”ì¶œ
  const growthPreference = extractGrowthPreference(miniModuleForFilter)
  console.log(`[V3 Analyze] Growth preference: ${growthPreference.preferredCurve} (confidence: ${growthPreference.confidence.toFixed(2)})`)

  // P3: ë‚´ë©´ê°ˆë“± ì‹¬ê°ë„ í™•ì¸
  const conflictSeverity = calculateConflictSeverity(miniModuleForFilter)
  console.log(`[V3 Analyze] Conflict severity: ${conflictSeverity.severity} (${conflictSeverity.level})`)

  // 3. ê° ì§ì—…ì— ì ìˆ˜ ì ìš© (slug, image_url í¬í•¨ + ë¯¸ë‹ˆëª¨ë“ˆ Risk Penalty)
  const scoredJobs: ScoredJob[] = jobsToScore.map(job => {
    const baseScores: JobScores = {
      like: job.base_like,
      can: job.base_can,
      risk_penalty: job.base_risk,
    }
    
    const adjusted = applyFactBoostsToJob(baseScores, job.attributes, factBoosts)
    
    // ë¯¸ë‹ˆëª¨ë“ˆ ê¸°ë°˜ ì¶”ê°€ Risk Penalty ì ìš©
    let additionalPenalty = 0
    const miniModulePenalty = calculateMiniModuleRiskPenalty(miniModuleForFilter, job.attributes)
    additionalPenalty += miniModulePenalty.penalty
    
    // Hard Bias ê¸°ë°˜ ì¶”ê°€ Penalty ì ìš©
    if (hardBiasConstraints && hardBiasConstraints.penalties.length > 0) {
      const hardBiasPenalty = applyHardBiasPenalty(job.attributes, hardBiasConstraints)
      additionalPenalty += hardBiasPenalty.penalty
    }

    // P2: Can ê¸°ë°˜ TAG í•„í„° í˜ë„í‹° ì ìš©
    const canFilterResult = applyCanBasedFilter(job.attributes, verifiedCan)
    additionalPenalty += canFilterResult.totalPenalty

    // P3: ì„±ì¥ê³¡ì„  ë§¤ì¹­ (Like Boost / Risk Penalty)
    const growthMatch = matchGrowthCurves(growthPreference, job.attributes, job.job_name)
    additionalPenalty += growthMatch.riskPenalty

    // P3: ë‚´ë©´ê°ˆë“± â†’ Risk ì¡°ì •
    const conflictRisk = calculateConflictRisk(miniModuleForFilter, job.attributes)
    additionalPenalty += conflictRisk.totalRiskAdjust

    const totalRiskPenalty = adjusted.risk_penalty + additionalPenalty

    // P0: Can-Like ë°¸ëŸ°ìŠ¤ ìº¡ ì ìš©
    // P3: ì„±ì¥ê³¡ì„  ë§¤ì¹­ Like Boost í¬í•¨
    const likeWithGrowthBoost = adjusted.like + growthMatch.likeBoost
    const balanced = applyBalanceCap(likeWithGrowthBoost, adjusted.can)
    const fit = Math.round(0.55 * balanced.like + 0.45 * balanced.can - totalRiskPenalty)

    // ì„ì‹œ scoredJob ê°ì²´ ìƒì„± (explanation ìƒì„±ìš©)
    const tempScoredJob: ScoredJob = {
      job_id: job.job_id,
      job_name: job.job_name,
      slug: job.slug,
      image_url: job.image_url || undefined,
      job_description: (job as any).job_description,
      scores: {
        fit: Math.max(0, fit),
        like: balanced.like,
        can: balanced.can,
        risk_penalty: totalRiskPenalty,
      },
      attributes: job.attributes,
    }

    // ê·¼ê±° ìƒì„± (V3: miniModuleForFilter ì „ë‹¬í•˜ì—¬ ê°œì¸í™”ëœ ì´ìœ  ìƒì„±)
    const explanation = generateJobExplanation(tempScoredJob, facts, factBoosts.applied_rules, miniModuleForFilter)
    const rationale = `[ì¢‹ì•„í•  ì´ìœ ] ${explanation.like_reason} [ì˜í•  ì´ìœ ] ${explanation.can_reason}`

    return {
      ...tempScoredJob,
      rationale,
      likeReason: explanation.like_reason,
      canReason: explanation.can_reason,
    }
  })

  // 4. ì •ë ¬ (Fit ê¸°ì¤€)
  scoredJobs.sort((a, b) => b.scores.fit - a.scores.fit)
  
  // í‰ê°€ ì§ì—… 100ê°œë¡œ í™•ì¥ (ê¸°ì¡´ 10ê°œ -> 100ê°œ)
  const top10 = scoredJobs.slice(0, 100)
  
  // Phase 4: Diversity Guard (Research Bias ë°©ì§€ + ë‹¤ì–‘ì„± í™•ë³´)
  const rawTop5 = top10.slice(0, 5)
  const diversityResult = applyDiversityGuard(rawTop5, scoredJobs)
  const top3 = diversityResult.adjusted

  // 5. Follow-up ì§ˆë¬¸ ìƒì„± (Stage-aware)
  const existingFactKeys = facts.map(f => ({ fact_key: f.fact_key }))

  let followupQuestions: FollowupQuestion[]
  if (stage) {
    // V3: Stage ê¸°ë°˜ ì§ˆë¬¸ ìƒì„±
    followupQuestions = generateStageBasedFollowups(
      scoredJobs,
      top10,
      existingFactKeys,
      stage,
      !!deepIntake
    )
  } else {
    // V2 fallback: ê¸°ì¡´ ë°©ì‹
    followupQuestions = generateFollowupQuestions({
      candidates: scoredJobs,
      topK: top10,
      existingFacts: existingFactKeys,
      hasDeepIntake: !!deepIntake,
    }, 3)
  }

  // 5.5 P0: Can ê²€ì¦ ì§ˆë¬¸ ì¶”ê°€ (ìê¸°í‰ê°€ ê°•ì  ê²½í—˜ í™•ì¸)
  // mini_module_result ì¡°ê¸° ì¶”ì¶œ (Can ê²€ì¦ìš©)
  const mmResultForCan = 'mini_module_result' in payload ? (payload as any).mini_module_result : undefined
  if (mmResultForCan?.strength_top?.length > 0) {
    // ì´ë¯¸ ê²€ì¦ëœ ê°•ì  í† í°ë“¤ ì°¾ê¸°
    const alreadyVerified = facts
      .filter(f => f.fact_key.startsWith('can_verified_'))
      .map(f => f.fact_key.replace('can_verified_', ''))

    // Can ê²€ì¦ ì§ˆë¬¸ ì„ íƒ (ì„¸ì…˜ë‹¹ ìµœëŒ€ 2ê°œ)
    const canValidationQuestions = selectCanValidationQuestions({
      max_questions_per_session: 2,
      already_verified: alreadyVerified,
      user_strength_tokens: mmResultForCan.strength_top,
    })

    // FollowupQuestion í˜•ì‹ìœ¼ë¡œ ë³€í™˜í•˜ì—¬ ì¶”ê°€
    for (const cvq of canValidationQuestions) {
      // ì„¸ì…˜ë‹¹ ì´ ì§ˆë¬¸ ìˆ˜ ìƒí•œ (4ê°œ)
      if (followupQuestions.length >= 4) break

      followupQuestions.push({
        id: cvq.id,
        type: 'can_validation',
        question: cvq.question,
        context: cvq.why_asked,
        options: cvq.options?.map(opt => ({
          value: opt.value,
          label: opt.label,
          tags: opt.tags,
        })) || [],
        fact_key: cvq.fact_key,
        affects_attributes: ['can'] as any,  // Can ì ìˆ˜ì— ì˜í–¥
      })
    }

    console.log(`[Can Validation] Added ${canValidationQuestions.length} questions, total: ${followupQuestions.length}`)
  }

  // 6. User Insight ìƒì„± (Stage-aware)
  const userInsight = stage
    ? generateStageAwareInsight(facts, deepIntake, factBoosts.applied_rules, stage)
    : generateUserInsight(facts, deepIntake, factBoosts.applied_rules)
  
  // 6.5. Premium Report ìƒì„± (V3 - LLM ê¸°ë°˜ í™œì„±í™”!)
  // mini_module_result ì¶”ì¶œ (payloadì—ì„œ)
  const miniModuleResult = 'mini_module_result' in payload ? (payload as any).mini_module_result : undefined
  const openaiKey = env.OPENAI_API_KEY  // Bindings íƒ€ì…ì— ì •ì˜ë¨
  const sessionId = `session-${requestId}`
  
  // â˜… ë””ë²„ê·¸: í™˜ê²½ ë³€ìˆ˜ í™•ì¸
  console.log(`[â˜… ENV CHECK] OPENAI_API_KEY exists: ${!!openaiKey}, AI exists: ${!!env.AI}`)
  
  // ============================================
  // LLM Judge: í›„ë³´ ì§ì—… LLM ì¬í‰ê°€ (Llama 3.1)
  // ============================================
  let llmJudgeResults: JudgeOutput | null = null
  let llmJudgeUsed = false
  
  // top10ì„ FilteredCandidate í˜•ì‹ìœ¼ë¡œ ë³€í™˜
  const candidatesForJudge = top10.map(j => ({
    job_id: j.job_id,
    job_name: j.job_name,
    slug: j.slug,
    image_url: j.image_url,
    attributes: j.attributes as Record<string, number>,
    base_scores: {
      like: j.scores.like,
      can: j.scores.can,
      risk_penalty: j.scores.risk_penalty,
    },
    exclusion_reasons: [],
  }))
  
  // SearchProfile êµ¬ì„±
  const searchProfile = {
    interests: miniModuleResult?.interests || [],
    values: miniModuleResult?.values || [],
    strengths: miniModuleResult?.strengths || [],
    constraints: userConstraints,
    sacrifice_flags: miniModuleResult?.sacrifice_flags || [],
    energy_drain_flags: miniModuleResult?.energy_drain_flags || [],
  }
  
  // roundAnswers êµ¬ì„± (deep_intakeì—ì„œ ì¶”ì¶œ)
  const roundAnswers = deepIntake?.questions?.map((q, i) => ({
    round: Math.min(3, Math.floor(i / 3) + 1) as 1 | 2 | 3,
    questionId: `q${i + 1}`,
    question: q.question,
    answer: q.answer,
    timestamp: new Date().toISOString(),
  })) || []
  
  // ë””ë²„ê·¸ ë¡œê·¸: LLM ì¡°ê±´ í™•ì¸
  console.log(`[LLM Debug] openaiKey: ${!!openaiKey}, candidates: ${candidatesForJudge.length}`)
  console.log(`[LLM Debug] openaiKey value (first 10): ${openaiKey ? String(openaiKey).substring(0, 10) + '...' : 'MISSING'}`)
  
  // LLM Judge: OpenAI API í‚¤ê°€ ìˆì„ ë•Œë§Œ ì‹œë„ (ì—†ìœ¼ë©´ ìŠ¤í‚µ)
  if (openaiKey && candidatesForJudge.length > 0) {
    try {
      console.log(`[LLM Judge] Evaluating ${candidatesForJudge.length} candidates with OpenAI GPT-4o-mini...`)
      const judgeInput: JudgeInput = {
        candidates: candidatesForJudge,
        searchProfile,
        roundAnswers,
        universalAnswers: universalAnswers,
        miniModuleResult: miniModuleResult,
      }
      llmJudgeResults = await judgeCandidates(openaiKey, db, judgeInput)
      llmJudgeUsed = true
      console.log(`[LLM Judge] Completed: ${llmJudgeResults.results.length} jobs evaluated, avg fit: ${llmJudgeResults.stats.averageFitScore.toFixed(1)}`)
    } catch (judgeError) {
      console.error('[LLM Judge] Failed, using rule-based scores:', judgeError)
    }
  }

  // LLM Judge ê²°ê³¼ë¥¼ scoredJobsì— ë³‘í•© (ê°œì¸í™”ëœ ì´ìœ  ë°˜ì˜)
  if (llmJudgeUsed && llmJudgeResults) {
    for (const judgeResult of llmJudgeResults.results) {
      const job = scoredJobs.find(j => j.job_id === judgeResult.job_id)
      if (job) {
        if (judgeResult.likeReason) job.likeReason = judgeResult.likeReason
        if (judgeResult.canReason) job.canReason = judgeResult.canReason
        if (judgeResult.rationale) job.rationale = judgeResult.rationale
      }
    }
    console.log(`[LLM Judge Merge] Updated reasons for ${llmJudgeResults.results.length} jobs in scoredJobs`)
  }

  // ============================================
  // Scoring Trace: í™•ì‹ ë„ + ê²°ì •ë³€ìˆ˜ ê³„ì‚°
  // ============================================
  const scoringTrace = createScoringTrace(
    top10.map(j => ({
      job_id: j.job_id,
      job_name: j.job_name,
      scores: j.scores,
      attributes: j.attributes as Record<string, number>,
    })),
    facts.map(f => ({ fact_key: f.fact_key, value_json: f.value_json }))
  )
  
  const confidenceResult = calculateConfidenceScore(
    facts.map(f => ({ fact_key: f.fact_key, value_json: f.value_json, source: 'minimodule' as const, timestamp: new Date().toISOString() })),
    roundAnswers.length,
    top10.slice(0, 3).map(j => j.scores.fit)
  )
  
  const keyDecisionVars = extractKeyDecisionVariables(
    scoringTrace,
    facts.map(f => ({ fact_key: f.fact_key, value_json: f.value_json }))
  )
  
  // ============================================
  // LLM Reporter: ì‹¬ë¦¬ë¶„ì„ ë¦¬í¬íŠ¸ ìƒì„± (GPT-4o-mini)
  // ============================================
  let premiumReport
  
  // Hard Cut ë¦¬ìŠ¤íŠ¸ êµ¬ì„± (í•„í„°ë§ëœ ì§ì—…ë“¤)
  const hardCutList = miniModuleFilterResult?.excluded?.map(j => ({
    job_id: j.job_id,
    job_name: j.job_name,
    rule_id: j.rule_id || 'mini_module_filter',
    reason: j.reason || 'ë¯¸ë‹ˆëª¨ë“ˆ ê¸°ë°˜ ì œì™¸',
  })) || []
  
  console.log(`[LLM Reporter Check] openaiKey: ${!!openaiKey}, llmJudgeResults: ${!!llmJudgeResults}, candidates: ${candidatesForJudge.length}`)

  // ============================================
  // NarrativeFacts ì¡°íšŒ (ì‹¬ë¦¬ ë¶„ì„ìš©)
  // ============================================
  let narrativeFacts: NarrativeFacts | undefined
  try {
    const narrativeRow = await db.prepare(`
      SELECT high_alive_moment, lost_moment
      FROM narrative_facts
      WHERE session_id = ?
    `).bind(payload.session_id).first<{
      high_alive_moment: string | null
      lost_moment: string | null
    }>()

    if (narrativeRow && (narrativeRow.high_alive_moment || narrativeRow.lost_moment)) {
      narrativeFacts = {
        highAliveMoment: narrativeRow.high_alive_moment || '',
        lostMoment: narrativeRow.lost_moment || '',
      }
      console.log('[LLM Reporter] NarrativeFacts loaded:', {
        hasHighAlive: !!narrativeFacts.highAliveMoment,
        hasLostMoment: !!narrativeFacts.lostMoment,
      })
    } else {
      console.log('[LLM Reporter] No narrative facts found for session')
    }
  } catch (narrativeError) {
    console.warn('[LLM Reporter] Failed to load narrative facts:', narrativeError)
  }

  // RoundAnswers ì¡°íšŒ (DBì—ì„œ ì‹¤ì œ ì €ì¥ëœ 3ë¼ìš´ë“œ ë‹µë³€ ê°€ì ¸ì˜¤ê¸°)
  let dbRoundAnswers: Array<{
    roundNumber: 1 | 2 | 3
    questionId: string
    answer: string
  }> = []

  try {
    const roundAnswersRows = await db.prepare(`
      SELECT round_number, question_id, answer
      FROM round_answers
      WHERE session_id = ?
      ORDER BY round_number, question_id
    `).bind(payload.session_id).all<{
      round_number: number
      question_id: string
      answer: string
    }>()

    if (roundAnswersRows.results && roundAnswersRows.results.length > 0) {
      dbRoundAnswers = roundAnswersRows.results.map(row => ({
        roundNumber: row.round_number as 1 | 2 | 3,
        questionId: row.question_id,
        answer: row.answer,
      }))
      console.log(`[LLM Reporter] Loaded ${dbRoundAnswers.length} round answers from DB`)
    } else {
      console.log('[LLM Reporter] No round answers found in DB')
    }
  } catch (roundError) {
    console.warn('[LLM Reporter] Failed to load round answers:', roundError)
  }

  // roundAnswers ë³‘í•©: DB ìš°ì„ , deep_intakeëŠ” fallback
  const finalRoundAnswers = dbRoundAnswers.length > 0
    ? dbRoundAnswers
    : (roundAnswers || [])

  console.log(`[LLM Reporter] Final round answers count: ${finalRoundAnswers.length}`)

  // LLM Reporter: OpenAI í‚¤ë§Œ ìˆìœ¼ë©´ í˜¸ì¶œ (í›„ë³´ê°€ ìˆì„ ë•Œ)
  if (openaiKey && candidatesForJudge.length > 0) {
    try {
      console.log('[LLM Reporter] â˜…â˜…â˜… Generating premium report with GPT-4o-mini... â˜…â˜…â˜…')
      const reporterInput: ReporterInput = {
        sessionId,
        judgeResults: llmJudgeResults?.results || [],
        searchProfile,
        narrativeFacts,  // â˜… ì¶”ê°€!
        roundAnswers: finalRoundAnswers,  // â˜… DBì—ì„œ ì¡°íšŒí•œ ì‹¤ì œ ë‹µë³€ ì‚¬ìš©!
        universalAnswers: universalAnswers,
        hardCutList,
        miniModuleResult,
      }
      premiumReport = await generateLLMPremiumReport(
        env?.AI || null,
        reporterInput,
        openaiKey
      )
      console.log('[LLM Reporter] Premium report generated successfully')
    } catch (reporterError) {
      console.error('[LLM Reporter] Failed, falling back to rule-based:', reporterError)
      // Fallback: ê·œì¹™ ê¸°ë°˜ ë¦¬í¬íŠ¸
      const premiumReportInputV3: PremiumReportInput = {
        session_id: sessionId,
        facts: facts.map(f => ({ fact_key: f.fact_key, value_json: f.value_json })),
        recommendations: top10.map(j => ({
          job_id: j.job_id,
          job_name: j.job_name,
          slug: j.slug,
          image_url: j.image_url,
          scores: j.scores,
          attributes: j.attributes as Record<string, number>,
        })),
        userInsight: userInsight,
        stage: stage,
        miniModuleResult: miniModuleResult,
      }
      premiumReport = fixParticlesDeep(generatePremiumReport(premiumReportInputV3))
    }
  } else {
    // OpenAI í‚¤ ì—†ìŒ: ê·œì¹™ ê¸°ë°˜ ë¦¬í¬íŠ¸
    console.log('[Premium Report] OpenAI key not available, using rule-based report')
    const premiumReportInputV3: PremiumReportInput = {
      session_id: sessionId,
      facts: facts.map(f => ({ fact_key: f.fact_key, value_json: f.value_json })),
      recommendations: top10.map(j => ({
        job_id: j.job_id,
        job_name: j.job_name,
        slug: j.slug,
        image_url: j.image_url,
        scores: j.scores,
        attributes: j.attributes as Record<string, number>,
      })),
      userInsight: userInsight,
      stage: stage,
      miniModuleResult: miniModuleResult,
    }
    premiumReport = fixParticlesDeep(generatePremiumReport(premiumReportInputV3))
  }
  
  // 7. ì…ë ¥ ì •ë³´ ì •ë¦¬
  const lifeConstraints = extractLifeConstraints(facts)
  const universalFactsCount = facts.filter(f => f.fact_key.startsWith('profile.')).length
  const confirmedConstraints = facts
    .filter(f => f.fact_key.startsWith('confirmed_constraint.'))
    .map(f => f.fact_key.replace('confirmed_constraint.', ''))
  
  // Legacy profile ì¶”ì¶œ (V2 í˜¸í™˜)
  const legacyProfile = 'profile' in payload && payload.profile
    ? payload.profile
    : { interest: { keywords: [] }, value: { priority: [] }, skill: [], dislike: { keywords: [] }, constraints: {} }
  
  // 8. ê²°ê³¼ êµ¬ì„±
  const result: AnalysisResultJSON = {
    engine_state: stage
      ? 'phase2_stage_based'  // V3: Stage-based ë¶„ì„
      : (deepIntake ? 'phase1a_mve' : (facts.length > 0 ? 'phase1a_mve' : 'phase1a_initial')),
    engine_version: 'v3',  // V3 í”„ë¦¬ë¯¸ì—„ ë¦¬í¬íŠ¸ í™œì„±í™”
    mini_module_result: miniModuleResult || null,

    versions: {
      recipe: VERSIONS.recipe,
      tagger: VERSIONS.tagger,
      scoring: VERSIONS.scoring,
    },

    input_summary: {
      profile_revision_id: `rev-${requestId}-${Date.now()}`,
      key_interests: legacyProfile.interest?.keywords?.slice(0, 3) || [],
      key_skills: legacyProfile.skill?.map(s => s.name).slice(0, 3) || [],
      non_negotiables: Object.entries(legacyProfile.constraints || {})
        .filter(([_, v]) => v === true)
        .map(([k]) => k),
      preferences: [],
      facts_applied: facts.length,
      applied_rules: factBoosts.applied_rules,
      deep_intake_provided: !!deepIntake,
      insight_tags: userInsight?.key_traits.map(t => t.trait) || [],
      // V3 ì¶”ê°€ í•„ë“œ
      stage: stage,
      universal_facts_count: universalFactsCount,
      life_constraints: lifeConstraints,
      confirmed_constraints: confirmedConstraints,
    },
    
    fit_top3: top3.map(j => {
      // Evidence ìƒì„± (facts + job attributes ê¸°ë°˜)
      // âš ï¸ EvidenceëŠ” "ì„¤ëª…ìš©"ì´ë©° ì ìˆ˜ ê³„ì‚°ì— ì˜í–¥ ì—†ìŒ
      const evidenceFacts: EvidenceFact[] = facts.map(f => ({
        fact_key: f.fact_key,
        value_json: f.value_json,
      }))
      let evidenceLinks = buildEvidenceLinks(evidenceFacts, j)
      
      // Evidenceê°€ ë¶€ì¡±í•˜ë©´ ê¸°ë³¸ Evidence ì¶”ê°€
      if (evidenceLinks.length < 3) {
        const defaultEvidence = generateDefaultEvidence(j)
        evidenceLinks = [...evidenceLinks, ...defaultEvidence].slice(0, 10)
      }
      
      // Dislike ê²½ê³  ìƒì„±
      const dislikeWarnings = generateDislikeWarnings(facts, j)

      return {
        job_id: j.job_id,
        job_name: j.job_name,
        slug: j.slug,
        image_url: j.image_url,
        job_description: j.job_description || null,
        rationale: j.rationale || null,
        like_reason: j.likeReason || null,
        can_reason: j.canReason || null,
        fit_score: j.scores.fit,
        like_score: j.scores.like,
        can_score: j.scores.can,
        risk_details: [],
        evidence_links: evidenceLinks,
        dislike_warnings: dislikeWarnings,
      }
    }),

    // like_top10: like_score ê¸°ì¤€ìœ¼ë¡œ ë³„ë„ ì •ë ¬ (Fit >= 25 í•„í„°)
    like_top10: [...scoredJobs]
      .filter(j => j.scores.fit >= 25)
      .sort((a, b) => b.scores.like - a.scores.like)
      .slice(0, 10)
      .map(j => ({
        job_id: j.job_id,
        job_name: j.job_name,
        slug: j.slug,
        image_url: j.image_url,
        job_description: j.job_description || null,
        fit_score: j.scores.fit,
        like_score: j.scores.like,
        can_score: j.scores.can,
        rationale: j.rationale || null,
        like_reason: j.likeReason || null,
        can_reason: j.canReason || null,
      })),

    // can_top10: can_score ê¸°ì¤€ìœ¼ë¡œ ë³„ë„ ì •ë ¬ (Fit >= 25 í•„í„°)
    can_top10: [...scoredJobs]
      .filter(j => j.scores.fit >= 25)
      .sort((a, b) => b.scores.can - a.scores.can)
      .slice(0, 10)
      .map(j => ({
        job_id: j.job_id,
        job_name: j.job_name,
        slug: j.slug,
        image_url: j.image_url,
        job_description: j.job_description || null,
        fit_score: j.scores.fit,
        like_score: j.scores.like,
        can_score: j.scores.can,
        rationale: j.rationale || null,
        like_reason: j.likeReason || null,
        can_reason: j.canReason || null,
      })),

    caution_jobs: scoredJobs
      .filter(j => j.scores.risk_penalty > 20)
      .slice(0, 5)
      .map(j => ({
        job_id: j.job_id,
        job_name: j.job_name,
        slug: j.slug,
        image_url: j.image_url,
        risk_penalty: j.scores.risk_penalty,
      })),

    followup_questions: followupQuestions,
    
    ux_flags: {
      has_caution_in_top3: top3.some(j => j.scores.risk_penalty > 20),
      has_unknown_in_top3: false,
      needs_followup_question: followupQuestions.length > 0,
      counts_before_filter: {
        safe_known: scoredJobs.filter(j => j.scores.risk_penalty <= 10).length,
        safe_unknown: scoredJobs.filter(j => j.scores.risk_penalty > 10 && j.scores.risk_penalty <= 20).length,
        caution: scoredJobs.filter(j => j.scores.risk_penalty > 20).length,
        total: scoredJobs.length,
      },
      counts_after_filter: {
        safe_known: scoredJobs.filter(j => j.scores.risk_penalty <= 10).length,
        safe_unknown: scoredJobs.filter(j => j.scores.risk_penalty > 10 && j.scores.risk_penalty <= 20).length,
        caution: scoredJobs.filter(j => j.scores.risk_penalty > 20).length,
        total: scoredJobs.length,
      },
    },
    
    llm_explanation: generateExplanation(facts, factBoosts.applied_rules, top3[0]?.job_name),
    generated_at: new Date().toISOString(),
    // ì „ì²´ ì§ì—… ìˆ˜ í‘œì‹œ (DBì˜ ëª¨ë“  ì§ì—… = í‰ê°€ ëŒ€ìƒ)
    total_candidates: totalJobsInDB,
    // ì‹¤ì œ ì ìˆ˜ ê³„ì‚°ëœ ì§ì—… ìˆ˜ (í•„í„°ë§ í›„)
    scored_candidates: scoredJobs.length,
    // ë¯¸ë‹ˆëª¨ë“ˆ í•˜ë“œ í•„í„°ë¡œ ì œì™¸ëœ ì§ì—… ìˆ˜
    filtered_out_candidates: miniModuleFilterResult.stats.totalFiltered,
    
    user_insight: userInsight,
    
    // Phase 4: Diversity Guard ì •ë³´
    phase4_applied: true,
    diversity_guard_active: diversityResult.diversityApplied,
    diversity_changes: diversityResult.changes,
    
    // V3: Stage ê¸°ë°˜ ë¶„ì„ ì •ë³´
    analysis_stage: stage,
    stage_specific_insights: stage ? generateStageInsights(stage, facts) : undefined,
    
    // V3: Premium Report
    premium_report: premiumReport,
    
    // V3: LLM ëª¨ë“ˆ ì •ë³´ (2026-02-03 í™œì„±í™”)
    llm_modules: {
      judge_used: llmJudgeUsed,
      judge_stats: llmJudgeResults?.stats || null,
      reporter_used: !!openaiKey,
      // ë””ë²„ê·¸ ì •ë³´ (í™˜ê²½ ë³€ìˆ˜ í™•ì¸ìš©)
      env_check: {
        openai_key_exists: !!openaiKey,
        openai_key_prefix: openaiKey ? String(openaiKey).substring(0, 7) : null,
        ai_binding_exists: !!env.AI,
        candidates_count: candidatesForJudge.length,
      },
    },
    
    // V3: í™•ì‹ ë„ + ê²°ì •ë³€ìˆ˜ (scoring-trace.ts)
    confidence: confidenceResult,
    key_decision_variables: keyDecisionVars.slice(0, 5),  // Top 5
    scoring_trace_summary: {
      total_features: scoringTrace.candidates[0]?.feature_contributions.length || 0,
      top_decision_variable: scoringTrace.decision_variables[0]?.fact_key || null,
    },
    
    // Debug info (only included when debugMode=true)
    debug_info: debugMode ? generateDebugInfo(
      candidateSource,
      candidateCount,
      sampleJobs.length,
      top3,
      sampleJobs,
      factBoosts,
      facts,
      followupQuestions[0],
      diversityResult
    ) : undefined,
  }
  
  return result
}

// ============================================
// Debug Info Generator (for test UI)
// ============================================
interface SampleJobWithBase {
  job_id: string
  job_name: string
  base_like: number
  base_can: number
  base_risk: number
  attributes: Record<string, number>
}

function generateDebugInfo(
  candidateSource: 'vectorize' | 'db_fallback' | 'sample_fallback' | 'vector' | 'random',
  candidateCount: number,
  totalInDb: number,
  top3: ScoredJob[],
  sampleJobs: SampleJobWithBase[],
  factBoosts: { like_boost: number; can_boost: number; risk_boost: number; applied_rules: string[] },
  facts: Fact[],
  firstFollowup: FollowupQuestion | undefined,
  diversityResult: { diversityApplied: boolean; changes: string[] }
): DebugInfo {
  // Score breakdown for TOP3
  const scoreBreakdown = top3.map(job => {
    const baseJob = sampleJobs.find(s => s.job_id === job.job_id)
    const baseLike = baseJob?.base_like || 50
    const baseCan = baseJob?.base_can || 50
    const baseRisk = baseJob?.base_risk || 10
    
    // Calculate boosts per rule (simplified)
    const likeBoosts = factBoosts.applied_rules
      .filter(r => r.includes('interest') || r.includes('priority') || r.includes('value'))
      .map(r => ({ rule: r, delta: Math.round(factBoosts.like_boost / Math.max(1, factBoosts.applied_rules.length)) }))
    
    const canBoosts = factBoosts.applied_rules
      .filter(r => r.includes('workstyle') || r.includes('strength') || r.includes('skill'))
      .map(r => ({ rule: r, delta: Math.round(factBoosts.can_boost / Math.max(1, factBoosts.applied_rules.length)) }))
    
    const riskBoosts = factBoosts.applied_rules
      .filter(r => r.includes('constraint') || r.includes('dislike'))
      .map(r => ({ rule: r, delta: factBoosts.risk_boost }))
    
    return {
      job_id: job.job_id,
      job_name: job.job_name,
      base_like: baseLike,
      base_can: baseCan,
      base_risk: baseRisk,
      like_boosts: likeBoosts,
      can_boosts: canBoosts,
      risk_boosts: riskBoosts,
      final_like: job.scores.like,
      final_can: job.scores.can,
      final_risk: job.scores.risk_penalty,
      final_fit: job.scores.fit,
    }
  })
  
  // Applied facts summary
  const appliedFacts = facts.slice(0, 10).map(f => {
    let valueStr = ''
    try {
      const parsed = JSON.parse(f.value_json)
      valueStr = Array.isArray(parsed.value) ? parsed.value.join(', ') : String(parsed.value || '')
    } catch {
      valueStr = f.value_json
    }
    return {
      fact_key: f.fact_key,
      value: valueStr.slice(0, 50),
      effect: factBoosts.applied_rules.includes(f.fact_key) ? 'applied' : 'stored',
    }
  })
  
  // Followup rationale
  const followupRationale = firstFollowup ? {
    split_attribute: firstFollowup.affects_attributes?.[0] || firstFollowup.constraint || 'unknown',
    split_gain: 0.5, // Placeholder - actual splitGain calculation is complex
    reason: firstFollowup.context || 'TOP3 ì§ì—… ê°„ ë¶„ë³„ë ¥ í–¥ìƒ',
  } : undefined
  
  return {
    candidate_source: candidateSource,
    candidate_count: candidateCount,
    total_in_db: totalInDb,
    score_breakdown: scoreBreakdown,
    followup_rationale: followupRationale,
    applied_facts: appliedFacts,
    versions: {
      recipe: VERSIONS.recipe,
      tagger: VERSIONS.tagger,
      scoring: VERSIONS.scoring,
      embedding: 'bge-base-en-v1.5', // Vectorize í™œì„±í™” (2026-01-16)
    },
    diversity_guard_triggered: diversityResult.diversityApplied,
    research_bias_cap_applied: diversityResult.changes.some(c => c.includes('Research')),
  }
}

// V3: Stageë³„ ë§ì¶¤ ì¸ì‚¬ì´íŠ¸ ìƒì„±
function generateStageInsights(stage: AnalysisStage, facts: Fact[]): string[] {
  const insights: string[] = []
  
  switch (stage) {
    case 'job_explore':
      insights.push('íƒìƒ‰ ë‹¨ê³„: ë‹¤ì–‘í•œ ì§ì—…êµ°ì„ í­ë„“ê²Œ ê²€í† í•˜ê³  ìˆìŠµë‹ˆë‹¤.')
      if (facts.some(f => f.fact_key.includes('interest'))) {
        insights.push('ê´€ì‹¬ ë¶„ì•¼ë¥¼ ê¸°ë°˜ìœ¼ë¡œ ì¶”ì²œì´ ì¡°ì •ë˜ì—ˆìŠµë‹ˆë‹¤.')
      }
      break
    case 'job_student':
      insights.push('í•™ìƒ ë‹¨ê³„: ì „ê³µÂ·ì§„ë¡œ ì—°ê³„ì„±ì´ ë°˜ì˜ë˜ì—ˆìŠµë‹ˆë‹¤.')
      if (facts.some(f => f.fact_key.includes('constraints'))) {
        insights.push('ìê²©Â·ì‹œê°„ ì œì•½ì´ ê³ ë ¤ë˜ì—ˆìŠµë‹ˆë‹¤.')
      }
      break
    case 'job_early':
      insights.push('ì´ˆê¸° ê²½ë ¥ ë‹¨ê³„: ì„±ì¥ ê°€ëŠ¥ì„±ì´ ê°•ì¡°ë˜ì—ˆìŠµë‹ˆë‹¤.')
      if (facts.some(f => f.fact_key.includes('dislike'))) {
        insights.push('ê¸°í”¼ ìš”ì†Œê°€ ì œì™¸ í•„í„°ì— ë°˜ì˜ë˜ì—ˆìŠµë‹ˆë‹¤.')
      }
      break
    default:
      break
  }
  
  return insights
}

// ============================================
// Stage-based Follow-up ì§ˆë¬¸ ìƒì„±
// ============================================
function generateStageBasedFollowups(
  candidates: ScoredJob[],
  topK: ScoredJob[],
  existingFacts: { fact_key: string }[],
  stage: AnalysisStage,
  hasDeepIntake: boolean
): FollowupQuestion[] {
  // Stageì— ë§ëŠ” ì§ˆë¬¸ í’€ ê°€ì ¸ì˜¤ê¸°
  const stageQuestions = getQuestionsForStage(stage)
  
  // ì´ë¯¸ ë‹µë³€í•œ fact_key ì œì™¸
  const answeredKeys = new Set(existingFacts.map(f => f.fact_key))
  const availableQuestions = stageQuestions.filter(q => !answeredKeys.has(q.fact_key))
  
  if (availableQuestions.length === 0) {
    // Stage ì§ˆë¬¸ì´ ì—†ìœ¼ë©´ ê¸°ì¡´ ë°©ì‹ìœ¼ë¡œ fallback
    return generateFollowupQuestions({
      candidates,
      topK,
      existingFacts,
      hasDeepIntake,
    }, 3)
  }
  
  // ì •ë³´ì´ë“ ì ìˆ˜ ê³„ì‚° (ê°„ë‹¨ ë²„ì „)
  const scoredQuestions = availableQuestions.map(q => {
    let score = 0
    
    // 1. íƒ€ì…ë³„ ê¸°ë³¸ ì ìˆ˜
    if (q.type === 'behavior' && !hasDeepIntake) score += 10
    if (q.type === 'tradeoff') score += 8
    if (q.type === 'projection') score += 6
    if (q.type === 'scenario') score += 5
    if (q.type === 'narrative' && hasDeepIntake) score += 7
    
    // 2. TOP10 ì°¨ë³„í™” ê°€ëŠ¥ì„±
    const affectedAttrs = q.affects_attributes
    const attrVariance = calculateAttributeVariance(topK, affectedAttrs)
    score += attrVariance * 2
    
    return { question: q, score }
  })
  
  // ì ìˆ˜ ê¸°ì¤€ ì •ë ¬
  scoredQuestions.sort((a, b) => b.score - a.score)
  
  // ìƒìœ„ 3~5ê°œ ì„ íƒ
  const maxQuestions = hasDeepIntake ? 3 : 5
  const selected = scoredQuestions.slice(0, maxQuestions)
  
  // FollowupQuestion í˜•ì‹ìœ¼ë¡œ ë³€í™˜
  return selected.map(sq => ({
    id: sq.question.question_id,
    type: sq.question.type as any,
    question: getQuestionText(sq.question, stage),
    context: `Stage: ${stage}`,
    options: sq.question.options.map(o => ({
      value: o.value,
      label: o.label,
      tags: o.tags,
    })),
    fact_key: sq.question.fact_key,
    affects_attributes: sq.question.affects_attributes,
  }))
}

// ì†ì„± ë¶„ì‚° ê³„ì‚° (TOP10 ê°„ ì°¨ì´)
function calculateAttributeVariance(jobs: ScoredJob[], attrs: string[]): number {
  if (jobs.length === 0 || attrs.length === 0) return 0
  
  let totalVariance = 0
  for (const attr of attrs) {
    const values = jobs.map(j => {
      const attrValue = (j.attributes as any)[attr]
      return typeof attrValue === 'number' ? attrValue : 50
    })
    
    const mean = values.reduce((a, b) => a + b, 0) / values.length
    const variance = values.reduce((sum, v) => sum + Math.pow(v - mean, 2), 0) / values.length
    totalVariance += Math.sqrt(variance)
  }
  
  return totalVariance / attrs.length
}

// ============================================
// Stage-aware User Insight ìƒì„±
// ============================================
function generateStageAwareInsight(
  facts: Fact[],
  deepIntake: NormalizedDeepIntake | undefined,
  appliedRules: string[],
  stage: AnalysisStage
): UserInsight | undefined {
  // ê¸°ë³¸ ì¸ì‚¬ì´íŠ¸ ìƒì„±
  const baseInsight = generateUserInsight(facts, deepIntake, appliedRules)
  
  if (!baseInsight) {
    // factsê°€ ì—†ì–´ë„ stage ê¸°ë°˜ ê¸°ë³¸ ë©”ì‹œì§€
    const wording = INSIGHT_WORDING[stage]
    return {
      summary: 'ë” ë§ì€ ì •ë³´ë¥¼ ì…ë ¥í•˜ì‹œë©´ ë§ì¶¤ ì¸ì‚¬ì´íŠ¸ë¥¼ ë“œë¦´ ìˆ˜ ìˆì–´ìš”.',
      key_traits: [],
      applied_facts: [],
    }
  }
  
  // Stageë³„ wording ì ìš©
  const wording = INSIGHT_WORDING[stage]
  
  // ìš”ì•½ ë¬¸ì¥ ì¬ìƒì„±
  const topTraits = baseInsight.key_traits.slice(0, 3).map(t => t.trait)
  let summary = ''
  
  if (topTraits.length > 0) {
    summary = `${wording.summary_prefix}${topTraits.join(', ')} ì„±í–¥ì´ ë³´ì…ë‹ˆë‹¤.`
  } else if (baseInsight.applied_facts.length > 0) {
    summary = `${baseInsight.applied_facts.length}ê°œì˜ ë‹µë³€ì„ ë¶„ì„í•˜ì—¬ ë§ì¶¤ ì¶”ì²œì„ ì œê³µí–ˆìŠµë‹ˆë‹¤.`
  } else {
    summary = 'ë” ë§ì€ ì •ë³´ë¥¼ ì…ë ¥í•˜ì‹œë©´ ë” ì •í™•í•œ ì¸ì‚¬ì´íŠ¸ë¥¼ ë“œë¦´ ìˆ˜ ìˆì–´ìš”.'
  }
  
  // evidence ë¼ë²¨ ì—…ë°ì´íŠ¸
  const updatedTraits = baseInsight.key_traits.map(trait => ({
    ...trait,
    evidence: trait.evidence.includes('ì„ íƒ') 
      ? trait.evidence.replace('ì„ íƒ', wording.evidence_label)
      : trait.evidence,
  }))
  
  return {
    summary,
    key_traits: updatedTraits,
    applied_facts: baseInsight.applied_facts,
  }
}

// Life constraints ì¶”ì¶œ
function extractLifeConstraints(facts: Fact[]): string[] {
  const lifeConstraintFact = facts.find(f => f.fact_key === 'profile.life_constraint')
  if (!lifeConstraintFact) return []
  
  try {
    const value = JSON.parse(lifeConstraintFact.value_json)
    return Array.isArray(value.value) ? value.value : [value.value]
  } catch {
    return []
  }
}

// ============================================
// V3 Premium Report API
// ============================================
analyzerRoutes.post('/v3/report', async (c) => {
  const env = c.env as { DB: D1Database }
  const db = env.DB
  
  try {
    const body = await c.req.json<{
      request_id: number
      session_id?: string
    }>()
    
    // request_idë¡œ ë¶„ì„ ê²°ê³¼ ì¡°íšŒ (ai_analysis_requests + ai_analysis_results JOIN)
    const requestRow = await db.prepare(`
      SELECT 
        req.id as request_id, 
        req.prompt_payload as request_payload, 
        res.result_json
      FROM ai_analysis_requests req
      LEFT JOIN ai_analysis_results res ON req.id = res.request_id
      WHERE req.id = ?
    `).bind(body.request_id).first<{
      request_id: number
      request_payload: string
      result_json: string | null
    }>()
    
    if (!requestRow) {
      return c.json({ 
        error: 'NOT_FOUND',
        message: 'ë¶„ì„ ê²°ê³¼ë¥¼ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤'
      }, 404)
    }
    
    if (!requestRow.result_json) {
      return c.json({
        error: 'NOT_READY',
        message: 'ë¶„ì„ì´ ì•„ì§ ì™„ë£Œë˜ì§€ ì•Šì•˜ìŠµë‹ˆë‹¤'
      }, 400)
    }
    
    // ë¶„ì„ ê²°ê³¼ íŒŒì‹±
    const analysisResult = JSON.parse(requestRow.result_json)
    
    // facts ì¡°íšŒ (facts í…Œì´ë¸”ì—ì„œ, ì—†ìœ¼ë©´ ë¹ˆ ë°°ì—´)
    let facts: { fact_key: string; value_json: string }[] = []
    try {
      const factsResult = await db.prepare(`
        SELECT fact_key, value_json
        FROM facts
        WHERE session_id = ?
        ORDER BY collected_at DESC
      `).bind(body.session_id || `session-${body.request_id}`).all<{
        fact_key: string
        value_json: string
      }>()
      facts = factsResult.results || []
    } catch (e) {
      // facts í…Œì´ë¸”ì´ ì—†ê±°ë‚˜ ì—ëŸ¬ì‹œ ë¹ˆ ë°°ì—´ ìœ ì§€
      console.log('Facts query failed, using empty array:', e)
    }
    
    // ì¶”ì²œ ê²°ê³¼ë¥¼ ScoredJobForEvidence í˜•íƒœë¡œ ë³€í™˜
    const recommendations: ScoredJobForEvidence[] = (analysisResult.fit_top3 || []).map((job: any) => ({
      job_id: job.job_id,
      job_name: job.job_name,
      scores: {
        fit: job.fit_score || 50,
        like: job.like_score || 50,
        can: job.can_score || 50,
        risk_penalty: 0,
      },
      attributes: {
        wlb: 50,
        growth: 50,
        stability: 50,
        income: 50,
        ...job.attributes,
      },
    }))
    
    // Premium Report ìƒì„±
    const sessionId = body.session_id || `session-${body.request_id}`
    const reportInput: PremiumReportInput = {
      session_id: sessionId,
      facts: facts.map(f => ({
        fact_key: f.fact_key,
        value_json: f.value_json,
      })),
      recommendations,
      userInsight: analysisResult.user_insight,
      stage: analysisResult.input_summary?.stage,
    }
    
    const premiumReport = generatePremiumReport(reportInput)
    
    // ============================================
    // User Profile ìŠ¤ëƒ…ìƒ· ì €ì¥ (P1 ê¸°ëŠ¥)
    // ============================================
    try {
      // ì´ì „ ìŠ¤ëƒ…ìƒ· ì¡°íšŒ
      const previousSnapshot = await getLatestProfileSnapshot(db, sessionId)
      
      // í”„ë¡œí•„ ë¹Œë“œ
      const profile = await buildProfileFromTurns(db, sessionId, previousSnapshot?.profile)
      
      // ìŠ¤ëƒ…ìƒ· ì €ì¥
      await saveProfileSnapshot(
        db,
        sessionId,
        undefined, // user_idëŠ” ì„¸ì…˜ì—ì„œ ê°€ì ¸ì˜¬ ìˆ˜ ìˆìœ¼ë©´ ì¶”ê°€
        body.request_id,
        premiumReport.report_id,
        'premium_report',
        profile,
        previousSnapshot?.id
      )
      
      console.log(`[V3 Report] Profile snapshot saved for session: ${sessionId}`)
    } catch (profileError) {
      // í”„ë¡œí•„ ì €ì¥ ì‹¤íŒ¨í•´ë„ ë³´ê³ ì„œëŠ” ë°˜í™˜ (graceful degradation)
      console.error('[V3 Report] Profile snapshot save failed:', profileError)
    }
    
    return c.json({
      success: true,
      report: premiumReport,
    })
    
  } catch (error) {
    console.error('[V3 Report] Error:', error)
    return c.json({
      error: 'INTERNAL_ERROR',
      message: error instanceof Error ? error.message : 'Unknown error',
    }, 500)
  }
})

// ============================================
// V3 Purpose-based Followup API
// ============================================
analyzerRoutes.post('/v3/followup-questions', async (c) => {
  const env = c.env as { DB: D1Database }
  const db = env.DB
  
  try {
    const body = await c.req.json<{
      request_id: number
      session_id?: string
    }>()
    
    // ê¸°ì¡´ ë¶„ì„ ê²°ê³¼ ì¡°íšŒ (ai_analysis_results í…Œì´ë¸”ì—ì„œ)
    const requestRow = await db.prepare(`
      SELECT result_json
      FROM ai_analysis_results
      WHERE request_id = ?
    `).bind(body.request_id).first<{ result_json: string | null }>()
    
    if (!requestRow?.result_json) {
      return c.json({
        error: 'NOT_FOUND',
        message: 'ë¶„ì„ ê²°ê³¼ë¥¼ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤'
      }, 404)
    }
    
    const analysisResult = JSON.parse(requestRow.result_json)
    
    // facts ì¡°íšŒ (facts í…Œì´ë¸”ì—ì„œ, ì—†ìœ¼ë©´ ë¹ˆ ë°°ì—´)
    let facts: { fact_key: string; value_json: string }[] = []
    try {
      const factsResult = await db.prepare(`
        SELECT fact_key, value_json
        FROM facts
        WHERE session_id = ?
        ORDER BY collected_at DESC
      `).bind(body.session_id || `session-${body.request_id}`).all<{
        fact_key: string
        value_json: string
      }>()
      facts = factsResult.results || []
    } catch (e) {
      console.log('Facts query failed, using empty array:', e)
    }
    
    // ì¶”ì²œ ê²°ê³¼ ë³€í™˜
    const topK = (analysisResult.fit_top3 || []).map((job: any) => ({
      job_id: job.job_id,
      job_name: job.job_name,
      scores: {
        fit: job.fit_score || 50,
        like: job.like_score || 50,
        can: job.can_score || 50,
        risk_penalty: 0,
      },
      attributes: {
        wlb: 50,
        growth: 50,
        stability: 50,
        income: 50,
        people_facing: 50,
        remote_possible: 50,
        ...job.attributes,
      },
    }))
    
    // Purpose-based Followup ìƒì„±
    const input: PurposeBasedFollowupInput = {
      candidates: topK,
      topK,
      facts,
      maxQuestions: 3,
    }
    
    const followupQuestions = generatePurposeBasedFollowups(input)
    
    return c.json({
      success: true,
      followup_questions: followupQuestions,
      question_count: followupQuestions.length,
    })
    
  } catch (error) {
    console.error('[V3 Followup Questions] Error:', error)
    return c.json({
      error: 'INTERNAL_ERROR',
      message: error instanceof Error ? error.message : 'Unknown error',
    }, 500)
  }
})

// ============================================
// Tagger Routes ì œê±°ë¨ (2026-01-26)
// íƒœê¹… ì˜ì¡´ë„ ì™„ì „ ì œê±° - ë²¡í„° ê²€ìƒ‰ìœ¼ë¡œ ëŒ€ì²´
// ============================================

// ============================================
// Mount Draft Routes (P0: ì„ì‹œì €ì¥)
// ============================================
analyzerRoutes.route('/draft', draftRoutes)

// ============================================
// Mount History Routes (ê²°ê³¼ ì´ë ¥)
// ============================================
analyzerRoutes.route('/history', historyRoutes)

// ============================================
// Mount Resume Routes (P0: ì´ë ¥ì„œ í…ìŠ¤íŠ¸ íŒŒì‹±)
// ============================================
analyzerRoutes.route('/resume', resumeRoutes)

// ============================================
// Mount Profile Routes (í”„ë¡œí•„ ì¡°íšŒ/ì—…ë°ì´íŠ¸)
// ============================================
analyzerRoutes.route('/profile', profileRoutes)

// ============================================
// V3: 3ë¼ìš´ë“œ ì‹¬ì¸µ ì§ˆë¬¸ ìƒì„± API (2026-01-16)
// POST /api/ai-analyzer/v3/round-questions
// ============================================
import { generateRoundQuestions, buildSearchProfileFromAnswers } from './llm-interviewer'

analyzerRoutes.post('/v3/round-questions', async (c) => {
  const env = c.env as Bindings
  const db = env.DB
  
  try {
    const body = await c.req.json<{
      session_id: string
      round_number: 1 | 2 | 3
      narrative_facts?: { highAliveMoment: string; lostMoment: string; life_story?: string; storyAnswer?: string }
      previous_round_answers?: Array<{
        questionId: string
        questionText?: string  // ì§ˆë¬¸ í…ìŠ¤íŠ¸ (ë‹¤ìŒ ë¼ìš´ë“œ ì»¨í…ìŠ¤íŠ¸ìš©)
        roundNumber: 1 | 2 | 3
        answer: string
        answeredAt: string
      }>
      universal_answers?: Record<string, string | string[]>
      career_state?: {
        role_identity: string
        career_stage_years: string
        transition_status: string
      }
      transition_signal?: Record<string, any>
      // ë¯¸ë‹ˆëª¨ë“ˆ ê²°ê³¼ (LLM íŒë‹¨ ì•µì»¤!)
      mini_module_result?: {
        interest_top: string[]
        value_top: string[]
        strength_top: string[]
        constraint_flags: string[]
        low_confidence_flags?: string[]
        internal_conflict_flags?: string[]
      }
    }>()
    
    const { session_id, round_number, narrative_facts, previous_round_answers, universal_answers, career_state, transition_signal, mini_module_result } = body
    
    // ê²€ì¦
    if (!session_id) {
      return c.json({ success: false, error: 'session_id is required' }, 400)
    }
    if (![1, 2, 3].includes(round_number)) {
      return c.json({ success: false, error: 'round_number must be 1, 2, or 3' }, 400)
    }
    
    // ============================================
    // LLM Gate: í•„ìˆ˜ ë°ì´í„° ê²€ì¦ + í´ë°± ì§ˆë¬¸ ë°˜í™˜
    // ============================================
    // AggregatedProfile ë¹Œë“œ (ìš”ì²­ ë°ì´í„°ë¡œ ì„ì‹œ ìƒì„±)
    const tempProfile: AggregatedProfile = {
      profile_version: 0,
      generated_at: new Date().toISOString(),
      anchors: {
        interest_top: mini_module_result?.interest_top || [],
        value_top: mini_module_result?.value_top || [],
        strength_top: mini_module_result?.strength_top || [],
        constraint_flags: mini_module_result?.constraint_flags || [],
        low_confidence_flags: mini_module_result?.low_confidence_flags,
        internal_conflict_flags: mini_module_result?.internal_conflict_flags,
      },
      narrative: {
        life_story: narrative_facts?.life_story || narrative_facts?.storyAnswer,
        high_alive: narrative_facts?.highAliveMoment,
        lost_moment: narrative_facts?.lostMoment,
      },
      universals: {
        priority: universal_answers?.univ_priority as string | undefined,
        interests: universal_answers?.univ_interest as string[] | undefined,
        dislikes: universal_answers?.univ_dislike as string[] | undefined,
      },
      transition: {},
      rounds: previous_round_answers ? [{
        round: (round_number > 1 ? round_number - 1 : 1) as 1|2|3,
        questions: [],
        answers: previous_round_answers.map(a => ({ question_id: a.questionId, answer: a.answer })),
      }] : [],
      memory: {
        stable_drivers: [],
        recurring_fears: [],
        decision_rules: [],
        contradictions: [],
        open_loops: [],
        resolved_loops: [],
        emotional_triggers: [],
      },
      evidence_index: {},
    }
    
    // Gate ê²€ì¦ (ë¡œê·¸ë§Œ ë‚¨ê¸°ê³  LLM í˜¸ì¶œì€ ê³„ì† ì§„í–‰)
    const gatePhase: GatePhase = `round${round_number}` as GatePhase
    const gateResult = assertReadyForLLM(tempProfile, gatePhase)
    
    if (!gateResult.passed) {
      // Gate ì‹¤íŒ¨í•´ë„ LLM í˜¸ì¶œì€ ê³„ì† ì§„í–‰ (ë°ì´í„° ë¶€ì¡±í•´ë„ ì§ˆë¬¸ ìƒì„± ì‹œë„)
      console.log(`[V3 Round Questions] Gate warning for round ${round_number} (proceeding anyway):`, gateResult.missing)
    }
    
    // ============================================
    // DBì—ì„œ Memory ì¡°íšŒ (ëˆ„ì  ë©”ëª¨ë¦¬)
    // ============================================
    let memoryData = undefined
    try {
      const draft = await db
        .prepare('SELECT memory_json FROM analyzer_drafts WHERE session_id = ?')
        .bind(session_id)
        .first<{ memory_json?: string }>()
      
      if (draft?.memory_json) {
        memoryData = JSON.parse(draft.memory_json)
        console.log('[V3 Round Questions] Memory loaded from DB')
      }
    } catch (memError) {
      console.warn('[V3 Round Questions] Failed to load memory:', memError)
    }
    
    // ============================================
    // CAG Manager: ì„¸ì…˜ ìƒíƒœ ê´€ë¦¬ (2026-02-03 í™œì„±í™”)
    // ============================================
    let cagState: CAGState | null = null
    try {
      cagState = await getOrCreateCAGState(db, session_id)
      console.log(`[V3 Round Questions] CAG state loaded: ${cagState.asked_questions_log.length} questions asked`)
    } catch (cagError) {
      console.warn('[V3 Round Questions] CAG state load failed (continuing without):', cagError)
    }
    
    // ì´ì „ ë¼ìš´ë“œ ë‹µë³€ì„ CAGì— ê¸°ë¡
    if (cagState && previous_round_answers && previous_round_answers.length > 0) {
      for (const ans of previous_round_answers) {
        logAnswerReceived(cagState, ans.questionId, ans.answer)
      }
    }
    
    // ============================================
    // LLM Interviewer í˜¸ì¶œ (Gate í†µê³¼ + Memory + CAG í¬í•¨)
    // ============================================
    const result = await generateRoundQuestions(env.AI || null, {
      sessionId: session_id,
      roundNumber: round_number,
      narrativeFacts: narrative_facts,
      previousRoundAnswers: previous_round_answers || [],
      universalAnswers: universal_answers,
      careerState: career_state,
      transitionSignal: transition_signal,
      miniModuleResult: mini_module_result,
      memory: memoryData,  // ëˆ„ì  ë©”ëª¨ë¦¬ ì „ë‹¬!
      openaiApiKey: (env as any).OPENAI_API_KEY,  // OpenAI API í‚¤ ì „ë‹¬
    })
    
    // ============================================
    // CAG: ìƒì„±ëœ ì§ˆë¬¸ ë¡œê·¸ ê¸°ë¡ + ì¤‘ë³µ í•„í„°
    // ============================================
    let filteredQuestions = result.questions
    if (cagState) {
      // ì¤‘ë³µ ì§ˆë¬¸ í•„í„°ë§
      filteredQuestions = result.questions.filter(q => {
        // questionTextê°€ ìˆì„ ë•Œë§Œ ì¤‘ë³µ ì²´í¬ (undefined ë°©ì§€)
        if (!q.questionText) return true
        const isDuplicate = isQuestionAlreadyAsked(cagState!, q.questionText, 0.8)
        if (isDuplicate) {
          console.log(`[CAG] Filtering duplicate question: ${q.questionId}`)
        }
        return !isDuplicate
      })

      // ìƒì„±ëœ ì§ˆë¬¸ ë¡œê·¸ ê¸°ë¡
      for (const q of filteredQuestions) {
        logAskedQuestion(cagState, {
          questionId: q.questionId,
          questionText: q.questionText,
          round: round_number,
          askedAt: new Date().toISOString(),
          answered: false,
        })
      }
      
      // CAG ìƒíƒœ ì €ì¥
      try {
        await saveCAGState(db, cagState)
        console.log(`[CAG] State saved: ${cagState.asked_questions_log.length} questions tracked`)
      } catch (saveError) {
        console.warn('[CAG] Failed to save state:', saveError)
      }
    }
    
    // P2: ìˆ˜ì§‘ ì§„í–‰ë„ ìš”ì•½ ìƒì„±
    const collectionProgressSummary = cagState
      ? getCollectionProgressSummary(cagState)
      : null

    return c.json({
      success: true,
      round: result.round,
      questions: filteredQuestions,
      generated_by: result.generatedBy,
      metadata: {
        ...result.metadata,
        cag_filtered: result.questions.length - filteredQuestions.length,
        total_questions_asked: cagState?.asked_questions_log.length || 0,
      },
      collection_progress: collectionProgressSummary,
    })
    
  } catch (error: any) {
    console.error('[V3 Round Questions] Error:', error.message, error.stack)
    return c.json({
      success: false,
      error: error.message || 'Unknown error',
      detail: error.stack ? error.stack.split('\n')[0] : undefined,
    }, 500)
  }
})

// ============================================
// V3: ë¼ìš´ë“œ ë‹µë³€ ì €ì¥ API
// POST /api/ai-analyzer/v3/round-answers
// ============================================
analyzerRoutes.post('/v3/round-answers', async (c) => {
  const env = c.env as Bindings
  const db = env.DB
  
  try {
    const body = await c.req.json<{
      session_id: string
      request_id?: number
      round_number: 1 | 2 | 3
      answers: Array<{
        question_id: string
        question_text?: string
        purpose_tag: 'ENGINE' | 'AVOIDANCE' | 'INTEGRATION'
        answer: string
      }>
    }>()
    
    const { session_id, request_id, round_number, answers } = body
    
    // P0-1: session_id ê²€ì¦ ê°•í™”
    if (!session_id || session_id.trim() === '') {
      console.error('[V3 Round Answers] VALIDATION FAILED: session_id is empty or null')
      return c.json({ 
        success: false, 
        error: 'session_id is required and cannot be empty',
        detail: 'session_id_validation_failed'
      }, 400)
    }
    
    if (!answers || answers.length === 0) {
      return c.json({ success: false, error: 'answers array is required' }, 400)
    }
    
    // P0-1: ì…ë ¥ê°’ ë¡œê¹…
    console.log('[V3 Round Answers] SAVE REQUEST:', {
      session_id,
      request_id,
      round_number,
      answers_count: answers.length,
    })
    
    // ë‹µë³€ ì €ì¥
    const insertStmt = db.prepare(`
      INSERT INTO round_answers (session_id, request_id, round_number, question_id, question_text, purpose_tag, answer)
      VALUES (?, ?, ?, ?, ?, ?, ?)
    `)
    
    const results = await Promise.all(
      answers.map(ans => 
        insertStmt.bind(
          session_id,
          request_id || null,
          round_number,
          ans.question_id,
          ans.question_text || null,
          ans.purpose_tag,
          ans.answer
        ).run()
      )
    )
    
    const successCount = results.filter(r => r.success).length
    console.log('[V3 Round Answers] SAVE SUCCESS:', {
      session_id,
      round_number,
      saved_count: successCount,
      meta: results.map(r => r.meta),
    })
    
    // ============================================
    // P2: ìˆ˜ì§‘ ì§„í–‰ë„ ì—…ë°ì´íŠ¸ (Round ë‹µë³€ ì €ì¥ í›„)
    // ============================================
    let collectionProgressSummary = null
    try {
      const cagState = await getOrCreateCAGState(db, session_id)

      // ë‹µë³€ purpose_tag â†’ fact_key ë§¤í•‘
      for (const ans of answers) {
        // purpose_tagì— ë”°ë¼ fact key prefix ê²°ì •
        let factKeyPrefix = ''
        if (ans.purpose_tag === 'ENGINE') {
          // ENGINE = Like (ê´€ì‹¬ì‚¬, ê°€ì¹˜ê´€)
          factKeyPrefix = 'like.round_answer'
        } else if (ans.purpose_tag === 'AVOIDANCE') {
          // AVOIDANCE = Risk (ì œì•½ì¡°ê±´)
          factKeyPrefix = 'risk.round_answer'
        } else if (ans.purpose_tag === 'INTEGRATION') {
          // INTEGRATION = Can (ëŠ¥ë ¥, ê²½í—˜)
          factKeyPrefix = 'can.round_answer'
        }

        if (factKeyPrefix) {
          const factKey = `${factKeyPrefix}.r${round_number}.${ans.question_id}`
          updateCollectionProgress(cagState, factKey)
        }
      }

      // ì €ì¥ ë° ìš”ì•½ ìƒì„±
      await saveCAGState(db, cagState)
      collectionProgressSummary = getCollectionProgressSummary(cagState)
      console.log('[V3 Round Answers] Collection progress:', collectionProgressSummary)
    } catch (progressError: any) {
      console.warn('[V3 Round Answers] Collection progress update failed:', progressError?.message)
    }

    // ============================================
    // Memory ì—…ë°ì´íŠ¸ (Round ë‹µë³€ ì €ì¥ í›„)
    // ============================================
    let memoryUpdated = false
    try {
      // 1. Draftì—ì„œ í˜„ì¬ AggregatedProfile ì¡°íšŒ
      const draft = await db
        .prepare('SELECT * FROM analyzer_drafts WHERE session_id = ?')
        .bind(session_id)
        .first<any>()
      
      if (draft) {
        // 2. ê¸°ì¡´ profile ê°€ì ¸ì˜¤ê¸° ë˜ëŠ” ìƒì„±
        let aggregatedProfile: AggregatedProfile
        if (draft.aggregated_profile_json) {
          aggregatedProfile = JSON.parse(draft.aggregated_profile_json)
        } else {
          aggregatedProfile = createEmptyProfile(draft.profile_version || 0)
        }
        
        // 3. Round ë‹µë³€ì„ RoundAnswer í˜•ì‹ìœ¼ë¡œ ë³€í™˜
        const roundAnswers = answers.map(ans => ({
          questionId: ans.question_id,
          roundNumber: round_number,
          answer: ans.answer,
          answeredAt: new Date().toISOString(),
        }))
        
        // 4. Memory ì—…ë°ì´íŠ¸ í˜¸ì¶œ
        console.log('[V3 Round Answers] Updating memory...')
        const updatedMemory = await updateMemory(
          env.AI || null,
          aggregatedProfile,
          { type: 'round_answers', data: roundAnswers, roundNumber: round_number },
          (env as any).OPENAI_API_KEY
        )
        
        // 5. Draftì— memory_json ì €ì¥
        aggregatedProfile.memory = updatedMemory
        await db
          .prepare(`
            UPDATE analyzer_drafts SET
              memory_json = ?,
              aggregated_profile_json = ?
            WHERE session_id = ?
          `)
          .bind(
            JSON.stringify(updatedMemory),
            JSON.stringify(aggregatedProfile),
            session_id
          )
          .run()
        
        memoryUpdated = true
        console.log('[V3 Round Answers] Memory updated successfully')
      }
    } catch (memoryError: any) {
      // Memory ì—…ë°ì´íŠ¸ ì‹¤íŒ¨í•´ë„ ë‹µë³€ ì €ì¥ì€ ì„±ê³µ
      console.error('[V3 Round Answers] Memory update failed:', memoryError?.message || memoryError)
    }
    
    return c.json({
      success: true,
      saved_count: successCount,
      round_number,
      memory_updated: memoryUpdated,
      collection_progress: collectionProgressSummary,
    })
    
  } catch (error: any) {
    // P0-1: ìƒì„¸ ì—ëŸ¬ ë¡œê¹…
    const errorDetail = {
      message: error.message || 'Unknown error',
      cause: error.cause?.message || error.cause || null,
      code: error.code || null,
      stack: error.stack?.substring(0, 500) || null,
    }
    console.error('[V3 Round Answers] SAVE FAILED:', errorDetail)
    
    return c.json({
      success: false,
      error: error.message || 'Database save failed',
      detail: errorDetail.cause || 'round_answers INSERT failed',
      code: errorDetail.code,
    }, 500)
  }
})

// ============================================
// V3: ì„œìˆ í˜• ë‹µë³€ ì €ì¥ API
// POST /api/ai-analyzer/v3/narrative-facts
// ============================================
analyzerRoutes.post('/v3/narrative-facts', async (c) => {
  const env = c.env as Bindings
  const db = env.DB
  
  try {
    const body = await c.req.json<{
      session_id: string
      user_id?: string
      high_alive_moment: string
      lost_moment: string
    }>()
    
    const { session_id, user_id, high_alive_moment, lost_moment } = body
    
    // P0-1: session_id ê²€ì¦ ê°•í™”
    if (!session_id || session_id.trim() === '') {
      console.error('[V3 Narrative Facts] VALIDATION FAILED: session_id is empty or null')
      return c.json({ 
        success: false, 
        error: 'session_id is required and cannot be empty',
        detail: 'session_id_validation_failed'
      }, 400)
    }
    
    // P0-1: ì…ë ¥ê°’ ë¡œê¹…
    console.log('[V3 Narrative Facts] SAVE REQUEST:', {
      session_id,
      user_id: user_id || null,
      high_alive_moment_length: high_alive_moment?.length || 0,
      lost_moment_length: lost_moment?.length || 0,
    })
    
    // UPSERT (ê¸°ì¡´ ìˆìœ¼ë©´ ì—…ë°ì´íŠ¸)
    const result = await db.prepare(`
      INSERT INTO narrative_facts (session_id, user_id, high_alive_moment, lost_moment)
      VALUES (?, ?, ?, ?)
      ON CONFLICT(session_id) DO UPDATE SET
        high_alive_moment = excluded.high_alive_moment,
        lost_moment = excluded.lost_moment,
        created_at = datetime('now')
    `).bind(session_id, user_id || null, high_alive_moment, lost_moment).run()
    
    console.log('[V3 Narrative Facts] SAVE SUCCESS:', {
      session_id,
      meta: result.meta,
      changes: result.meta?.changes || 0,
    })
    
    return c.json({
      success: true,
      session_id,
      meta: result.meta,
    })
    
  } catch (error: any) {
    // P0-1: ìƒì„¸ ì—ëŸ¬ ë¡œê¹…
    const errorDetail = {
      message: error.message || 'Unknown error',
      cause: error.cause?.message || error.cause || null,
      code: error.code || null,
      stack: error.stack?.substring(0, 500) || null,
    }
    console.error('[V3 Narrative Facts] SAVE FAILED:', errorDetail)
    
    return c.json({
      success: false,
      error: error.message || 'Database save failed',
      detail: errorDetail.cause || 'narrative_facts UPSERT failed',
      code: errorDetail.code,
    }, 500)
  }
})

// ============================================
// Vectorize í…ŒìŠ¤íŠ¸ API (2026-01-16)
// GET /api/ai-analyzer/vectorize-test?query=ì†Œí”„íŠ¸ì›¨ì–´ ê°œë°œì
// ============================================
analyzerRoutes.get('/vectorize-test', async (c) => {
  const env = c.env as Bindings
  const query = c.req.query('query') || 'ì†Œí”„íŠ¸ì›¨ì–´ ê°œë°œ í”„ë¡œê·¸ë˜ë°'
  const topK = parseInt(c.req.query('topK') || '10')
  
  // Vectorize ë°”ì¸ë”© í™•ì¸
  const openaiApiKey = (env as any).OPENAI_API_KEY
  
  if (!env.VECTORIZE) {
    return c.json({
      success: false,
      error: 'VECTORIZE_NOT_AVAILABLE',
      message: 'Vectorize ë°”ì¸ë”©ì´ ì„¤ì •ë˜ì§€ ì•Šì•˜ìŠµë‹ˆë‹¤',
      hint: 'wrangler.jsoncì—ì„œ vectorize ë°”ì¸ë”©ì„ í™•ì¸í•˜ì„¸ìš”',
    }, 503)
  }
  
  if (!openaiApiKey) {
    return c.json({
      success: false,
      error: 'OPENAI_API_KEY_NOT_SET',
      message: 'OpenAI API í‚¤ê°€ ì„¤ì •ë˜ì§€ ì•Šì•˜ìŠµë‹ˆë‹¤',
      hint: '.dev.vars ë˜ëŠ” Cloudflare Dashboardì—ì„œ OPENAI_API_KEYë¥¼ ì„¤ì •í•˜ì„¸ìš”',
    }, 503)
  }
  
  try {
    // ì¿¼ë¦¬ ì„ë² ë”© ìƒì„± (OpenAI - í•œêµ­ì–´ ì§ì ‘ ì²˜ë¦¬)
    const startTime = Date.now()
    const { embeddings } = await generateOpenAIEmbedding(openaiApiKey, query)
    const embeddingTime = Date.now() - startTime
    
    const queryEmbedding = embeddings[0]
    
    // ë²¡í„° ê²€ìƒ‰
    // Cloudflare Vectorize limits: returnMetadata='indexed' â†’ max topK=100
    const clampedTopK = Math.min(topK, 100)
    const searchStart = Date.now()
    const searchResult = await env.VECTORIZE.query(queryEmbedding, {
      topK: clampedTopK,
      returnValues: false,
      returnMetadata: 'indexed',
    })
    const searchTime = Date.now() - searchStart
    
    // ê²°ê³¼ ë³€í™˜
    const results = searchResult.matches.map(match => ({
      job_id: match.id,
      job_name: (match.metadata?.job_name as string) || match.id,
      score: match.score.toFixed(4),
      metadata: match.metadata,
    }))
    
    return c.json({
      success: true,
      query,
      topK,
      embedding_model: 'text-embedding-3-small',
      embedding_dimensions: OPENAI_EMBEDDING_DIMENSIONS,
      results_count: results.length,
      timing: {
        embedding_ms: embeddingTime,
        search_ms: searchTime,
        total_ms: embeddingTime + searchTime,
      },
      results,
    })
    
  } catch (error) {
    console.error('[Vectorize Test] Error:', error)
    return c.json({
      success: false,
      error: 'VECTORIZE_ERROR',
      message: error instanceof Error ? error.message : 'Unknown error',
    }, 500)
  }
})

// ============================================
// ê´€ë¦¬ì: ì§ì—… ë°ì´í„° ì¬ì¸ë±ì‹± (OpenAI Embedding)
// ============================================
analyzerRoutes.post('/admin/reindex-jobs', async (c) => {
  const env = c.env as Bindings
  const db = env.DB
  const openaiApiKey = (env as any).OPENAI_API_KEY
  
  if (!env.VECTORIZE) {
    return c.json({
      success: false,
      error: 'VECTORIZE_NOT_AVAILABLE',
      message: 'Vectorize ë°”ì¸ë”©ì´ ì„¤ì •ë˜ì§€ ì•Šì•˜ìŠµë‹ˆë‹¤',
    }, 503)
  }
  
  if (!openaiApiKey) {
    return c.json({
      success: false,
      error: 'OPENAI_API_KEY_NOT_SET',
      message: 'OpenAI API í‚¤ê°€ ì„¤ì •ë˜ì§€ ì•Šì•˜ìŠµë‹ˆë‹¤',
    }, 503)
  }
  
  try {
    console.log('[Reindex] Starting job reindexing with OpenAI Embedding...')
    const startTime = Date.now()
    
    const result = await indexJobsToVectorize(
      db,
      env.VECTORIZE,
      openaiApiKey,
      50  // ë°°ì¹˜ í¬ê¸° (OpenAI rate limit ê³ ë ¤)
    )
    
    const duration = Date.now() - startTime
    
    console.log(`[Reindex] Completed: ${result.indexed} indexed, ${result.errors} errors in ${duration}ms`)
    
    return c.json({
      success: true,
      indexed: result.indexed,
      errors: result.errors,
      duration_ms: duration,
      embedding_model: 'text-embedding-3-small',
      embedding_dimensions: 1536,
    })
    
  } catch (error) {
    console.error('[Reindex] Error:', error)
    return c.json({
      success: false,
      error: 'REINDEX_ERROR',
      message: error instanceof Error ? error.message : 'Unknown error',
    }, 500)
  }
})

// ============================================
// Freeze v1.1: Recommendation Mode API
// ============================================
// ì œì¶œ ì‹œì ì—ë§Œ ìµœì¢… ì¶”ì²œ íŒŒì´í”„ë¼ì¸ ì‹¤í–‰
// Interview Modeì™€ ë¶„ë¦¬í•˜ì—¬ ì„¤ë¬¸ ì¤‘ í¸í–¥ ë°©ì§€
// ============================================
import { generateQSP, qspToPromptHints } from './qsp-generator'
import { createEmptyAxisCoverage } from './axis-framework'
import { incrementalUpsertToVectorize, countJobsNeedingIndexing } from './vectorize-pipeline'

// Fallback ì‹¬ë¦¬ë¶„ì„ ìƒì„± (LLM ì‹¤íŒ¨ ì‹œ)
function generateFallbackWorkStyle(miniModule: {
  interest_top?: string[]
  value_top?: string[]
  strength_top?: string[]
  energy_drain_flags?: string[]
}): string {
  const parts: string[] = []

  const INTEREST_MAP: Record<string, string> = {
    problem_solving: 'ë¬¸ì œ í•´ê²°',
    data_numbers: 'ë°ì´í„°ì™€ ìˆ«ì',
    tech: 'ê¸°ìˆ /IT',
    creative: 'ì°½ì‘/ì˜ˆìˆ ',
    people: 'ì‚¬ëŒê³¼ ì†Œí†µ',
    helping: 'íƒ€ì¸ ë•ê¸°',
    business: 'ë¹„ì¦ˆë‹ˆìŠ¤',
    creating: 'ì°½ì‘ í™œë™',
    influencing: 'ì˜í–¥ë ¥ ë°œíœ˜',
  }

  const VALUE_MAP: Record<string, string> = {
    growth: 'ì„±ì¥',
    autonomy: 'ììœ¨ì„±',
    stability: 'ì•ˆì •ì„±',
    income: 'ë†’ì€ ìˆ˜ì…',
    meaning: 'ì˜ë¯¸ì™€ ì‚¬íšŒ ê¸°ì—¬',
    recognition: 'ì¸ì •ê³¼ ì˜í–¥ë ¥',
    wlb: 'ì¼ê³¼ ì‚¶ì˜ ê· í˜•',
  }

  const STRENGTH_MAP: Record<string, string> = {
    analytical: 'ë¶„ì„ë ¥',
    fast_learning: 'ë¹ ë¥¸ í•™ìŠµ ëŠ¥ë ¥',
    creativity: 'ì°½ì˜ì„±',
    communication: 'ì†Œí†µ ëŠ¥ë ¥',
    leadership: 'ë¦¬ë”ì‹­',
    empathy: 'ê³µê° ëŠ¥ë ¥',
  }

  if (miniModule.interest_top?.length) {
    const interests = miniModule.interest_top
      .map(i => INTEREST_MAP[i] || i)
      .slice(0, 2)
      .join('ê³¼ ')
    parts.push(`${interests}ì— ê´€ì‹¬ì´ ë§ìŠµë‹ˆë‹¤`)
  }

  if (miniModule.value_top?.length) {
    const values = miniModule.value_top
      .map(v => VALUE_MAP[v] || v)
      .slice(0, 2)
      .join('ê³¼ ')
    parts.push(`${values}${ì„ë¥¼(values)} ì¤‘ìš”í•˜ê²Œ ì—¬ê¹ë‹ˆë‹¤`)
  }

  if (miniModule.strength_top?.length) {
    const strengths = miniModule.strength_top
      .map(s => STRENGTH_MAP[s] || s)
      .slice(0, 2)
      .join(', ')
    parts.push(`${strengths}${ì´ê°€(strengths)} ê°•ì ì…ë‹ˆë‹¤`)
  }

  if (parts.length === 0) {
    return 'ë‹¹ì‹ ì˜ ì»¤ë¦¬ì–´ ì„±í–¥ì„ ë¶„ì„í•˜ê³  ìˆìŠµë‹ˆë‹¤.'
  }

  return `ë‹¹ì‹ ì€ ${parts.join('. ')}. ì´ëŸ¬í•œ íŠ¹ì„±ì„ ë°”íƒ•ìœ¼ë¡œ ì í•©í•œ ì§ì—…ì„ ì¶”ì²œí•´ ë“œë¦½ë‹ˆë‹¤.`
}

interface RecommendationModePayload {
  session_id: string
  // ìµœì¢… SearchProfile (Interview Modeì—ì„œ ìˆ˜ì§‘ëœ ëª¨ë“  ë°ì´í„°)
  searchProfile?: {
    desiredThemes: string[]
    dislikedThemes: string[]
    strengthsHypothesis: string[]
    environmentPreferences: string[]
    hardConstraints: string[]
    riskSignals: string[]
    keywords: string[]
  }
  // ë¯¸ë‹ˆëª¨ë“ˆ ê²°ê³¼ (MiniModule Hard Filterìš©)
  mini_module_result?: {
    interest_top: string[]
    value_top: string[]
    strength_top: string[]
    constraint_flags: string[]
    sacrifice_flags?: string[]
    energy_drain_flags?: string[]
    achievement_feedback_top?: string[]
    execution_style?: string
    impact_scope?: string
    failure_response?: string
    persistence_anchor?: string
    external_expectation?: string
  }
  // ì˜µì…˜: ê¸°ì¡´ draft_idë¡œ ìë™ í”„ë¡œí•„ ë¹Œë“œ
  draft_id?: number
  // ì˜µì…˜
  topK?: number  // ê¸°ë³¸: 800
  judgeTopN?: number  // ê¸°ë³¸: 20
  debug?: boolean
}

analyzerRoutes.post('/v3/recommend', async (c) => {
  const env = c.env as Bindings
  const db = env.DB
  const openaiApiKey = c.env.OPENAI_API_KEY
  const payload = await c.req.json<RecommendationModePayload>()

  // ì¸ì¦ëœ ì‚¬ìš©ì ì •ë³´ ê°€ì ¸ì˜¤ê¸° (ìˆìœ¼ë©´)
  const authUser = c.get('user') as { id: number } | undefined
  const userId = authUser?.id || null

  const { session_id, draft_id, topK = 800, judgeTopN = 20, debug = false } = payload
  
  if (!session_id) {
    return c.json(createErrorResponse(
      'VALIDATION_ERROR',
      'session_id is required'
    ), 400)
  }
  
  if (!openaiApiKey) {
    return c.json(createErrorResponse(
      'INTERNAL_ERROR',
      'OpenAI API key not configured'
    ), 500)
  }
  
  try {
    const startTime = Date.now()
    
    // 1. SearchProfile í™•ì •
    let searchProfile = payload.searchProfile
    
    if (!searchProfile && draft_id) {
      // Draftì—ì„œ í”„ë¡œí•„ ë¹Œë“œ
      const draft = await db.prepare(`
        SELECT * FROM ai_analysis_drafts WHERE id = ?
      `).bind(draft_id).first<AggDraftData>()
      
      if (draft) {
        const aggregated = buildAggregatedProfile(draft)
        // AggregatedProfile â†’ SearchProfile ë³€í™˜
        searchProfile = {
          desiredThemes: [
            ...aggregated.anchors.interest_top,
            ...aggregated.anchors.value_top,
          ],
          dislikedThemes: aggregated.universals.dislikes || [],
          strengthsHypothesis: aggregated.anchors.strength_top,
          environmentPreferences: [],
          hardConstraints: aggregated.anchors.constraint_flags,
          riskSignals: [],
          keywords: [
            ...aggregated.anchors.interest_top,
            ...aggregated.anchors.strength_top,
            ...Object.values(aggregated.universals.interests || []),
          ],
        }
      }
    }
    
    // Fallback: mini_module_resultì—ì„œ SearchProfile ìë™ ë¹Œë“œ
    if (!searchProfile && payload.mini_module_result) {
      searchProfile = buildSearchProfileFromMiniModule(payload.mini_module_result)
      console.log('[Recommendation Mode] Built searchProfile from mini_module_result (fallback)')
    }

    // Fallback 2: ì„¸ì…˜ DBì—ì„œ ë¯¸ë‹ˆëª¨ë“ˆ ë°ì´í„° ë³µì›
    if (!searchProfile) {
      try {
        const analysisResult = await db.prepare(`
          SELECT result_json FROM ai_analysis_results
          WHERE request_id IN (
            SELECT id FROM ai_analysis_requests WHERE session_id = ? ORDER BY id DESC LIMIT 1
          )
        `).bind(session_id).first<{ result_json: string }>()

        if (analysisResult?.result_json) {
          const parsed = JSON.parse(analysisResult.result_json)
          // ê¸°ì¡´ ë¶„ì„ ê²°ê³¼ì˜ input_summaryì—ì„œ í”„ë¡œí•„ ë³µì›
          const mm = parsed.mini_module_result || (parsed as any).miniModuleResult
          if (mm) {
            searchProfile = buildSearchProfileFromMiniModule(mm)
            if (!payload.mini_module_result) {
              ;(payload as any).mini_module_result = mm
            }
            console.log('[Recommendation Mode] Restored searchProfile from session DB (fallback 2)')
          }
        }
      } catch (err) {
        console.warn('[Recommendation Mode] Session DB fallback failed:', err)
      }
    }

    if (!searchProfile) {
      return c.json(createErrorResponse(
        'VALIDATION_ERROR',
        'searchProfile or draft_id or mini_module_result required'
      ), 400)
    }
    
    // 2. Vectorize 1íšŒ (TopK=800)
    console.log(`[Recommendation Mode] Starting with topK=${topK}`)

    // â˜… ì¤‘ìš”: mini_module_resultê°€ ìˆìœ¼ë©´ ì´ë¥¼ ê¸°ë°˜ìœ¼ë¡œ SearchProfile ìƒì„±
    // ì´ë ‡ê²Œ í•´ì•¼ "ë¶„ì„í˜• ìœ ì €"ì—ê²Œ ë¶„ì„ ê´€ë ¨ ì§ì—…ì´ ê²€ìƒ‰ë¨
    let vectorSearchProfile = searchProfile
    if (payload.mini_module_result) {
      vectorSearchProfile = buildSearchProfileFromMiniModule(payload.mini_module_result)
      console.log(`[Recommendation Mode] Built SearchProfile from MiniModule:`, {
        desiredThemes: vectorSearchProfile.desiredThemes.slice(0, 3),
        strengthsHypothesis: vectorSearchProfile.strengthsHypothesis,
        keywords: vectorSearchProfile.keywords.slice(0, 5),
      })
    }

    const expansionResult = await expandCandidatesV3(
      db,
      env.VECTORIZE,
      openaiApiKey,
      vectorSearchProfile,
      {
        targetSize: topK,
        miniModule: payload.mini_module_result,
      }
    )

    console.log(`[Recommendation Mode] Vectorize returned ${expansionResult.candidates.length} candidates`)
    
    // 3. TAG Hard Filter
    const userConstraints = extractUserConstraints(
      searchProfile.hardConstraints.reduce((acc, c) => {
        if (c.includes('time') || c.includes('ì‹œê°„')) acc.work_hours_strict = true
        if (c.includes('remote') || c.includes('ì›ê²©')) acc.remote_only = true
        if (c.includes('shift') || c.includes('êµëŒ€')) acc.shift_work_no = true
        if (c.includes('degree') || c.includes('í•™ìœ„')) acc.degree_impossible = true
        if (c.includes('license') || c.includes('ìê²©')) acc.license_impossible = true
        return acc
      }, {} as { [key: string]: boolean }),
      {}
    )
    
    // 3-1. TAG Filter ë¨¼ì € ì ìš© (VectorSearchResult íƒ€ì…)
    console.log('[Recommendation Mode] Step 3-1: Applying TAG filter...')
    console.log('[Recommendation Mode] candidates count:', expansionResult.candidates.length)

    let tagFilterResult
    try {
      tagFilterResult = await applyTagFilter(db, expansionResult.candidates, userConstraints)
      console.log(`[Recommendation Mode] After TAG filter: ${tagFilterResult.passed.length} jobs (excluded: ${tagFilterResult.excluded.length})`)
    } catch (tagError) {
      console.error('[Recommendation Mode] TAG filter error:', tagError)
      throw tagError
    }

    // 3-2. í•„í„°ë§ëœ ê²°ê³¼ë¥¼ ScoredJobìœ¼ë¡œ ë³€í™˜
    console.log('[Recommendation Mode] Step 3-2: Converting to ScoredJobs...')
    const tagPassedAsVectorResults = tagFilterResult.passed.map(p => ({
      job_id: p.job_id,
      job_name: p.job_name,
      score: p.score,  // VectorSearchResultì˜ score
    }))
    console.log('[Recommendation Mode] tagPassedAsVectorResults count:', tagPassedAsVectorResults.length)

    let scoredJobs
    try {
      scoredJobs = await vectorResultsToScoredJobs(db, tagPassedAsVectorResults, payload.mini_module_result)
      console.log('[Recommendation Mode] scoredJobs count:', scoredJobs.length)
      // ğŸ” DEBUG: ì²« ë²ˆì§¸ ì§ì—…ì˜ image_url, job_description í™•ì¸
      if (scoredJobs.length > 0) {
        const sample = scoredJobs[0]
        console.log('[Recommendation Mode] ğŸ” Sample scoredJob:', {
          job_id: sample.job_id,
          job_name: sample.job_name,
          slug: sample.slug,
          image_url: sample.image_url,
          job_description: sample.job_description?.substring(0, 50),
        })
      }
    } catch (scoreError) {
      console.error('[Recommendation Mode] vectorResultsToScoredJobs error:', scoreError)
      throw scoreError
    }

    // 3-3. MiniModule Hard Filter (ë¶„ì„í˜• ê°•ì  â†’ í˜„ì¥ì§ ì œì™¸ ë“±)
    console.log('[Recommendation Mode] Step 3-3: Applying MiniModule filter...')
    console.log('[Recommendation Mode] mini_module_result:', JSON.stringify(payload.mini_module_result))

    let filteredJobs, mmFilterResult
    try {
      const mmResult = applyMiniModuleHardFilter(scoredJobs, payload.mini_module_result)
      filteredJobs = mmResult.filtered
      mmFilterResult = mmResult.filterResult
      console.log(`[Recommendation Mode] After MiniModule filter: ${filteredJobs.length} jobs (excluded: ${mmFilterResult.excludedJobIds.length})`)
    } catch (mmError) {
      console.error('[Recommendation Mode] MiniModule filter error:', mmError)
      throw mmError
    }

    if (mmFilterResult.appliedRules.length > 0) {
      console.log(`[Recommendation Mode] Applied rules: ${mmFilterResult.appliedRules.map(r => `${r.ruleId}(${r.excludedCount})`).join(', ')}`)
    }
    
    // 4. LLM Judge í˜¸ì¶œ (Top 20 ê²°ì • + rationale ìƒì„±)
    let topJobs: any[]

    // ì‚¬ì „ í•„í„°ëœ ìƒìœ„ í›„ë³´êµ°
    const preFilteredJobs = filteredJobs
      .sort((a, b) => (b.final_score || b.like_score || 0) - (a.final_score || a.like_score || 0))
      .slice(0, Math.min(judgeTopN * 2, 40))

    // ScoredJobì„ FilteredCandidate í˜•íƒœë¡œ ë³€í™˜
    const candidatesForJudge: FilteredCandidate[] = preFilteredJobs.map(job => ({
      job_id: job.job_id,
      job_name: job.job_name,
      score: job.final_score || job.like_score || 0,
      riskPenalty: job.risk_penalty || 0,
      riskWarnings: [],
      tagSource: 'tagged' as const,
    }))

    // ê°„ë‹¨í•œ rationale ìƒì„± í•¨ìˆ˜ (LLM Judge ì—†ì„ ë•Œ ì‚¬ìš©)
    const generateSimpleRationale = (job: any, mm: any) => {
      const parts: string[] = []
      const interestMap: Record<string, string> = {
        problem_solving: 'ë¬¸ì œí•´ê²°', data_numbers: 'ë°ì´í„°/ìˆ«ì', tech: 'ê¸°ìˆ /IT',
        creative: 'ì°½ì‘/ì˜ˆìˆ ', helping: 'ë„ì›€/ê°€ë¥´ì¹¨', organizing: 'ì¡°ì§/ê´€ë¦¬'
      }
      const strengthMap: Record<string, string> = {
        analytical: 'ë¶„ì„ë ¥', fast_learning: 'ë¹ ë¥¸ í•™ìŠµ', communication: 'ì†Œí†µ',
        persistence: 'ëˆê¸°', creativity: 'ì°½ì˜ì„±', leadership: 'ë¦¬ë”ì‹­'
      }

      if (mm?.interest_top?.length > 0) {
        const interests = mm.interest_top.slice(0, 2).map((i: string) => interestMap[i] || i).join(', ')
        parts.push(`${interests} ë¶„ì•¼ì— ëŒ€í•œ ê´€ì‹¬`)
      }
      if (mm?.strength_top?.length > 0) {
        const strengths = mm.strength_top.slice(0, 2).map((s: string) => strengthMap[s] || s).join(', ')
        parts.push(`${strengths} ê°•ì  í™œìš©`)
      }

      if (parts.length > 0) {
        return `ë‹¹ì‹ ì˜ ${parts.join('ê³¼ ')}ì— ì í•©í•œ ì§ì—…ì…ë‹ˆë‹¤.`
      }
      return null
    }

    if (openaiApiKey && candidatesForJudge.length > 0) {
      try {
        console.log(`[Recommendation Mode] Calling LLM Judge (OpenAI) for ${candidatesForJudge.length} candidates...`)

        const judgeInput: JudgeInput = {
          candidates: candidatesForJudge,
          searchProfile,
          miniModuleResult: payload.mini_module_result,
        }

        const judgeResults = await judgeCandidates(openaiApiKey, db, judgeInput)

        // Judge ê²°ê³¼ë¥¼ topJobsì— ë§¤í•‘ (rationale + likeReason/canReason í¬í•¨)
        // ì ìˆ˜ ë§¤í•‘ ì •ì •:
        //   desireScore (í¥ë¯¸/ê°€ì¹˜ ë§¤ì¹­) â†’ like_score (ì¢‹ì•„í•  ê°€ëŠ¥ì„±)
        //   fitScore (ê°•ì /ì—­ëŸ‰ ë§¤ì¹­) â†’ can_score (ì˜í•  ê°€ëŠ¥ì„±)
        //   overallScore (ì¢…í•©) â†’ fit_score (ì¢…í•© ì í•©ë„)
        //   feasibilityScore (ì§„ì…ì¥ë²½/í˜„ì‹¤ì„±) â†’ fit_score ê³„ì‚°ì— ì´ë¯¸ ë°˜ì˜ë¨
        topJobs = judgeResults.results.slice(0, judgeTopN).map(result => {
          const originalJob = preFilteredJobs.find(j => j.job_id === result.job_id)
          return {
            ...originalJob,
            like_score: result.desireScore,       // Like = í¥ë¯¸/ê°€ì¹˜ ë§¤ì¹­
            can_score: result.fitScore,            // Can = ê°•ì /ì—­ëŸ‰ ë§¤ì¹­
            fit_score: result.overallScore,        // Fit = ì¢…í•© (LikeÃ—0.35 + CanÃ—0.45 + FeasibilityÃ—0.20 - Risk)
            final_score: result.overallScore,
            risk_penalty: result.riskPenalty,
            feasibility_score: result.feasibilityScore,  // í˜„ì‹¤ì„± ì ìˆ˜ (ì°¸ê³ ìš©)
            rationale: result.rationale,
            like_reason: result.likeReason,   // ì¢‹ì•„í•  ì´ìœ 
            can_reason: result.canReason,     // ì˜í•  ì´ìœ 
            evidence_quotes: result.evidenceQuotes,
          }
        })

        console.log(`[Recommendation Mode] LLM Judge completed: ${topJobs.length} jobs ranked`)
        // ğŸ” DEBUG: topJobsì— image_url, job_descriptionì´ ìˆëŠ”ì§€ í™•ì¸
        if (topJobs.length > 0) {
          const sample = topJobs[0]
          console.log('[Recommendation Mode] ğŸ” Sample topJob after Judge:', {
            job_id: sample.job_id,
            job_name: sample.job_name,
            slug: sample.slug,
            image_url: sample.image_url,
            job_description: sample.job_description?.substring(0, 50),
            rationale: sample.rationale?.substring(0, 50),
          })
        }
      } catch (judgeError) {
        // LLM Judge ì‹¤íŒ¨ ì‹œ ì—ëŸ¬ ë°˜í™˜ (fallback ì—†ìŒ)
        console.error('[Recommendation Mode] âŒ LLM Judge failed:', judgeError)
        const errorMessage = judgeError instanceof Error ? judgeError.message : String(judgeError)
        return c.json(createErrorResponse(
          'LLM_JUDGE_FAILED',
          `LLM ë¶„ì„ì´ ì‹¤íŒ¨í–ˆìŠµë‹ˆë‹¤: ${errorMessage}`
        ), 500)
      }
    } else {
      // API í‚¤ê°€ ì—†ìœ¼ë©´ ì—ëŸ¬ (ìœ„ì—ì„œ ì´ë¯¸ ì²´í¬í•˜ë¯€ë¡œ ì—¬ê¸° ë„ë‹¬í•˜ë©´ ì•ˆë¨)
      console.error('[Recommendation Mode] âŒ OpenAI API key not available!')
      return c.json(createErrorResponse(
        'INTERNAL_ERROR',
        'OpenAI API í‚¤ê°€ ì„¤ì •ë˜ì§€ ì•Šì•˜ìŠµë‹ˆë‹¤.'
      ), 500)
    }

    // ============================================
    // 5. LLM Reporter: ì‹¬ë¦¬ë¶„ì„ ë¦¬í¬íŠ¸ ìƒì„±
    // ============================================
    let premiumReport: any = null

    // 5-1. NarrativeFacts ì¡°íšŒ
    let narrativeFacts: { highAliveMoment: string; lostMoment: string } | undefined
    try {
      const narrativeRow = await db.prepare(`
        SELECT high_alive_moment, lost_moment
        FROM narrative_facts
        WHERE session_id = ?
      `).bind(session_id).first<{
        high_alive_moment: string | null
        lost_moment: string | null
      }>()

      if (narrativeRow && (narrativeRow.high_alive_moment || narrativeRow.lost_moment)) {
        narrativeFacts = {
          highAliveMoment: narrativeRow.high_alive_moment || '',
          lostMoment: narrativeRow.lost_moment || '',
        }
        console.log('[Recommendation Mode] NarrativeFacts loaded')
      }
    } catch (narrativeError) {
      console.warn('[Recommendation Mode] Failed to load narrative facts:', narrativeError)
    }

    // 5-2. RoundAnswers ì¡°íšŒ
    let roundAnswers: Array<{ roundNumber: 1 | 2 | 3; questionId: string; answer: string }> = []
    try {
      const roundAnswersRows = await db.prepare(`
        SELECT round_number, question_id, answer
        FROM round_answers
        WHERE session_id = ?
        ORDER BY round_number, question_id
      `).bind(session_id).all<{
        round_number: number
        question_id: string
        answer: string
      }>()

      if (roundAnswersRows.results && roundAnswersRows.results.length > 0) {
        roundAnswers = roundAnswersRows.results.map(row => ({
          roundNumber: row.round_number as 1 | 2 | 3,
          questionId: row.question_id,
          answer: row.answer,
        }))
        console.log(`[Recommendation Mode] Loaded ${roundAnswers.length} round answers`)
      }
    } catch (roundError) {
      console.warn('[Recommendation Mode] Failed to load round answers:', roundError)
    }

    // 5-3. LLM Reporter í˜¸ì¶œ
    let reportMode: 'llm' | 'fallback' | 'none' = 'none'
    if (openaiApiKey && topJobs.length > 0) {
      try {
        console.log('[Recommendation Mode] â˜…â˜…â˜… Generating premium report with GPT-4o-mini... â˜…â˜…â˜…')

        // excluded jobs ëª©ë¡ (Hard Cut)
        const hardCutList = mmFilterResult.excludedJobIds.map((jobId, idx) => ({
          job_id: jobId,
          job_name: `Excluded Job ${idx + 1}`,
          rule_id: mmFilterResult.appliedRules[0]?.ruleId || 'mini_module_filter',
          reason: mmFilterResult.appliedRules[0]?.description || 'ë¯¸ë‹ˆëª¨ë“ˆ ê¸°ë°˜ ì œì™¸',
        })).slice(0, 10)

        const reporterInput: ReporterInput = {
          sessionId: session_id,
          judgeResults: topJobs.map(job => ({
            job_id: job.job_id,
            job_name: job.job_name,
            slug: job.slug || '',
            image_url: job.image_url || undefined,
            like_score: job.like_score || 50,
            can_score: job.can_score || 50,
            fit_verdict: job.like_score && job.like_score >= 70 ? 'STRONG_FIT' : 'MODERATE_FIT',
            fit_summary: job.job_description || `${job.job_name}${ì€ëŠ”(job.job_name)} ë‹¹ì‹ ì˜ ê´€ì‹¬ì‚¬ì™€ ì˜ ë§ìŠµë‹ˆë‹¤.`,
            risk_note: job.risk_penalty && job.risk_penalty > 0 ? 'ì¼ë¶€ ì œì•½ ì¡°ê±´ ê³ ë ¤ í•„ìš”' : undefined,
          })),
          searchProfile,
          narrativeFacts,
          roundAnswers,
          universalAnswers: {},
          hardCutList,
          miniModuleResult: payload.mini_module_result,
        }

        premiumReport = await generateLLMPremiumReport(
          env?.AI || null,
          reporterInput,
          openaiApiKey
        )
        // LLM ë¦¬í¬íŠ¸ì¸ì§€ fallbackì¸ì§€ í™•ì¸ (metaCognition._meta.generated_byë¡œ íŒë‹¨)
        reportMode = premiumReport?.metaCognition?._meta?.generated_by === 'rule' ? 'fallback' : 'llm'
        console.log(`[Recommendation Mode] Premium report generated - mode: ${reportMode}`)
      } catch (reporterError) {
        console.error('[Recommendation Mode] LLM Reporter failed:', reporterError)
        reportMode = 'fallback'
        // Fallback: ê¸°ë³¸ ì‹¬ë¦¬ ë¶„ì„ ì œê³µ
        const mm = payload.mini_module_result
        const toKo = (t: string) => TOKEN_TO_KOREAN[t] || t
        premiumReport = fixParticlesDeep({
          workStyleNarrative: mm ?
            generateFallbackWorkStyle(mm) : null,
          lifeVersionStatement: {
            oneLiner: 'ë‹¹ì‹ ì˜ ì„ íƒì„ ê¸°ë°˜ìœ¼ë¡œ ë¶„ì„ ì¤‘ì…ë‹ˆë‹¤.',
            expanded: [],
          },
          // profileInterpretation fallback
          profileInterpretation: mm ? {
            interests: (mm.interest_top || []).map((t: string) => ({
              token: t,
              label: toKo(t),
              meaning: `${toKo(t)}ì— ê´€ì‹¬ì´ ìˆìŠµë‹ˆë‹¤.`
            })),
            interests_summary: (mm.interest_top || []).length ? `ë‹¹ì‹ ì€ ${(mm.interest_top || []).map(toKo).join('ê³¼(ì™€) ')}ì— ê´€ì‹¬ì´ ìˆìŠµë‹ˆë‹¤.` : '',
            strengths: (mm.strength_top || []).map((t: string) => ({
              token: t,
              label: toKo(t),
              meaning: `${toKo(t)}ì´(ê°€) ê°•ì ì…ë‹ˆë‹¤.`
            })),
            strengths_summary: (mm.strength_top || []).length ? `ë‹¹ì‹ ì€ ${(mm.strength_top || []).map(toKo).join('ê³¼(ì™€) ')}ì— ê°•ì ì„ ê°€ì§€ê³  ìˆìŠµë‹ˆë‹¤.` : '',
            values: (mm.value_top || []).map((t: string) => ({
              token: t,
              label: toKo(t),
              meaning: `${toKo(t)}ì„(ë¥¼) ì¤‘ìš”í•˜ê²Œ ìƒê°í•©ë‹ˆë‹¤.`
            })),
            values_summary: (mm.value_top || []).length ? `ë‹¹ì‹ ì—ê²Œ ${(mm.value_top || []).map(toKo).join('ê³¼(ì™€) ')}ì€(ëŠ”) ì¤‘ìš”í•œ ê°€ì¹˜ì…ë‹ˆë‹¤.` : '',
            constraints: (mm.constraint_flags || []).map((t: string) => ({
              token: t,
              label: toKo(t),
              meaning: `${toKo(t)}ì„(ë¥¼) í”¼í•˜ê³  ì‹¶ì–´í•©ë‹ˆë‹¤.`
            })),
            constraints_summary: (() => { const flags = (mm.constraint_flags || []).map(toKo); if (!flags.length) return ''; return `ë‹¹ì‹ ì€ ${flags.join('ê³¼(ì™€) ')}ì„(ë¥¼) í”¼í•˜ê³  ì‹¶ì–´í•©ë‹ˆë‹¤.` })(),
            overall_profile: 'í”„ë¡œí•„ ë¶„ì„ì„ ìœ„í•´ ë” ë§ì€ ì •ë³´ê°€ í•„ìš”í•©ë‹ˆë‹¤.'
          } : undefined,
        })
      }
    }

    // 6. ê²°ê³¼ ë°˜í™˜
    const duration = Date.now() - startTime

    // like_top10: like_score ê¸°ì¤€ ì •ë ¬ (Fit >= 25 í•„í„°)
    const likeTop10 = [...topJobs]
      .filter(job => (job.final_score || job.fit_score || 0) >= 25)
      .sort((a, b) => (b.like_score || 0) - (a.like_score || 0))
      .slice(0, 10)
      .map(job => ({
        job_id: job.job_id,
        job_name: job.job_name,
        job_description: job.job_description || null,
        slug: job.slug,
        image_url: job.image_url || null,
        fit_score: job.final_score || job.fit_score || 50,
        like_score: job.like_score || 50,
        can_score: job.can_score || 50,
        rationale: job.rationale || null,
        like_reason: job.like_reason || null,
        can_reason: job.can_reason || null,
      }))

    // can_top10: can_score ê¸°ì¤€ ì •ë ¬ (Fit >= 25 í•„í„°)
    const canTop10 = [...topJobs]
      .filter(job => (job.final_score || job.fit_score || 0) >= 25)
      .sort((a, b) => (b.can_score || 0) - (a.can_score || 0))
      .slice(0, 10)
      .map(job => ({
        job_id: job.job_id,
        job_name: job.job_name,
        job_description: job.job_description || null,
        slug: job.slug,
        image_url: job.image_url || null,
        fit_score: job.final_score || job.fit_score || 50,
        like_score: job.like_score || 50,
        can_score: job.can_score || 50,
        rationale: job.rationale || null,
        like_reason: job.like_reason || null,
        can_reason: job.can_reason || null,
      }))

    // ============================================
    // 7. ê²°ê³¼ ì €ì¥ (ai_analysis_requests + ai_analysis_results)
    // ============================================
    const resultToSave = {
      engine_version: 'v3',
      mini_module_result: payload.mini_module_result || null,
      fit_top3: topJobs.slice(0, 10).map(job => ({
        job_id: job.job_id,
        job_name: job.job_name,
        job_description: job.job_description || null,
        slug: job.slug,
        image_url: job.image_url || null,
        fit_score: job.final_score || job.fit_score || 50,
        like_score: job.like_score || 50,
        can_score: job.can_score || 50,
        rationale: job.rationale || null,
        like_reason: job.like_reason || null,
        can_reason: job.can_reason || null,
      })),
      like_top10: likeTop10,
      can_top10: canTop10,
      premium_report: premiumReport,
      search_profile: searchProfile,
      total_candidates: expansionResult.candidates.length,
      filtered_count: filteredJobs.length,
    }

    let savedRequestId: number | null = null
    try {
      // 1. ë¨¼ì € ai_analysis_requestsì— ë ˆì½”ë“œ ìƒì„± (ë˜ëŠ” ê¸°ì¡´ ê²ƒ ì¡°íšŒ)
      // user_id: ì¸ì¦ëœ ì‚¬ìš©ì ë˜ëŠ” draftì—ì„œ ê°€ì ¸ì˜´
      let effectiveUserId = userId

      // userIdê°€ ì—†ìœ¼ë©´ draftì—ì„œ user_id ì¡°íšŒ
      if (!effectiveUserId) {
        const draftOwner = await db.prepare(`
          SELECT user_id FROM analyzer_drafts WHERE session_id = ?
        `).bind(session_id).first<{ user_id: number | null }>()
        effectiveUserId = draftOwner?.user_id || null
      }

      // ê¸°ì¡´ request í™•ì¸
      const existingRequest = await db.prepare(`
        SELECT id FROM ai_analysis_requests WHERE session_id = ?
      `).bind(session_id).first<{ id: number }>()

      if (existingRequest) {
        savedRequestId = existingRequest.id
        // ê¸°ì¡´ request ìƒíƒœ ì—…ë°ì´íŠ¸ (user_idê°€ ì—†ìœ¼ë©´ effectiveUserIdë¡œ ì„¤ì •)
        await db.prepare(`
          UPDATE ai_analysis_requests
          SET status = 'completed',
              processed_at = CURRENT_TIMESTAMP,
              user_id = COALESCE(user_id, ?)
          WHERE id = ?
        `).bind(effectiveUserId, savedRequestId).run()
      } else {
        // ìƒˆ request ìƒì„±
        const insertResult = await db.prepare(`
          INSERT INTO ai_analysis_requests (session_id, user_id, analysis_type, status, processed_at, created_at)
          VALUES (?, ?, 'job', 'completed', CURRENT_TIMESTAMP, CURRENT_TIMESTAMP)
        `).bind(session_id, effectiveUserId).run()
        savedRequestId = insertResult.meta?.last_row_id as number || null
      }

      if (savedRequestId) {
        // 2. ai_analysis_resultsì— ì €ì¥ (UPDATE ìš°ì„ , ì—†ìœ¼ë©´ INSERT)
        // NOTE: request_idì— UNIQUE ì œì•½ì´ ì—†ì–´ ON CONFLICT UPSERT ë¶ˆê°€ â†’ UPDATE+INSERT íŒ¨í„´ ì‚¬ìš©
        const resultJson = JSON.stringify(resultToSave)
        const premiumJson = JSON.stringify(premiumReport)

        const updateResult = await db.prepare(`
          UPDATE ai_analysis_results
          SET result_json = ?,
              engine_version = 'v3',
              premium_report_json = ?
          WHERE request_id = ?
        `).bind(resultJson, premiumJson, savedRequestId).run()

        if (!updateResult.meta?.changes || updateResult.meta.changes === 0) {
          // ê¸°ì¡´ í–‰ì´ ì—†ìœ¼ë©´ ìƒˆë¡œ INSERT
          await db.prepare(`
            INSERT INTO ai_analysis_results (request_id, result_json, engine_version, premium_report_json, created_at)
            VALUES (?, ?, 'v3', ?, CURRENT_TIMESTAMP)
          `).bind(savedRequestId, resultJson, premiumJson).run()
        }

        console.log(`[Recommendation Mode] âœ… Result saved - request_id: ${savedRequestId}, session: ${session_id}, updated: ${updateResult.meta?.changes || 0}`)
      }
    } catch (saveError) {
      // ì €ì¥ ì‹¤íŒ¨í•´ë„ ê²°ê³¼ëŠ” ë°˜í™˜ (ë¡œê·¸ë§Œ ë‚¨ê¹€)
      console.error('[Recommendation Mode] âš ï¸ Failed to save result:', saveError)
    }

    return c.json({
      success: true,
      mode: 'recommendation',
      session_id,
      request_id: savedRequestId,  // ê²°ê³¼ í˜ì´ì§€ ì´ë™ìš©
      recommendations: {
        top_jobs: topJobs.map(job => ({
          job_id: job.job_id,
          job_name: job.job_name,
          job_description: job.job_description || null,
          slug: job.slug,
          image_url: job.image_url || null,
          fit_score: job.final_score || job.fit_score || job.like_score || 50,
          like_score: job.like_score || 50,
          can_score: job.can_score || 50,
          risk_penalty: job.risk_penalty || 0,
          rationale: job.rationale || null,
          like_reason: job.like_reason || null,   // ì¢‹ì•„í•  ì´ìœ 
          can_reason: job.can_reason || null,     // ì˜í•  ì´ìœ 
          evidence_quotes: job.evidence_quotes || [],
        })),
        like_top10: likeTop10,
        can_top10: canTop10,
        total_candidates: expansionResult.candidates.length,
        filtered_count: filteredJobs.length,
        search_duration_ms: expansionResult.search_duration_ms,
      },
      premium_report: premiumReport,
      report_mode: reportMode,  // 'llm' | 'fallback' | 'none'
      search_profile_used: searchProfile,
      debug_info: debug ? {
        vectorize_topK: topK,
        judge_topN: judgeTopN,
        cache_hit: expansionResult.cacheHit,
        fallback_used: expansionResult.fallback_used,
        has_narrative_facts: !!narrativeFacts,
        round_answers_count: roundAnswers.length,
      } : undefined,
      duration_ms: duration,
    })
    
  } catch (error) {
    console.error('[Recommendation Mode] Error:', error)
    console.error('[Recommendation Mode] Stack:', error instanceof Error ? error.stack : 'No stack')
    return c.json(createErrorResponse(
      'ANALYSIS_FAILED',
      error instanceof Error ? `${error.message} | ${error.stack?.split('\n')[1] || ''}` : 'Recommendation failed'
    ), 500)
  }
})

// ============================================
// Freeze v1.1: Interview Mode - QSP ìƒì„± API
// ============================================
// ì„¤ë¬¸ ì¤‘ Vectorize ì„¼ì„œìš© QSP ìƒì„±
// ============================================
analyzerRoutes.post('/v3/interview/qsp', async (c) => {
  const env = c.env as Bindings
  const db = env.DB
  const openaiApiKey = c.env.OPENAI_API_KEY
  
  const payload = await c.req.json<{
    session_id: string
    round?: 1 | 2 | 3
    draft_id?: number
  }>()
  
  const { session_id, round = 1, draft_id } = payload
  
  if (!session_id) {
    return c.json(createErrorResponse('VALIDATION_ERROR', 'session_id required'), 400)
  }
  
  if (!openaiApiKey) {
    return c.json(createErrorResponse('INTERNAL_ERROR', 'OpenAI API key not configured'), 500)
  }
  
  try {
    // Draftì—ì„œ í”„ë¡œí•„ ë¹Œë“œ (ìˆìœ¼ë©´)
    let axisCoverage = createEmptyAxisCoverage()
    
    if (draft_id) {
      const draft = await db.prepare(`
        SELECT * FROM ai_analysis_drafts WHERE id = ?
      `).bind(draft_id).first<AggDraftData>()
      
      if (draft) {
        const aggregated = buildAggregatedProfile(draft)
        // TODO: AggregatedProfileì—ì„œ AxisCoverage ì¶”ì¶œ
        // í˜„ì¬ëŠ” ë¹ˆ ì»¤ë²„ë¦¬ì§€ ì‚¬ìš©
      }
    }
    
    // Vectorize ì„¼ì„œ ì¿¼ë¦¬ (topK=500, ì§ì—…ëª… ë¹„ë…¸ì¶œ)
    const expansionResult = await expandCandidatesV3WithCache(
      db,
      env.VECTORIZE,
      openaiApiKey,
      {
        narrativeFacts: undefined,
        roundAnswers: [],
        universalAnswers: {},
      },
      {
        sessionId: session_id,
        targetSize: 500,  // ì„¼ì„œìš©
      }
    )
    
    // QSP ìƒì„±
    const qsp = generateQSP({
      vectorResults: expansionResult.candidates,
      axisCoverage,
      round,
    })
    
    return c.json({
      success: true,
      qsp,
      prompt_hints: qspToPromptHints(qsp),
      vectorize_stats: {
        total_candidates: expansionResult.candidates.length,
        cache_hit: expansionResult.cacheHit,
      },
    })
    
  } catch (error) {
    console.error('[Interview QSP] Error:', error)
    return c.json(createErrorResponse(
      'ANALYSIS_FAILED',
      error instanceof Error ? error.message : 'QSP generation failed'
    ), 500)
  }
})

// ============================================
// Freeze v1.1: ì¸ë±ì‹± ìƒíƒœ í™•ì¸ API
// ============================================
analyzerRoutes.get('/admin/indexing-status', async (c) => {
  const env = c.env as Bindings
  const db = env.DB
  
  try {
    const status = await countJobsNeedingIndexing(db)
    
    return c.json({
      success: true,
      ...status,
      current_version: `JPC_${JOB_PROFILE_COMPACT_VERSION}`,
      percentage_indexed: status.total > 0 
        ? Math.round((status.upToDate / status.total) * 100) 
        : 0,
    })
    
  } catch (error) {
    console.error('[Indexing Status] Error:', error)
    return c.json({
      success: false,
      error: error instanceof Error ? error.message : 'Unknown error',
    }, 500)
  }
})

// ============================================
// Freeze v1.1: ì¦ë¶„ ì—…ì„œíŠ¸ API
// ============================================
analyzerRoutes.post('/admin/incremental-upsert', async (c) => {
  const env = c.env as Bindings
  const db = env.DB
  const openaiApiKey = c.env.OPENAI_API_KEY
  
  if (!env.VECTORIZE) {
    return c.json({ success: false, error: 'VECTORIZE_NOT_AVAILABLE' }, 503)
  }
  
  if (!openaiApiKey) {
    return c.json({ success: false, error: 'OPENAI_API_KEY_NOT_SET' }, 503)
  }
  
  const payload = await c.req.json<{ max_jobs?: number }>().catch(() => ({}))
  const maxJobs = payload.max_jobs || 100
  
  try {
    const result = await incrementalUpsertToVectorize(
      db,
      env.VECTORIZE,
      openaiApiKey,
      { maxJobs }
    )
    
    return c.json({
      success: true,
      ...result,
    })
    
  } catch (error) {
    console.error('[Incremental Upsert] Error:', error)
    return c.json({
      success: false,
      error: error instanceof Error ? error.message : 'Unknown error',
    }, 500)
  }
})

import { JOB_PROFILE_COMPACT_VERSION } from '../../constants/embedding-versions'

// ============================================
// ìë™í™” ì‹œë‚˜ë¦¬ì˜¤ í…ŒìŠ¤íŠ¸ API
// P0/P1/P2/P3 ì „ì²´ ê¸°ëŠ¥ ìë™ ê²€ì¦
// ============================================

// ì‹œë‚˜ë¦¬ì˜¤ ëª©ë¡ ì¡°íšŒ
analyzerRoutes.get('/test/scenarios', async (c) => {
  const scenarios = getAllScenarioSummary()
  return c.json({
    success: true,
    scenarios,
    total: scenarios.length,
  })
})

// íŠ¹ì • ì‹œë‚˜ë¦¬ì˜¤ ì‹¤í–‰
analyzerRoutes.post('/test/run-scenario', async (c) => {
  const env = c.env as Bindings
  const db = env.DB

  const payload = await c.req.json<{ scenario_id: string }>().catch(() => ({ scenario_id: '' }))
  const { scenario_id } = payload

  if (!scenario_id) {
    return c.json({ success: false, error: 'scenario_id required' }, 400)
  }

  const scenario = getScenarioById(scenario_id)
  if (!scenario) {
    return c.json({ success: false, error: `Scenario not found: ${scenario_id}` }, 404)
  }

  console.log(`[Test Scenario] Starting: ${scenario.name} (${scenario.id})`)

  try {
    // 1. í…ŒìŠ¤íŠ¸ìš© ì„¸ì…˜ ID ìƒì„±
    const testSessionId = `test_${scenario_id}_${Date.now()}`

    // 2. Mini Module ê¸°ë°˜ ë¶„ì„ ì‹¤í–‰ (V3 Analyze ë¡œì§ ì¬ì‚¬ìš©)
    const miniModule = scenario.miniModule

    // 3. Facts ìƒì„± (Can ê²€ì¦ ë‹µë³€ ê¸°ë°˜)
    const facts: Array<{ fact_key: string; value_json: string }> = []

    // Can ê²€ì¦ ë‹µë³€ì„ factsë¡œ ë³€í™˜
    for (const [questionId, answer] of Object.entries(scenario.canValidationAnswers)) {
      const canToken = questionId.replace('can_', '')
      const canBoostMap: Record<string, number> = {
        'work_experience_3plus': 25,
        'work_experience_1to3': 18,
        'project_experience': 12,
        'study_only': 5,
        'none': 0,
      }
      const boost = canBoostMap[answer] || 5
      facts.push({
        fact_key: `can_verified.${canToken}`,
        value_json: JSON.stringify({ boost, answer }),
      })
    }

    // Risk ê°•ë„ ë‹µë³€ì„ factsë¡œ ë³€í™˜
    for (const [questionId, intensity] of Object.entries(scenario.constraintIntensityAnswers)) {
      facts.push({
        fact_key: `constraint.${questionId.replace('intensity_', '')}.intensity`,
        value_json: JSON.stringify(intensity),
      })
    }

    // 4. ì§ì—… ë°ì´í„° ì¡°íšŒ (jobs í…Œì´ë¸”ê³¼ ì¡°ì¸)
    const jobAttrs = await db.prepare(`
      SELECT
        ja.job_id,
        j.name as job_name,
        j.slug,
        j.image_url,
        j.api_data_json,
        j.merged_profile_json,
        COALESCE(ja.wlb, 50) as wlb,
        COALESCE(ja.growth, 50) as growth,
        COALESCE(ja.stability, 50) as stability,
        COALESCE(ja.income, 50) as income,
        COALESCE(ja.teamwork, 50) as teamwork,
        COALESCE(ja.solo_deep, 50) as solo_deep,
        COALESCE(ja.analytical, 50) as analytical,
        COALESCE(ja.creative, 50) as creative,
        COALESCE(ja.execution, 50) as execution,
        COALESCE(ja.people_facing, 50) as people_facing,
        COALESCE(ja.work_hours, 'normal') as work_hours,
        COALESCE(ja.shift_work, 'rare') as shift_work,
        COALESCE(ja.travel, 'rare') as travel,
        COALESCE(ja.remote_possible, 'possible') as remote_possible,
        COALESCE(ja.degree_required, 'none') as degree_required,
        COALESCE(ja.license_required, 'none') as license_required
      FROM job_attributes ja
      INNER JOIN jobs j ON ja.job_id = j.id
      LIMIT 200
    `).all<{
      job_id: string
      job_name: string
      slug: string | null
      image_url: string | null
      api_data_json: string | null
      merged_profile_json: string | null
      wlb: number
      growth: number
      stability: number
      income: number
      teamwork: number
      solo_deep: number
      analytical: number
      creative: number
      execution: number
      people_facing: number
      work_hours: string
      shift_work: string
      travel: string
      remote_possible: string
      degree_required: string
      license_required: string
    }>()

    if (!jobAttrs.results || jobAttrs.results.length === 0) {
      return c.json({ success: false, error: 'No job data available' }, 500)
    }

    // 5. ì ìˆ˜ ê³„ì‚° (ì „ì²´ íŒŒì´í”„ë¼ì¸ ì ìš©)
    const sampleJobs = jobAttrs.results.map(j => ({
      job_id: j.job_id,
      job_name: j.job_name,
      slug: j.slug || j.job_id,
      image_url: j.image_url,
      job_description: extractJobDescription(j.api_data_json, j.merged_profile_json, j.job_name),
      base_like: Math.round((j.wlb + j.growth + j.stability + j.income) / 4),
      base_can: Math.round((j.teamwork + j.analytical * 0.7 + j.creative) / 3),
      base_risk: 10,
      attributes: {
        wlb: j.wlb,
        growth: j.growth,
        stability: j.stability,
        income: j.income,
        teamwork: j.teamwork,
        solo_deep: j.solo_deep,
        analytical: j.analytical,
        creative: j.creative,
        execution: j.execution,
        people_facing: j.people_facing,
        work_hours: j.work_hours,
        shift_work: j.shift_work,
        travel: j.travel,
        remote_possible: j.remote_possible,
        degree_required: j.degree_required,
        license_required: j.license_required,
      },
    }))

    // Mini Module Hard Filter ì ìš©
    const { filtered: filteredJobs } = applyMiniModuleHardFilter(sampleJobs, miniModule)
    const jobsToScore = filteredJobs.length > 0 ? filteredJobs : sampleJobs

    // Fact boosts ê³„ì‚°
    const factBoosts = calculateFactBoosts(facts)

    // Verified Can ì¶”ì¶œ
    const verifiedCan = extractVerifiedCanFromFacts(facts)

    // ì„±ì¥ê³¡ì„  ì„ í˜¸ë„ ì¶”ì¶œ
    const growthPreference = extractGrowthPreference(miniModule)

    // ë‚´ë©´ê°ˆë“± ì‹¬ê°ë„ í™•ì¸
    const conflictSeverity = calculateConflictSeverity(miniModule)

    // ì¶”ì  ë³€ìˆ˜
    let growthCurveApplied = false
    let conflictRiskApplied = false
    let canFilterApplied = false
    let balanceCapApplied = false

    // ê° ì§ì—… ì ìˆ˜ ê³„ì‚°
    const scoredJobs = jobsToScore.map(job => {
      const baseScores: JobScores = {
        like: job.base_like,
        can: job.base_can,
        risk_penalty: job.base_risk,
      }

      // Fact boosts ì ìš©
      const adjusted = applyFactBoostsToJob(baseScores, job.attributes, factBoosts)

      // ì¶”ê°€ í˜ë„í‹° ê³„ì‚°
      let additionalPenalty = 0

      // Mini Module Risk Penalty
      const miniModuleRisk = calculateMiniModuleRiskPenalty(miniModule, job.attributes)
      additionalPenalty += miniModuleRisk.penalty

      // Can ê¸°ë°˜ í•„í„° Penalty
      const canFilterResult = applyCanBasedFilter(job.attributes, verifiedCan)
      additionalPenalty += canFilterResult.totalPenalty
      if (canFilterResult.totalPenalty > 0 || canFilterResult.appliedRules.length > 0) {
        canFilterApplied = true
      }

      // ì„±ì¥ê³¡ì„  ë§¤ì¹­
      const growthMatch = matchGrowthCurves(growthPreference, job.attributes, job.job_name)
      additionalPenalty += growthMatch.riskPenalty
      if (growthMatch.riskPenalty > 0 || growthMatch.likeBoost > 0) {
        growthCurveApplied = true
      }

      // ë‚´ë©´ê°ˆë“± Risk ì¡°ì •
      const conflictRisk = calculateConflictRisk(miniModule, job.attributes)
      additionalPenalty += conflictRisk.totalRiskAdjust
      if (conflictRisk.totalRiskAdjust > 0) {
        conflictRiskApplied = true
      }

      // Likeì— ì„±ì¥ê³¡ì„  ë¶€ìŠ¤íŠ¸ ì ìš©
      const likeWithGrowthBoost = adjusted.like + growthMatch.likeBoost

      // Balance Cap ì ìš©
      const balanced = applyBalanceCap(likeWithGrowthBoost, adjusted.can)
      if (balanced.balance_cap_applied) {
        balanceCapApplied = true
      }

      // ìµœì¢… Fit ê³„ì‚°
      const totalRiskPenalty = adjusted.risk_penalty + additionalPenalty
      const fit = Math.round(0.55 * balanced.like + 0.45 * balanced.can - totalRiskPenalty)

      return {
        job_id: job.job_id,
        job_name: job.job_name,
        scores: {
          fit: Math.max(0, fit),
          like: balanced.like,
          can: balanced.can,
          risk_penalty: totalRiskPenalty,
        },
      }
    })

    // ì ìˆ˜ìˆœ ì •ë ¬
    scoredJobs.sort((a, b) => b.scores.fit - a.scores.fit)

    // 6. ê²°ê³¼ ê²€ì¦
    const verificationResult = verifyTestResults(scenario, {
      topJobs: scoredJobs.slice(0, 10),
      excludedJobs: sampleJobs
        .filter(j => !filteredJobs.some(f => f.job_id === j.job_id))
        .slice(0, 10)
        .map(j => ({ job_name: j.job_name, reason: 'Hard Filter' })),
      appliedFeatures: {
        growthCurveApplied,
        conflictRiskApplied,
        canFilterApplied,
        balanceCapApplied,
      },
    })

    console.log(`[Test Scenario] Completed: ${scenario.name} - ${verificationResult.summary}`)

    // 7. ê²°ê³¼ë¥¼ DBì— ì €ì¥í•˜ì—¬ ê²°ê³¼ í˜ì´ì§€ì—ì„œ ë³¼ ìˆ˜ ìˆë„ë¡
    const top10WithDetails = scoredJobs.slice(0, 10).map(j => {
      const original = sampleJobs.find(s => s.job_id === j.job_id)
      return {
        job_id: j.job_id,
        job_name: j.job_name,
        slug: original?.slug || j.job_id,
        image_url: original?.image_url || null,
        job_description: original?.job_description || null,
        fit_score: j.scores.fit,
        like_score: j.scores.like,
        can_score: j.scores.can,
        risk_penalty: j.scores.risk_penalty,
        rationale: `${scenario.name} ì‹œë‚˜ë¦¬ì˜¤ í…ŒìŠ¤íŠ¸ ê²°ê³¼`,
      }
    })

    // ë¯¸ë‹ˆëª¨ë“ˆ ê¸°ë°˜ ìš”ì•½ ìƒì„±
    const mm = scenario.miniModule
    const interestLabels = {
      data_numbers: 'ë°ì´í„° ë¶„ì„', problem_solving: 'ë¬¸ì œ í•´ê²°', research: 'ì—°êµ¬',
      creative: 'ì°½ì˜ì  í™œë™', design: 'ë””ìì¸', art: 'ì˜ˆìˆ ',
      helping: 'ì‚¬ëŒ ë•ê¸°', organizing: 'ì¡°ì§/ê´€ë¦¬', routine: 'ì •í•´ì§„ ì—…ë¬´',
      tech: 'ê¸°ìˆ /IT',
    }
    const valueLabels = {
      autonomy: 'ììœ¨ì„±', growth: 'ì„±ì¥', expertise: 'ì „ë¬¸ì„±',
      stability: 'ì•ˆì •', wlb: 'ì›Œë¼ë°¸', income: 'ìˆ˜ì…',
      creativity: 'ì°½ì˜ì„±', recognition: 'ì¸ì •',
    }
    const strengthLabels = {
      analytical: 'ë¶„ì„ë ¥', fast_learning: 'ë¹ ë¥¸ í•™ìŠµ', persistence: 'ëˆê¸°',
      creative: 'ì°½ì˜ì„±', communication: 'ì†Œí†µ', structured_execution: 'ì²´ê³„ì  ì‹¤í–‰',
    }

    const interestStr = (mm.interest_top || []).map(i => interestLabels[i] || i).join(', ')
    const valueStr = (mm.value_top || []).map(v => valueLabels[v] || v).join(', ')
    const strengthStr = (mm.strength_top || []).map(s => strengthLabels[s] || s).join(', ')

    const resultToSave = {
      engine_version: 'v3-test',
      fit_top3: top10WithDetails,
      like_top10: top10WithDetails,
      can_top10: top10WithDetails,
      test_scenario: {
        id: scenario.id,
        name: scenario.name,
        verification: verificationResult,
      },
      // í”„ë¡ íŠ¸ì—”ë“œ ê²°ê³¼ í˜ì´ì§€ í˜¸í™˜ì„ ìœ„í•œ premium_report
      premium_report: {
        executiveSummary: `[í…ŒìŠ¤íŠ¸ ì‹œë‚˜ë¦¬ì˜¤: ${scenario.name}] ${scenario.description}`,
        workStyleNarrative: `ë‹¹ì‹ ì€ ${interestStr}ì— ê´€ì‹¬ì´ ìˆê³ , ${valueStr}${ì„ë¥¼(valueStr)} ì¤‘ìš”í•˜ê²Œ ìƒê°í•©ë‹ˆë‹¤. ${strengthStr}${ì´ê°€(strengthStr)} ê°•ì ì…ë‹ˆë‹¤.`,
        lifeVersionStatement: {
          oneLiner: scenario.description,
          expanded: [`í…ŒìŠ¤íŠ¸ ì‹œë‚˜ë¦¬ì˜¤: ${scenario.id}`, `ê²€ì¦ ì ìˆ˜: ${verificationResult.score}/100`],
        },
        workStyleMap: {
          socialStyle: mm.workstyle_top?.[0] || 'balanced',
          decisionStyle: mm.execution_style || 'planner',
        },
        innerConflictAnalysis: mm.internal_conflict_flags?.length
          ? `ë‚´ë©´ ê°ˆë“± ê°ì§€: ${mm.internal_conflict_flags.join(', ')}`
          : 'íŠ¹ë³„í•œ ë‚´ë©´ ê°ˆë“±ì´ ê°ì§€ë˜ì§€ ì•Šì•˜ìŠµë‹ˆë‹¤.',
        growthCurveType: mm.persistence_anchor === 'growth_anchor' ? 'ë„ì „ ì„±ì¥í˜•' : 'ì•ˆì • ì„±ì¥í˜•',
        growthCurveDescription: mm.persistence_anchor === 'growth_anchor'
          ? 'ì„±ì¥ ì§€í–¥ì ì¸ ì»¤ë¦¬ì–´ íŒ¨ìŠ¤ë¥¼ ì„ í˜¸í•©ë‹ˆë‹¤.'
          : 'ì•ˆì •ì ì¸ ì»¤ë¦¬ì–´ íŒ¨ìŠ¤ë¥¼ ì„ í˜¸í•©ë‹ˆë‹¤.',
        stressProfile: mm.energy_drain_flags?.length
          ? `ì—ë„ˆì§€ ì†Œëª¨ ìš”ì¸: ${mm.energy_drain_flags.join(', ')}`
          : 'íŠ¹ë³„í•œ ìŠ¤íŠ¸ë ˆìŠ¤ ìš”ì¸ì´ ì—†ìŠµë‹ˆë‹¤.',
        stressTriggers: mm.energy_drain_flags || [],
        expertGuidance: {
          doNow: [`${scenario.name} ì‹œë‚˜ë¦¬ì˜¤ í…ŒìŠ¤íŠ¸ ê²°ê³¼ë¥¼ ê²€í† í•˜ì„¸ìš”.`],
        },
        jobRecommendations: {
          overallTop5: top10WithDetails.slice(0, 5),
        },
        // í”„ë¡œí•„ í•´ì„ (Quick Testìš©)
        profileInterpretation: {
          interests: (mm.interest_top || []).map((t: string) => ({
            token: t,
            label: interestLabels[t] || t,
            meaning: `${interestLabels[t] || t}ì— ê´€ì‹¬ì´ ìˆìŠµë‹ˆë‹¤.`
          })),
          interests_summary: interestStr ? `ë‹¹ì‹ ì€ ${interestStr}ì— ê´€ì‹¬ì´ ìˆìŠµë‹ˆë‹¤.` : '',
          strengths: (mm.strength_top || []).map((t: string) => ({
            token: t,
            label: strengthLabels[t] || t,
            meaning: (() => { const l = strengthLabels[t] || t; return `${l}${ì´ê°€(l)} ê°•ì ì…ë‹ˆë‹¤.` })()
          })),
          strengths_summary: strengthStr ? `ë‹¹ì‹ ì€ ${strengthStr}ì— ê°•ì ì„ ê°€ì§€ê³  ìˆìŠµë‹ˆë‹¤.` : '',
          values: (mm.value_top || []).map((t: string) => ({
            token: t,
            label: valueLabels[t] || t,
            meaning: (() => { const l = valueLabels[t] || t; return `${l}${ì„ë¥¼(l)} ì¤‘ìš”í•˜ê²Œ ìƒê°í•©ë‹ˆë‹¤.` })()
          })),
          values_summary: valueStr ? `ë‹¹ì‹ ì—ê²Œ ${valueStr}ëŠ” ì¤‘ìš”í•œ ê°€ì¹˜ì…ë‹ˆë‹¤.` : '',
          constraints: (mm.constraint_flags || []).map((t: string) => ({
            token: t,
            label: t,
            meaning: `${t}${ì„ë¥¼(t)} í”¼í•˜ê³  ì‹¶ì–´í•©ë‹ˆë‹¤.`
          })),
          constraints_summary: (() => { const flags = mm.constraint_flags || []; if (!flags.length) return ''; const last = flags[flags.length - 1]; return `ë‹¹ì‹ ì€ ${flags.join(', ')}${ì„ë¥¼(last)} í”¼í•˜ê³  ì‹¶ì–´í•©ë‹ˆë‹¤.` })(),
          overall_profile: `${scenario.name} ì‹œë‚˜ë¦¬ì˜¤ ê¸°ë°˜ í…ŒìŠ¤íŠ¸ í”„ë¡œí•„ì…ë‹ˆë‹¤.`
        },
      },
      user_insight: {
        summary: scenario.description,
        traits: [
          { name: 'ê´€ì‹¬ì‚¬', values: mm.interest_top || [] },
          { name: 'ê°€ì¹˜', values: mm.value_top || [] },
          { name: 'ê°•ì ', values: mm.strength_top || [] },
        ],
      },
    }

    let savedRequestId: number | null = null
    try {
      // 1. ë¨¼ì € ai_sessionsì— í…ŒìŠ¤íŠ¸ ì„¸ì…˜ ìƒì„± (ì™¸ë˜ í‚¤ ì œì•½ ì¡°ê±´ ì¶©ì¡±)
      await db.prepare(`
        INSERT OR IGNORE INTO ai_sessions (id, user_identifier, traits_snapshot, created_at, last_active_at)
        VALUES (?, 'test_scenario', ?, CURRENT_TIMESTAMP, CURRENT_TIMESTAMP)
      `).bind(testSessionId, JSON.stringify(scenario.miniModule)).run()
      console.log(`[Test Scenario] Session created/exists: ${testSessionId}`)

      // 2. ai_analysis_requestsì— ì €ì¥ (prompt_payloadëŠ” í•„ìˆ˜)
      const promptPayload = JSON.stringify({
        test_scenario: scenario.id,
        miniModule: scenario.miniModule,
      })
      const insertResult = await db.prepare(`
        INSERT INTO ai_analysis_requests (session_id, analysis_type, pricing_tier, prompt_payload, status, processed_at, requested_at)
        VALUES (?, 'job', 'free', ?, 'completed', CURRENT_TIMESTAMP, CURRENT_TIMESTAMP)
      `).bind(testSessionId, promptPayload).run()
      savedRequestId = insertResult.meta?.last_row_id as number || null

      if (savedRequestId) {
        // 3. ai_analysis_resultsì— ì €ì¥ (ì‹¤ì œ í”„ë¡œë•ì…˜ ìŠ¤í‚¤ë§ˆ: result_json ì‚¬ìš©)
        await db.prepare(`
          INSERT INTO ai_analysis_results (request_id, result_json, engine_version, created_at)
          VALUES (?, ?, 'v3-test', CURRENT_TIMESTAMP)
        `).bind(savedRequestId, JSON.stringify(resultToSave)).run()
        console.log(`[Test Scenario] Result saved - request_id: ${savedRequestId}`)
      }
    } catch (saveError) {
      console.warn('[Test Scenario] Failed to save result to DB:', saveError)
    }

    return c.json({
      success: true,
      scenario: {
        id: scenario.id,
        name: scenario.name,
        description: scenario.description,
        category: scenario.category,
      },
      test_session_id: testSessionId,
      request_id: savedRequestId,
      result_page_url: savedRequestId
        ? `/analyzer/job?request_id=${savedRequestId}`
        : null,
      result_api_url: savedRequestId
        ? `/api/ai-analyzer/result/${savedRequestId}`
        : null,
      results: {
        top_jobs: scoredJobs.slice(0, 5).map(j => {
          const original = sampleJobs.find(s => s.job_id === j.job_id)
          return {
            job_id: j.job_id,
            job_name: j.job_name,
            slug: original?.slug || j.job_id,
            image_url: original?.image_url || null,
            fit: j.scores.fit,
            like: j.scores.like,
            can: j.scores.can,
            risk: j.scores.risk_penalty,
          }
        }),
        excluded_count: sampleJobs.length - filteredJobs.length,
        total_scored: scoredJobs.length,
        applied_features: {
          growth_curve: growthCurveApplied,
          conflict_risk: conflictRiskApplied,
          can_filter: canFilterApplied,
          balance_cap: balanceCapApplied,
        },
      },
      verification: verificationResult,
      fact_boosts_applied: factBoosts.applied_rules,
    })

  } catch (error) {
    console.error('[Test Scenario] Error:', error)
    return c.json({
      success: false,
      error: error instanceof Error ? error.message : 'Test execution failed',
    }, 500)
  }
})

// ì „ì²´ ì‹œë‚˜ë¦¬ì˜¤ ì¼ê´„ ì‹¤í–‰
analyzerRoutes.post('/test/run-all', async (c) => {
  const env = c.env as Bindings
  const db = env.DB

  const results: Array<{
    scenario_id: string
    scenario_name: string
    passed: boolean
    score: number
    summary: string
  }> = []

  for (const scenario of TEST_SCENARIOS) {
    try {
      // ê°„ì†Œí™”ëœ í…ŒìŠ¤íŠ¸ (ìœ„ì˜ ë¡œì§ì„ í•¨ìˆ˜ë¡œ ì¶”ì¶œí•´ì„œ ì¬ì‚¬ìš©í•˜ë©´ ë” ì¢‹ìŒ)
      // ì—¬ê¸°ì„œëŠ” ì‹œë‚˜ë¦¬ì˜¤ ì‹¤í–‰ ê²°ê³¼ë§Œ ìš”ì•½
      const testUrl = `/api/ai-analyzer/test/run-scenario`
      // ì‹¤ì œë¡œëŠ” ë‚´ë¶€ í˜¸ì¶œí•˜ê±°ë‚˜ ë¡œì§ ì¬ì‚¬ìš©

      results.push({
        scenario_id: scenario.id,
        scenario_name: scenario.name,
        passed: true,  // ì‹¤ì œ ê²€ì¦ í•„ìš”
        score: 0,      // ì‹¤ì œ ê²€ì¦ í•„ìš”
        summary: `${scenario.name} í…ŒìŠ¤íŠ¸ ì‹¤í–‰ í•„ìš”`,
      })
    } catch (error) {
      results.push({
        scenario_id: scenario.id,
        scenario_name: scenario.name,
        passed: false,
        score: 0,
        summary: error instanceof Error ? error.message : 'Failed',
      })
    }
  }

  const passedCount = results.filter(r => r.passed).length
  const totalScore = results.reduce((sum, r) => sum + r.score, 0) / results.length

  return c.json({
    success: true,
    summary: {
      total: results.length,
      passed: passedCount,
      failed: results.length - passedCount,
      average_score: totalScore.toFixed(1),
    },
    results,
  })
})

// ============================================
// E2E í…ŒìŠ¤íŠ¸ìš© LLM ë‹µë³€ ìƒì„± ì—”ë“œí¬ì¸íŠ¸
// ============================================
analyzerRoutes.post('/test/generate-answer', async (c) => {
  const env = c.env as Bindings
  const openaiKey = env.OPENAI_API_KEY

  if (!openaiKey) {
    return c.json({ success: false, error: 'OPENAI_API_KEY not configured' }, 500)
  }

  try {
    const body = await c.req.json() as {
      question: string
      round: number
      persona: {
        name: string
        career_state: string
        interests: string[]
        strengths: string[]
        values: string[]
        constraints: string[]
        narrative_context?: string
      }
      previous_answers?: string[]
    }

    const { question, round, persona, previous_answers = [] } = body

    if (!question || !persona) {
      return c.json({ success: false, error: 'Missing question or persona' }, 400)
    }

    // í˜ë¥´ì†Œë‚˜ ì»¨í…ìŠ¤íŠ¸ êµ¬ì„±
    const personaContext = `
ë‹¹ì‹ ì€ ë‹¤ìŒê³¼ ê°™ì€ í”„ë¡œí•„ì„ ê°€ì§„ êµ¬ì§ìì…ë‹ˆë‹¤:
- ì´ë¦„: ${persona.name}
- í˜„ì¬ ìƒíƒœ: ${persona.career_state === 'employed' ? 'ì¬ì§ ì¤‘' : persona.career_state === 'job_seeker' ? 'êµ¬ì§ ì¤‘' : persona.career_state === 'student' ? 'í•™ìƒ' : 'ì´ì§ ê³ ë ¤ ì¤‘'}
- ê´€ì‹¬ ë¶„ì•¼: ${persona.interests?.join(', ') || 'ë‹¤ì–‘í•œ ë¶„ì•¼'}
- ê°•ì : ${persona.strengths?.join(', ') || 'ë¶„ì„ë ¥, ë¬¸ì œí•´ê²°'}
- ì¤‘ìš” ê°€ì¹˜: ${persona.values?.join(', ') || 'ì„±ì¥, ì•ˆì •'}
- í”¼í•˜ê³  ì‹¶ì€ ê²ƒ: ${persona.constraints?.join(', ') || 'ì•¼ê·¼, ë¶ˆê·œì¹™í•œ ê·¼ë¬´'}
${persona.narrative_context ? `\nì¶”ê°€ ë°°ê²½:\n${persona.narrative_context}` : ''}
`.trim()

    // ë¼ìš´ë“œë³„ ë‹µë³€ ìŠ¤íƒ€ì¼ ê°€ì´ë“œ
    const roundGuide = round === 1
      ? 'ìš•ë§ê³¼ ëª©í‘œì— ëŒ€í•´ ì†”ì§í•˜ê²Œ ë‹µë³€í•˜ì„¸ìš”. ë¬´ì—‡ì„ ì›í•˜ëŠ”ì§€, ì–´ë–¤ ë¯¸ë˜ë¥¼ ê¿ˆê¾¸ëŠ”ì§€.'
      : round === 2
      ? 'í”¼í•˜ê³  ì‹¶ì€ ê²ƒ, ìŠ¤íŠ¸ë ˆìŠ¤ ìš”ì¸ì— ëŒ€í•´ ë‹µë³€í•˜ì„¸ìš”. ì–´ë–¤ í™˜ê²½ì´ ë§ì§€ ì•ŠëŠ”ì§€.'
      : 'í˜„ì‹¤ì  ì œì•½ê³¼ íƒ€í˜‘ì ì— ëŒ€í•´ ë‹µë³€í•˜ì„¸ìš”. ì„±ì¥ì„ ìœ„í•´ ë¬´ì—‡ì„ ê°ìˆ˜í•  ìˆ˜ ìˆëŠ”ì§€.'

    const systemPrompt = `${personaContext}

ì»¤ë¦¬ì–´ ìƒë‹´ì‚¬ì˜ ì§ˆë¬¸ì— ë‹µë³€í•´ì•¼ í•©ë‹ˆë‹¤.
ë¼ìš´ë“œ ${round}: ${roundGuide}

ë‹µë³€ ê·œì¹™:
1. 1~3ë¬¸ì¥ìœ¼ë¡œ ê°„ê²°í•˜ê²Œ ë‹µë³€í•˜ì„¸ìš”
2. í˜ë¥´ì†Œë‚˜ì˜ íŠ¹ì„±ì— ë§ê²Œ ì¼ê´€ì„± ìˆê²Œ ë‹µë³€í•˜ì„¸ìš”
3. êµ¬ì²´ì ì¸ ì˜ˆì‹œë‚˜ ê²½í—˜ì„ ì–¸ê¸‰í•˜ë©´ ì¢‹ìŠµë‹ˆë‹¤
4. í•œêµ­ì–´ë¡œ ìì—°ìŠ¤ëŸ½ê²Œ ë‹µë³€í•˜ì„¸ìš”`

    const messages: Array<{ role: 'system' | 'user' | 'assistant', content: string }> = [
      { role: 'system', content: systemPrompt }
    ]

    // ì´ì „ ë‹µë³€ë“¤ì„ ì»¨í…ìŠ¤íŠ¸ì— ì¶”ê°€
    if (previous_answers.length > 0) {
      messages.push({
        role: 'user',
        content: `ì´ì „ ë‹µë³€ë“¤:\n${previous_answers.map((a, i) => `${i + 1}. ${a}`).join('\n')}`
      })
    }

    messages.push({ role: 'user', content: `ì§ˆë¬¸: ${question}` })

    // OpenAI í˜¸ì¶œ
    const response = await fetch('https://api.openai.com/v1/chat/completions', {
      method: 'POST',
      headers: {
        'Content-Type': 'application/json',
        'Authorization': `Bearer ${openaiKey}`,
      },
      body: JSON.stringify({
        model: 'gpt-4o-mini',
        messages,
        temperature: 0.8,
        max_tokens: 200,
      }),
    })

    if (!response.ok) {
      const error = await response.text()
      console.error('[E2E] OpenAI error:', error)
      return c.json({ success: false, error: 'OpenAI API error' }, 500)
    }

    const data = await response.json() as {
      choices: Array<{ message: { content: string } }>
      usage: { total_tokens: number }
    }

    const answer = data.choices[0]?.message?.content?.trim() || ''

    console.log(`[E2E] Generated answer for round ${round}:`, answer.substring(0, 100))

    return c.json({
      success: true,
      answer,
      usage: data.usage,
    })
  } catch (error) {
    console.error('[E2E] Generate answer error:', error)
    return c.json({
      success: false,
      error: error instanceof Error ? error.message : 'Unknown error',
    }, 500)
  }
})

export { analyzerRoutes }
